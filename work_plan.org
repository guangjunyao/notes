#+TITLEs: Document title
#+LANGUAGE: en
#+OPTIONS: toc:nil h:4 html-postamble:nil html-preamble:t tex:t f:t
#+OPTIONS: prop:("VERSION")
#+HTML_DOCTYPE: <!DOCTYPE html>
#+HTML_HEAD: <link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="css/style.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./styles/demo/css/style.css"/>

#+HTML: <div class="outline-2" id="meta">
| *Author* | {{{author}}} ({{{email}}})    |
| *Date*   | {{{time(%Y-%m-%d %H:%M:%S)}}} |
#+HTML: </div>


* Workflow
1. search.
2. read pdf/code, take notes.
3. present general idea.
4. write pseudo code, set input parameters.
5. write/modify code.
6. plugging gs to run with data.
7. check result.
8. prepare presentation of all previous steps.
9. present and get feedback.
10. deploy function definition, function sampling.

* GS Definition:
Graph Strategist is a intelligent research platform.

User can start a research from extracting paper idea, to check what data, model the paper uses, to expect what result, modeling input, signal, python function, backtesting, fitness statistics.

difference from DIY, change parameters, taking notes and communicate with peers, saving working sessions, changing models, data.

自然语言处理嵌入gs：
简单描述说明一下如何把自然语言处理提供的一些功能应用到GS产品，与GS提供的功能结合，目前能够做到的自然语言处理功能包括情感分析，命名实体识别，Word Embedding, 知识网络， 网络爬虫。

使用需求：
1. 产生事件驱动策略信号：
根据数据库analyst report, market view, news, company annoucement 的文本所描述的事件，产生文本所描述指数，产业或者股票的买卖交易信号。
Event-Driven Strategies
- Earnings Releases:
Corporate earnings tend to move markets when they come in above or below the market’s expectations, which means that it’s important for active traders to understand the expected figures beforehand.
- Mergers & Acquisitions:
M&A tends to produce dramatic increases or decreases in share prices depending on the terms of the deal, while creating an opportunity for arbitrage strategies between the buyer and seller.
- Spin-Offs:
Spin-offs tend to see an initial decline in share price as institutional investors who received shares sell off their stake to comply with regulatory requirements or other rules, thereby creating opportunities for traders.

2. gs选择函数时产生相关推荐：
当用户在选择gs函数时，以及将来Reinforcement Learning功能使用时可能需要更多的类似函数或者联想出更多相关的函数。例如：当用户选择函数时打入去极值函数时，gs自动推荐去极值的几种函数，如分位数去极值，标准差去极值，中位数去极值，Median Absolute Deviation(MAD)去极值。

以及当进行函数的输入数据选择时，联想出更多的相关数据选项。例如当选择股票池时，打入上证50，gs自动推荐出其它的几个相关的数据选项，如沪深300，上证100，中证500；再如当进行因子回测时，打入市盈率因子，gs自动推荐出其它因子像PB, ROE 等数据名字。

3. 探索跟某一公司，产业，技术在业务上相关联的公司，产业或者技术：

设计方案：
1. gs编写Python函数，进行情感分析，命名实体识别，关键字分析，产生事件驱动策略信号：

2. Word Embedding：

3. 知识网络，网络爬虫：
框架：


* Deep Learning
** Natural Language Processing

*** TODO Personalized Intelligent Agents(PIA)
**** Objective:
***** Short Term
- make traditional and mature NLP model customizable visually by end users at multiple levels e.g., training data, model parameter.
- incorporate customizable NLP signals into both qualitative and quantitative investment strategies and research process.
- recommendation of context sensitive synonyms.
- Hot-word trending analysis on various topics, technology, industrial sectors, analysts comment, company announcement, Deep Learning authors comments, popular topics, papges, etc. Scrape the historical archives of web text in order to get for each post the following information: date, keywords, text.

***** Long Term
- context sensitive, Personalized Intelligent Agents, forming an eco-system collaborating, competing and evolving together on behalf of their owners.
- Graph2vec, 把word2vec应用在用户workflow使用序列上，根据用户使用app/func的顺序，把app/func看做单词，也是可以形成这样的序列数据，进而训练处每个app对应的向量。利用这个向量计算app之间的相似度，把真正内容相关的app聚合在一起，推荐给用户。
- 可计算的知识服务引擎, like WolframAlpha.
****** 智能投顾
- 基于金融理财产品知识图谱的语义查询。知识图谱从互联网上公开信息构建，并支持如“高收益”这样的语义搜索。
- 搜索相关主题的股票，及其新闻，公告，如“雄安题材”，“苹果主题”，“独角兽”等。
****** 金融新闻Bots
- 实时监控各大金融新闻网站的数据，构建了一个监控平台。 根据用户输入的自然语言问题，自动推送与其相关的新闻，并标记其中和用户输入问题相关的段落。
- Event-Driven Stock prediction, using sentiment analysis on the web-crawling tokens/documents with doc2vec.
******* 金融搜索
显示当前关键词所有的新闻消息，公告，根据消息数量得出的热度。
****** 智能选股
| 百科类筛选 | ST股票， 非ST股票， 上证50股票                                         |
| 数据类筛选 | 股价小于100块股票                                                      |
| 事件类筛选 | 刚复牌股票，今年刚上市的股票                                           |
| 选股类筛选 | 每股收益在0.2-0.3之间，且市盈率小于30的股票;浙江省市盈率小于20倍的公司 |

****** 知识图谱分析
通过知识图谱相关技术从招股书、年报、公司公告、券商研究报告、新闻等半结构化表格和非结构化文本数据中批量自动抽取公司的股东、子公司、供应商、客户、合作伙伴、竞争对手等信息，构建出公司的知识图谱。

关系可以是企业的上下游、合作、竞争对手、子母公司、投资、对标等关系，可以是高管与企业间的任职等关系，也可以是行业间的逻辑关系，实体则是投资机构、投资人、企业等等，把它们用知识图谱表示出来，从而进行更深入的知识推理。

在某个宏观经济事件或者企业相关事件发生的时候，券商分析师、交易员、基金公司基金经理等投资研究人员可以通过此图谱做更深层次的分析和更好的投资决策，比如在美国限制向中兴通讯出口的消息发布之后，如果我们有中兴通讯的客户供应商、合作伙伴以及竞争对手的关系图谱，就能在中兴通讯停牌的情况下快速地筛选出受影响的国际国内上市公司从而挖掘投资机会或者进行投资组合风险控制。

某公司打算借壳上市，通过知识图谱相关技术寻找相关联的公司，找到有可能的潜在借壳对象。
[[./img/knowledge_graph_company.jpg]]

[[./img/knowledge_graph_organization.png]]
****** Event-Driven Strategies
******* Goal:
Detect event from announcement, predict stock price.
******* events:
- Earnings Releases:
Corporate earnings tend to move markets when they come in above or below the market’s expectations, which means that it’s important for active traders to understand the expected figures beforehand.
- Mergers & Acquisitions:
M&A tends to produce dramatic increases or decreases in share prices depending on the terms of the deal, while creating an opportunity for arbitrage strategies between the buyer and seller.
- Spin-Offs:
Spin-offs tend to see an initial decline in share price as institutional investors who received shares sell off their stake to comply with regulatory requirements or other rules, thereby creating opportunities for traders.
******* Tool:
Named Entity Recognization, Entity Relation Extraction, Recurrent Neural Networks, CNN.
**** Overview Steps:

- 数据爬虫 -> 数据清洗(干净数据) -> 语义分析, 命名实体识别(识别金融实体、实体提取及消歧、关系提取,分区语义网络) -> 文档树/表 -> 图谱(根据业务需要，定义并识别金融实体间的各种关系，进而生成知识图谱) -> 本体，本体存储 -> 语用分析 -> 分析推理，逻辑(定义并表达业务逻辑，通过在知识图谱上实现各种具体任务来体现数据价值，如推理等，实现数据到智能的升华。)

- Pipeline: choose category and depth in GS skill -> return pages GID -> fetch paper via GID in python -> text processing -> word embedding models, entity extraction, classification, topic extraction, relation extraction, sequence to sequenc, sentiment analysis -> help users to select X,y.


**** Model
- State of the Art NL models based on DNN
  - CBOW
  - Skip gram
  - RNN(Sequence to sequence)
  - LSTM
  - Convolutional Network Sentence classification
- Libraries
  - Gensim
  - SPACY
  - IEPY
  - Stanford coreNLP
  - NLTK
**** Data
***** Text Labeled with (semi) structured data
- [X] wiki page titles and page text, wiki category
- [ ] company filings text, filing classification, event time series data (attributes manually extracted)
- [ ] finance conference scripts.
- [ ] corporate telephone comference notes, memo.
- [ ] company filings text, company industry classification
- [ ] textbook or CFA chapter name, chapter exercise questions, multiple choice questions
- [ ] SSRN paper keyword, category, title, abstract, data, model and conclusion
***** Text Unlabeled
- [ ] analyst research reports, analyst research reports, report classification, analyst ranking
- [ ] textbooks
*** TODO Knowledge Graph
**** DONE entity relationship<2018-03-19 Mon> - <2018-03-21 Wed>
CLOSED: [2018-03-26 Mon 13:52]
:LOGBOOK:
CLOCK: [2018-03-21 Wed 11:10]--[2018-03-21 Wed 18:56] =>  7:46
CLOCK: [2018-02-07 Wed 11:13]--[2018-02-07 Wed 15:36] =>  4:23
CLOCK: [2018-02-06 Tue 10:07]--[2018-02-06 Tue 11:32] =>  1:25
CLOCK: [2018-02-05 Mon 15:38]--[2018-02-05 Mon 16:21] =>  0:43
CLOCK: [2018-02-05 Mon 09:54]--[2018-02-05 Mon 15:34] =>  5:40
CLOCK: [2018-02-02 Fri 10:23]--[2018-02-02 Fri 17:25] =>  7:02
CLOCK: [2018-02-01 Thu 15:42]--[2018-02-01 Thu 20:26] =>  4:44
CLOCK: [2018-02-01 Thu 10:00]--[2018-02-01 Thu 11:52] =>  1:52
CLOCK: [2018-01-31 Wed 15:16]--[2018-01-31 Wed 19:29] =>  4:13
CLOCK: [2018-01-31 Wed 10:53]--[2018-01-31 Wed 12:08] =>  1:15
CLOCK: [2018-01-31 Wed 10:09]--[2018-01-31 Wed 10:52] =>  0:43
CLOCK: [2018-01-30 Tue 17:45]--[2018-01-30 Tue 18:35] =>  0:50
:END:
- 命名实体识别
现在常用的方法有「条件随机场（CRF）」、「最大熵隐马尔科夫」、「隐马尔科夫」等序列标注模型。 主要的处理思想有:

- [X] finish join learning entity extraction paper.<2018-03-20 Tue>
- [ ] summerize text first, then event extraction?
- [X] find source code and scheme for this paper.
- [ ] from survey paper -> book -> reference paper -> citation paper -> application -> open source library.
- [ ] company relation
- [ ] analyst relation
- [ ] entity extraction resolution detection like author, publisher.
- [X] pseudo code of node, edge upload.
- [X] summerize nlp library extraction result comparison in jupyter notebook.
- [X] find the difference of attirbutes not in Juyuan database, searching for useful information.
聚源数据库已经包含了大量的公司信息，暂时没有在百科三元组发现更有价值的信息。
- [X] extract the triple relation information.
- [X] visualization of triples.
- [ ] NER of all listed company pages content what analyst care about: 有关内容包括：主要产品，产业链，竞争对手，合作伙伴，投资方，key person(如公司跟投资人关联), 上市交易所，sentiment, 分析师评级，评论，公司重大公告.
- [X] Chinese NER model is missing, searching. models are in the Chinese model jar file.
- [X] test stanford-corenlp to extract keywords and NER en.
- [X] compare nlp libraries.
- [X] extract Named Entity Recognition.
- [ ] extract RDF company triples.
- [ ] listed companies triples importing to neo4j.
- [ ] read Q&A knowledge graph paper.

**** TODO 语料收集:<2018-03-21 Wed>
- 目标语料格式：
实体1  实体2  关系  包括实体1，实体2和他们之间关系的语句。
- 加快语料收集的想法：
  1. 自定义字典法，利用已有的种子实体。
  2. 在SSE上搜索已经有的投资，收购等种子实体关系，得到语料。
  3. 利用NER_IDCNN_CRF的实体识别得到语料里面的实体，现有模型支持人名，组织机构和位置。
  4. 从distant supervision的方法中获取灵感，可以首先找到具有确定关系的实体对，然后再去爬取该实体对共同出现的语句作为正样本。负样本则从实体库中随机产生没有关系的实体对，最后去爬取这样实体对共同出现的语句，这样的语句可以通过网络爬虫从雪球，google news抓取。*这样保证了语料收集的快速性和关系数量的扩展性*。
  5. 对于具有确定关系的实体对，从百度百科Triples得到。

- [X] finish Att BLSTM paper.<2018-03-21 Wed>
- [X] 先完成“投资”这一类语料的收集。
- [X] 目标：按实体 实体 关系 语料内容的格式放入训练文件，以供模型训练。
- [X] 丰富语料的思路：通过word2vec 相似词找到“投资”的相似词，如设立，增资，入股，收购，并购，换股;再找以上6个词的相似词。
下表为投资这一大类所包含的相似关系。

| 设立     | 增资     | 入股     | 收购     | 并购     | 换股 |
|----------+----------+----------+----------+----------+------|
| 成立     | 受让     | 现金出资 | 要约收购 | 海外并购 | 转股 |
| 发起设立 | 扩股     | 携手     | 拟收购   | 重组     | 交换 |
| 组建     | 扩股     | 间接持有 | 并表     | 整合     | 配股 |
| 新设     | 占股     | 所持     | 过户     | 兼并     |      |
| 出资     | 转让给   | 联手     | 收购了   | 业务整合 |      |
| 共同出资 | 认缴     | 正式成为 | 资产收购 | 借壳上市 |      |
| 全资     | 定向增发 | 转让给   | 通过收购 |          |      |
| 参股     |          | 参股     |          |          |      |
| 入驻     |          |          |          |          |      |
| 创投     |          |          |          |          |      |
**** TODO 实体和关系的联合抽取处理思想：<2018-03-22 Thu> -
***** goal
1. 利用NER_IDCNN_CRF的实体识别得到语料里面的实体，现有模型支持人名，组织机构和位置。
2. RE_BGRU_2ATT关系识别。
***** TODO pseudo code
:LOGBOOK:
CLOCK: [2018-03-28 Wed 09:47]
:END:
***** review
- the limits of GRU, its memory performance without attention, find out the threshold.
- selecting GRU or LSTM Depends on length of input sentence.
- using existing Knowledge graph and collected  training data.
- use quantitative research, economic indicator formula, analyst report as training data.

***** bugs:
****** multiple white space in the entities.
**** extract the structure of a document, represent as a graph
https://www.iwencai.com/msgconsule/search?qs=pc_~soniu~info~all~resultpage~topsearchbox&tid=report&w=%E8%B4%B5%E5%B7%9E%E8%8C%85%E5%8F%B0

[[./img/knowledge_graph_report.png]]
**** extract the knowledge from company and products.
[[./img/knowledge_graph_fruit.png]]
*** DONE Word Embedding(Word2Vec):<2017-12-01 Fri> - <2017-12-31 Sun>
CLOSED: [2018-03-26 Mon 12:47]
:LOGBOOK:
CLOCK: [2017-12-22 Fri 15:44]--[2017-12-22 Fri 19:21] =>  3:37
CLOCK: [2017-12-14 Thu 10:04]--[2017-12-14 Thu 12:01] =>  1:57
CLOCK: [2017-12-13 Wed 10:04]--[2017-12-13 Wed 15:45] =>  5:41
CLOCK: [2017-12-05 Tue 11:46]--[2017-12-05 Tue 12:04] =>  0:18
CLOCK: [2017-11-27 Mon 10:28]--[2017-11-27 Mon 12:02] =>  1:34
CLOCK: [2017-11-21 Tue 09:07]--[2017-11-21 Tue 15:05] =>  5:58
:END:
**** Goal/use case
- use such word2vec to find similar keywords.
**** jobs: 数据收集， 清洗
- [ ] train analyst report and save model on hdfs, load this model as a j node.
- [ ] gs similar words function test use analyst report.
- [X] upload all vocabulary in word2vec model to Neo4j.
- [X] create a function: word_rec(model, keywords, topn)
- [ ] manually add categories and page links in sql file.
- [X] return word embedding model to next step in GS.
- [X] word embedding的设计文档修改.
- [X] pack pages into a corpus file.
- [ ] compare cutting on paragraph and document.
- [ ] train few files to see if there's repeat training on word2vec.
- [ ] extract data from financial documents — usually PDFs — in an automated way, and to produce “better-than-human” analyses. extract data from tables and text.
- [ ] train function names based on wiki pages on functions, models, and python/matlab/sas/cpp-reference manuals, function names and function descriptions, excel formula, VBA, VB, guass, whatever software which has a function dictionary and manual.
- [ ] retrieve pages title and id under categories from mysql.
- [ ] LSA or LDA analysis on unstructured text, which will give a clustering of words on every topic.
- [ ] visualize vocabulary embedding using t-SNE which project embedding vectors into 2-D surface from an proper perspective using tensorboard locally which can ignore uploading to projector online.
- [ ] create LSTM networks on xarray data.
- [X] create test program to run word embedding, to visualize output.
- [ ] *What is fueling heavy investment in machine learning in the financial industry and how does it fit into customers’ workflows?*
  A lot of our customers’ workflows are being automated, entirely or partially. What they’re doing today is more on the cognitive side: strategy and portfolio selection, formulating the investment theses, etc. People are trying to solve many, many problems in finance using these methods, because they allow for the building of more sophisticated intelligence into trading and client facing workflows. These methods can improve efficiency, or, crucially, allow us to approach problems which heretofore were intractable – due to complicated interactions in the data, complexity of the problem, availability of data or computational resources, and so on.
- [X] provide xarray data to Zhou.
- [X] provide Sun Chinese wiki.
- [X] network Bloomberg about tensorflow.
- [X] retrieving speed test from mongodb.
- [X] test case on finance domain word embedding prediction.
- [X] dumping wiki pages to mongodb.
- [X] testing GPU server.
- [X] configuring deep learning hardware, operation system, software.
- [X] test sets simularity, A-B=C-D?, A+B=?
- [X] incremental training finance pages based via online training.
  online training can not continue missing frequency in pretrained google binary file.
- [ ] cut/training Chinese osets words into files.
- [ ] compare similarity between category and end-to-node oset element.
- [ ] compare the results from GS searching engine and word embedding.
- [ ] import xml pages to elasticsearch.
- [X] clustering categories by word embedding, osets, idea.
  To calculate the similarity matrix between all 160706 vocabulary in RAM, 160706 *160706 *4(bytes)/1024(bytes)/1014(bytes)=99491MB will be needed.
- [X] use [[http://www.cis.lmu.de/pub/phraseEmbedding.txt.bz2][phrase embedding]] as test.
  better phrasing results.
- [X] take a look at cite space iii.
- [X] test word2vec model from finance.
- [X] cut paragraph to short sentences, then phrase.
- [X] phrase text8
- [X] train phrasing sentences word2vec model.
- [X] phrase detection with google pretrained vectors.
- [X] find available library to extract wiki content.
- [X] find all page titles from level 5 finance sub-categories.
- [X] extract page section from wiki xml file.
- [ ] parse Chinese wiki, remove stopwords.
- [X] model wiki token corpus.
- [ ] [[https://github.com/ryankiros/skip-thoughts][skip-thought]].
- [X] find corporate finance/mba questions corpos.
- [ ] read A primer on Neural Network Models.
- [X] tensorflow structure.
- [X] train word2vec model.
- [X] test finding similar words from Wiki corpus.
- [X] download wiki xml file.
- [X] transfer wiki xml file to text format.
- [X] load pre-trained vector matrix, predict the context using a word based on the Skip-Gram model.
- [X] overview of word2vec, why does it work.
- [ ] video explained by Xin Rong.
- [ ] forward propagation vs backward propagation, CNN explained by Andrew Ng.
- [X] paper word2vec Parameter Learning Explained.
- [X] understand Tensorflow Word2Vec example.
- [X] build a backward propagation network.
- [ ] fi or function def from output of wants whose idea word2vec is close to target want's idea.
建一个想法，根据这个想法找到匹配的FI, or FD. 例如，建一个optimize需求，自动推荐black litterman model, or markowitz mean/variance model.
- [ ] fi and its function def whose word2vec is close to word2vec of function instances of current function def to be built.
当前FI,查找相关的下一步FI.

*** DONE Chinese wiki model. <2018-01-01 Mon> - <2018-01-12 Fri>
CLOSED: [2018-02-09 Fri 18:57]
**** jobs: 训练中文维基数据，嵌入GS
- choose model using most related model, use wiki category relation similarity to choose model, train specific field category model. get the related category tree, use regular expression to get responding categories from the wiki xml file.
- [X] train financial fields model(58+ categories).
- [X] use similarity distance to find the nearest category of target words.
- [X] similarity test on specific model.
- [X] add all pages title to jieba dict.
- [X] 中文短语处理，当短语不存在词汇库中时，拆开成词输入到模型。
- [X] preprocessing workflow.
  英文text preprocessing需要的注意一些点，及应提供的选择
  1. cut段落或文章
  2. phrase是否进行转换
  3. 停词(a, the, of, that, this, he, I...)是否保留
  4. 数字是否转为英文单词, 中间有数字的单词是否保留(th8)
  5. 提取词干（时态转换，单复数单词转换）
  6. 标点（撇号'，所有格,缩写如don’t），符号（%,#,&,?,@,\,/,",是否保留）
  7. 大小写转换（句首大写转小写，保留全部大写词，专有名词首字母大写保留）

  中文分词（主要利用结巴分词）
  - [X] 1. cut段落或文章
  - [X] 2. 去停词
  - [X] 去标点符号
  - [X] 去数字
- [ ] word2vec fast text comparison.
- [X] compare the training results with or without stopwords.
- [X] demo code.
- [X] visualize & compare results.
- [X] create index for zhwiki.
- [X] test model.
- [X] assign wiki pages extraction task.
- [X] insert Chinese wiki to mongo, transform traditional Chinese to simple Chinese.
- [X] get rid of the stopwords.
- [X] retrie Chinese financial wiki pages from mongo and train.
- [ ] fix zhwiki to mongodb words count.
                                                                       :wait:

*** DONE Building the Wikipedia Knowledge Graph in Neo4j <2018-01-13 Sat> - <2018-03-09 Fri>
CLOSED: [2018-02-09 Fri 18:58]
:LOGBOOK:
CLOCK: [2017-11-15 Wed 14:01]--[2017-11-15 Wed 16:04] =>  2:03
:END:
[[file:/home/weiwu/website/leolle.github.io/CS/MachineLearning/NaturalLanguageProcessing.org][NLP]]
- [X] wiki SQL database links graph.
- [X] pulling wiki knowledge categories(id), pages(id) and relations to local csv, sql file.
**** DONE Data dumps/Import -> create nodes
CLOSED: [2018-03-26 Mon 13:54]
- methods
[[https://meta.wikimedia.org/wiki/Data_dumps]]

[[https://meta.wikimedia.org/wiki/Data_dumps/Import_examples]]

[[https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/]]
- tools
[[http://wikipapers.referata.com/wiki/List_of_visualization_tools]]

- [ ] Import into an empty wiki of el wiktionary on Linux with MySQL, or Neo4j
- [ ] create special wiki reference edge between read only text nodes
- [X] watch the youtube video
[[https://www.youtube.com/watch?v=o6wueyweC34 ]]
- [X] read Neo4j document
[[http://guides.neo4j.com/wiki]]
- [X] try Neo4j sandbox
[[https://neo4j.com/sandbox-v2/]]
- [X] create Neo4j docker.
**** DONE create wiki knowledge graph -> create edges
CLOSED: [2018-03-26 Mon 12:48] DEADLINE: <2018-01-23 Tue>
:LOGBOOK:
CLOCK: [2018-02-07 Wed 15:36]--[2018-02-07 Wed 18:56] =>  3:20
CLOCK: [2018-02-07 Wed 10:09]--[2018-02-07 Wed 11:12] =>  1:03
CLOCK: [2018-02-06 Tue 11:32]--[2018-02-06 Tue 17:22] =>  5:50
CLOCK: [2018-01-30 Tue 19:24]--[2018-01-30 Tue 20:52] =>  1:28
CLOCK: [2018-01-30 Tue 13:11]--[2018-01-30 Tue 17:45] =>  4:34
CLOCK: [2018-01-30 Tue 10:58]--[2018-01-30 Tue 12:39] =>  1:41
CLOCK: [2018-01-29 Mon 10:36]--[2018-01-29 Mon 20:18] =>  9:42
CLOCK: [2018-01-26 Fri 09:49]--[2018-01-26 Fri 11:16] =>  1:27
CLOCK: [2018-01-25 Thu 10:32]--[2018-01-25 Thu 15:59] =>  5:27
CLOCK: [2018-01-24 Wed 13:58]--[2018-01-24 Wed 19:40] =>  5:42
CLOCK: [2018-01-23 Tue 13:47]--[2018-01-23 Tue 15:47] =>  2:00
CLOCK: [2018-01-23 Tue 09:56]--[2018-01-23 Tue 12:05] =>  2:09
CLOCK: [2018-01-22 Mon 16:45]--[2018-01-22 Mon 19:34] =>  2:49
CLOCK: [2018-01-22 Mon 13:38]--[2018-01-22 Mon 14:28] =>  0:50
CLOCK: [2018-01-22 Mon 10:00]--[2018-01-22 Mon 12:29] =>  2:29
:END:
- [X] extract gid from get skill to graph.
- [X] importing wiki categories and page edge relation to Neo4j.
- [X] 上传完备份我再建边.
- [X] 加一个loop detection算法，现在只做了direct cycle detection algorithm.
  - [X] use networkx to detect loop.
  - [X] it's too hard to detect cycles in the whole graph. Starting in a small categories.
  - [X] don't add direct loop edges to a graph, find_cycles will only show such direct loop. save this graph.
  - [X] remove direct cycle and full cycle at a node completely.
- [X] skill GID generating in Python.
- [X] 把节点上传. wiki 上传了1040229 page, 381475 categories.
- [X] train word2vec model based on GID.
- [X] import edge, loop detecting for linking categories nodes.
- [X] fetching pages binary content via GID.
- [X] test response GID, same with GID saved on Chrome.
- [X] test fetching binary text with GID.
- [X] extract page to neo4j from xml file.
businessID.domain = https://zh.wikipedia.org/wiki/:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
node.url = encoded_url
- [X] import category to neo4j from sql file.
businessID.domain = https://zh.wikipedia.org/wiki/Category:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
- [X] double check GID with Shenbing after importing a small set of page.
- [X] import page from mongo to neo4j.
- [X] backup neo4j after importing categories and page.
- [X] delete edges.
- [X] importing wiki categories nodes and page nodes to Neo4j.
- [X] test importing wiki categories nodes.
- [X] skill_2_graph
=C-M-r= in gs, create 查路径, drag GID: 81F49335AC9C4D84A5F27F7A02AAABBA into the input box, input Parent GID in the parent box.
***** Thomson Reuters Knowledge graph perim
- [ ] read how to use the RFM dataset.
***** relation extraction from training data
- [ ] search paper and public code.
- [X] Stanford NLP relation extraction video.
**** DONE manual import unsaved categories and edges into Neo4j.
CLOSED: [2018-03-09 Fri 15:15]
- [X] find unsaved categories under 金融 category.
- [X] save those to a sql file.
- [X] upload sql file and edge.
*** information extraction system
**** DONE opinion sentiment analysis. <2018-03-09 Fri> - <2018-03-16 Fri>
CLOSED: [2018-03-26 Mon 12:49]
:LOGBOOK:
CLOCK: [2018-03-07 Wed 10:05]--[2018-03-07 Wed 17:57] =>  7:52
CLOCK: [2018-03-02 Fri 09:56]--[2018-03-02 Fri 19:07] =>  9:11
CLOCK: [2018-03-01 Thu 10:38]--[2018-03-01 Thu 12:01] =>  1:23
:END:
***** DONE read_RMDB_table -> NLP_sentiment_analysis -> generate_sentiment_signal.
CLOSED: [2018-03-26 Mon 13:37]
[[https://nlp.stanford.edu/courses/cs224n/2011/reports/nccohen-aatreya-jameszjj.pdf][sentiment prediction]]
- Sentiment analysis 算法.
SVM, HMM, naive bayes, 最大熵, K-NN, Dictionary.
- 爬取Google news, 雪球， 虎嗅， 微信上所有300支股票的文档，再进行sentiment analysis, 结果再排序，选最好的5只。
- [ ] sentiment score做为单因子测试, upload sentiment data to hadoop and test factor in FS.
- [X] read paper *joint extraction of entities and relations*.
- [ ] read paper *Anomalies and Investor Sentiment*.
- [X] 情感分析指标的设计在GS上实现。
- [X] news, market-view articles sentiment analysis.
- [X] 发现2018-02-08, 情感指数0.54，2-9日出现大跌。
能否用这个指数来预警，今天可以扩大一下样本空间，看看上证在1%下跌的情况下前一日的情感指数值是如何变化。
- [ ] search paper and books how to use sentiment analysis.
**** information retrieval system
***** goal
****** question and answering from a document
- what is tha data.
- what is the algorithm.
- what is the conclusion.
****** News summary
- [ ] classify 1 year of analyst research articles.
- [ ] convert PDFs to text files.
- [ ] summerize articles
- [ ] send summary to baidu api article classification.
- [ ] get news summary from webs.
****** syntactic parsing

* GID table
| date             | GID                              | descriptions                                      |
| 2017/6/6	       | 64C9DC1C96FD4A0693305C87E905E039 | 组合                                              |
|                  | 69C3DC21A8E1440AA9C54442AB8A1BD3 | activeX                                           |
|                  | 528FC2E4CC73469496D330AC39E4AE50 | GROUP WEIGHT                                      |
|                  | 05E0CAE034244B75BB8CD5580CEF3F5C | 搭函数                                            |
|                  | 047D8ECFE75E44689D1D10EDF06620BF | FD, portfolio optimization                        |
|                  | 171A21059C62E84D330A4D7B53D72DDC | set target risk and target return                 |
|                  | 3A29CB69BD7879553AAF2EDDD23810F0 | 基准风格因子暴露_python                           |
|                  | E647EFFF80C540A58A2BFC7719658756 | 组合优化FR                                        |
|                  | F7960FEA8B534BE8885639B36C3816A8 | 风格因子暴露 刘鑫                                 |
|                  | A5A7240AE18DC1D9C84A3D7629EC1B63 | STYLE风格因子暴露 刘鑫                            |
|                  | 363F23CF55484C6BB57291A6543385BC | function of generate factors                      |
|                  | 0D5E7E6A76524DC8A6DA5F5AF0BFB3C3 | function of getting factor exposure               |
|                  | B58B375C952C486C80CC94F041B0EF63 | j of getting factor exposure                      |
| <2017-08-11 Fri> | B04D8EC4FC14CBDCCD9FD61B9C831955 | FI optimization                                   |
| <2017-08-11 Fri> | DB7575569F231472384F462E9A0E69AF | j stock portfolio optimization                    |
|                  | 6224F5BAEF1D4BC9BAC9FFE0244357D8 | multiple period optimization                      |
| <2017-08-14 Mon> | DB7575569F231472384F462E9A0E69AF | stocks opts with multi-period exposure constraint |
| <2017-10-17 Tue> | 2D7FFE3045D945B9A84D7CE26329C0D6 | 分类搜索（旧） (吴伟;函数定义;)                   |
|                  | 44DE982C48224407BF421AA865E9D0C0 | 搜索指标  分类搜索（旧） (吴伟;指标;)             |
|                  | A894DF1EA89F4229B04ADECB5D1A0661 | 搭函数                                            |
|                  | E9FA85212E6D4A9C879E36CD7261D84A | 添加Element (吴伟;股票因子（OTV&O是A股股票）;)    |
|                  | BA205C0589AB4F6CA1FCAF885763B927 | 分类搜索（旧）                                    |
|                  | FE1171DA00604BF797EE8D6A2AD63924 | 添加Element (吴伟;函数定义;)                      |
| <2017-10-27 Fri> | 39B85F94CA20442D99C118A6306984F7 | map homepage                                      |
| <2017-10-31 Tue> | 7D13DEA9E6C405B8176961A15D0D4860 | 总市值L                                           |
|                  | 83F882F5CB3AF07D00ADA79F77A7C4DD | A股日涨跌幅_L                                     |
|                  | DAA28C9560375A07DE91319BA0946298 | 资产收益率_TTM_PIT_L                              |
|                  | 08CFD0E436A442E5B48894CFCBACAB2D | factor return covariance                          |
|                  | 22503DED4E434CF09DE6DBAAD8BA717C | factor return specific risk                       |
| <2017-11-13 Mon> | 44CE41A497504EA2A158AC868EDFB48C | risk model compatible                             |
|                  | 1084AEFAC31C8780344A53F281B1D2EF | risk model data preprocess                        |
|                  | 58F613096423BCD84783F165C8918368 | regression                                        |
| <2017-11-14 Tue> | 566186C314C7A760C626249E168D3362 | risk model(extract result)                        |
| <2017-11-16 Thu> | 8C01AABF99EB43F28A10D2FB9E3F9F8C | black litterman function F                        |
| <2017-11-17 Fri> | 46F5B372BFEC95833468B698AF7A8120 | black litterman j                                 |
|                  | C482A7C8AAA93082803F78F03D37F51B | ROE_daily（PIT）     % change                     |
|                  | CCA1D8B93BE90E135FFBBCA59723146A | 盈利ROE_daily_PIT_L                               |
|                  | B8EA49740ACCF83BABAE7168BAA895CC | ROE（daily）                                      |
| <2018-01-16 Tue> | 4CF56BE55CB641DEA428D1C0BBF6BD17 | skill instance example                            |
| <2018-02-23 Fri> | 2F773D63314DD8C30C2C212E0AAA88C7 | join(PIT)                                         |
| <2018-03-05 Mon> | 34B4A9FDF6EF4960A03A87C5A8070AA3 | 搭函数                                            |
| <2018-03-19 Mon> | 2ABBAB65792E46DD9DFAA734A4A6B575 | Ctrl+Shift+s                                      |
|                  |                                  |                                                   |

* other projects
- [X] 搭建基金策略
- [X] 对基金策略进行组合优化
- [X] 分析优化后结果，把组合优化加入流程
- Hidden factor model
ICA
- portfolio optimization based on factor model
- non linear constrain on factor model

- [X] 公司帮忙照看搬家。
8 hours.

* Personal stuff
** Studying
- [X] make studying plan on deep learning and NLP course.
** knowledge database website
- [X] modify python content aligned with python library index.
[[http://usyiyi.cn/translate/python_352/library/index.html][python library]]
- [ ] publish org files to gitbook.
- [ ] Machine Learning

* Errand

* Machine Learning
Principle: research and project oriented, adjusting speed based on progress.

Spend at most 1 week to learn all those, then pick one to go deeper.
** Week 1/2
*** Logistic Regression
*** Regularization
*** Classification
**** Linear Discriminant Analysis

**** Comparison
*** Decision tree and random forest
*** Boosting
*** XGBoost
*** SVM
*** Clustering
*** Bayesian Network
*** LDA
*** HMM
*** Neural Network
** Studying method:
1. model assumption, input data, application, detail.
* Risk model
DEADLINE: <2017-11-14 Tue>
:LOGBOOK:
CLOCK: [2017-11-14 Tue 10:57]--[2017-11-14 Tue 19:30] =>  8:33
:END:
** removing risk model/portfolio optimization for loop
- [ ] remove risk model for loop.
- [X] calculate factor return and residual(specific risk).
  - [X] stock return should add one more column constraint.
  - [X] extract factor exposure and stock return as regression model input X and Y.
  - [X] apply regression function on groupby.
  - [X] change regression input parameter name.
  - [X] regression day by day in a for loop.
- [X] load xarray data.
- read the barra&axioma documents about risk model.
- create charts from the risk model report.
* GS
** Research workflow
DEADLINE: <2017-11-06 Mon>
:LOGBOOK:
CLOCK: [2017-11-06 Mon 10:25]--[2017-11-07 Tue 10:03] => 23:38

#+END:

:END:
1. [X] Search SSRN and find topics
2. [X] Highlight those topics to create topics idea
3. [X] Create topics OSET and Want based on topics idea
   1. Such want can be under project want or under agent want such as SSRN, Journal
4. [X] Create a Want workflow to research the topics, e.g.,
   1. find top school and authors of that topics
   2. find top papers chronologically in that topics
5. [X] Search top papers and websites under the topics
   1. meet the wants in research workflow with top author, school, etc
6. [X] Download those papers and Meet the Want(s) of that topics
7. [X] Create an idea workflow for such topics
   1. define agent behavior under study
   2. define data
   3. models
   4. results
   5. related topics
8. [ ] Create wants of different oset (doc, idea, func instance, func def, oset) for idea workflow
9. [ ] highlights in related papers to meet above wants, create additional f def and f inst oset, then follow GS workflow to create f def, f instance, etc
10. [ ] define function sampling.

** organize OSet structure
DEADLINE: <2017-10-23 Mon>
[[https://docs.google.com/document/d/1eRArN_yGxYnEEbA-AMDAhGOSGSFsL_CvBstbuJ7v4lE/edit]]
- [-] 从Handbook Of Equity Market Anomalies提取因子，策略，转化为能产生alpha的需求。
  - [ ] Analyst-related anomalies
  - [X] GS技术分析因子分类
  - [ ] Accural anomalies
  - [ ] Post-earnings announcement drift
  - [X] GS函数定义机器学习分类，到需求tree
  - [ ] 建立动量需求，搜动量因子满足需求
  - [ ] 盈余惯性
  - [X] 分析师预测和盈利预测
- [ ] 原有因子指标加入新建OSet
- [X] add machine learning functions to statistical inference
- [X] add abnormal factors on GS
- [X] add computer age statistical inference function content on GS
- [ ] strategies
- [X] stocks
- [X] f def
- [X] abnormal factors
** GS workflow

* optimization

** black-litterman model
DEADLINE: <2017-11-01 Wed>
- [X] calculate unconstrainted optimized weight.
bug: inaccurate view will cause extreme result.
- [ ] translate ROE to views.
look ahead 3 months strategy.
- [ ] Omega should be calculated via 2 methods, one is to calculate Omega using LC and Calibration Factor, the other is integrating tilt to new weight w.
- [ ] write/modify code.

- [ ] plugging gs to run with data.
- [ ] check result.
- [ ] prepare presentation of all previous steps.
- [ ] present and get feedback.
- [ ] deploy function definition, function sampling.
<2017-10-01 Sun> - <2017-10-31 Tue>
**- [X] simply use stocks in a sector instead of all sector classes that ignore sector market capitalization.
- [X] prepare data on GS.
- [X] create idea map on GS.
- [X] write pseudo code.
- [X] prepapre industial historical price data, data problem on class market capitalization.
- [X] read pdf/code, take notes.
- [X] present general idea.
vivid views on expected return.
explain exact meaning with each parameter with cases.
search more industrial use cases.
view details.
- [X] write pseudo code, set input parameters.
*** why most of the cases are on assets classes optimization, not on individual stocks.
1. The data points of individual asset are not enough, and are not stable.

2. Sector classes perform stably for a long time.
** cvxportfolio
- [ ] 查看cvxportfolio的使用说明。
- [ ] 在开发的帮助下安装cvxportfolio library，调试。
- [ ] 调试cvxportfolio library，把代码换用该库。
- [ ] 在gs上调试使用cvxpy和cvxportfolio的因子中性程序。
** cvxpy
- [ ] research on soft constraint using cvxpy.
- [ ] 搭建workflow。
- [ ] 多期优化。
** archive
<2017-07-21 Fri> - <2017-09-18 Mon>
- [ ] construnct FS, FR, workflow.
- [ ] soft constraint.
- [ ] worse-case risk analysis.
- [X] add excess single returns and excess cumulative returns on PNLFitness.
- [X] move functions defines osets.
- [X] read future simulation code.
- [X] pack constraint input parameters.
- [ ] DONE multi-period optimization using cvxportfolio.
- [ ] 用多期因子做一下回测看看效果，没有看到在OBJECTIVE FUNCTION里面加TRADE COST LOSS FUNCTION的做法，RETURN, RISK和 COST 不在同一个数量级的.
- [ ] 继续在CVXPORTFOLIO上改用RISK MODEL.
- [ ] 周末把每一期按单期优化处理后得到了新的权重，没有加入预测，上午跑一下看看效果。
- [ ] 把RISK MODEL加到CVXPORTFOLIO。
- [X] GS说明文档，结合以前学到的东西，WORKFLOW, J, FR, FI, 边，等等，重新理解一下。
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] 组合优化当hard constraint无法达成时，relax constraint，结果同时输出fitness。
- [X] mode 参数变成一个枚举值。
- [ ] gs输入signature需要指定 oset.
oset组织不完整，postpone.
- [X] risk model api.
- [-] risk model api test.
  - [X] change factor return dataframe name to b char array.
  - [ ] risk model api function.
    - factor exposure(panel)
      size(1536*8*2885).
      items: datetime index
      major axis: factors
      minor axis: symbols
    - sigma(panel)
      size(1536*35*2885)
    - specific risk(panel)
      panel size is too large for calculation(1536*2885*2885).
    - covariance matrix(panel)
    - factor return(dataframe)
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] test the api according to my own requirement.
  - [ ] risk model class function.
    - factor exposure(panel)
      size(1536*8*2885).
      items: datetime index
      major axis: factors
      minor axis: symbols
    - sigma(panel)
      size(1536*35*2885)
    - specific risk(panel)
      panel size is too large for calculation(1536*2885*2885).
    - covariance matrix(panel)
    - factor return(dataframe)
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] which industry does the benchmark return come form?
using the robust linear regression to estimate the coefficient and do the ranking.
consider use the lasso regression maybe a better choice.
- [X] 将新写的api代码应用到gs.
- [X] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [X] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] Paper on industry classification.
- [X] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [X] 将 group constaint 的条件泛化。
- [X] test on gs.
- [X] select assets by returns and volatility according to target mode.
- [X] soft constraint based on penalty function.
- [ ] Paper on industry classification.
1. portfolio optimization
- [ ] write pseudo code.
- [ ] fix const variable issue for target mode naming.
- [ ] write fd for asset constraint on gs.

2. soft constraint
- [ ] write pseudo code.
- [ ] move soft constraint test on gs.

3. industry classification paper
finish this paper and write summary.
find a dateset for industry, product, financial statement, etc, extract nouns from text matching these classes.
duplicate classification algorithm.

- [X] fix const variable issue for target mode naming.
- [X] write fd for asset constraint on gs.
- [X] write pseudo code.
- [ ] move soft constraint test on gs.
- [X] finish text-based network industry classification(TBNIC) and summarizing.
- [X] deploy the constraint function on gs.
- [ ] collect product noun words from description using NLP.
- [ ] get factors list GID
- [ ] review optimization function map, functions signature, pseudo code.
- [ ] collect product noun words from description using NLP.
- [X] programe the pair input data structure.
- [X] make this structure available on GS.
- [X] OTVV的数据结构是通过上传还是写python function还在试验看哪种方式更高效，不过我自己已经把这种函数写出来了。
- [ ] put the constraint GS, review code.
- [X] setup the constraint, covariance matrix, sigma, delta on GS.
- [ ] add industry to portfolio bug: output of this function will ignore assets whose industry information is missing.
- [X] 数据传入没有问题，在计算时出现bug，估计是传入数据dataframe column的顺序出现问题，debug。
fix by fillna with 0 in the factor exposure dataframe.
- [X] deploy the whole function on Jupyter and gs.
- [X] we have a position limit option, length is x.
asset information is a diagonal matrix coming from the input portfolio, whose size is n*n, weight variable should be an m*1 vector, so select m assets from asset information dataframe first.
we can create two constraint with the formula (wT*M1)T>= V_1, (wT*M1)T <= V_2.
- [X] calculate the benchmark factor exposure as the reference j to the exposure constraint input.
- [ ] debug the optimization with exposure constraint.
BUG: asColumnTab will get wrong data if the content in the GftTable is a dataframe with datetimeindex.
- [X] debug the optimization with exposure constraint.
- [X] 程序已经调通，加入factor exposure constraint找到最优解
  1. check the calculation of the factor exposure.
  2. 放松约束条件.
  3. [X] bug found: group constraint loading dataframe index order should be aligned within calculation.
- [-] test:
  - [X] original portfolio is constructed by 59 assets in a single period. gid: 3C9221A6176B231A095969B6BFE8BFB2.
  - [ ] original portfolio is constructed by all hs300 components.
  - [ ] industry weight constraint match the benchmark weight.
- [X] profile the program.
90% of program running time is spent on loading data and decompresssing data.
- [ ] present &review project.
- [ ] portfolio construction with risk-parity model.
- [-] test:
  - [X] original portfolio is constructed by 59 assets in a single period. gid: 3C9221A6176B231A095969B6BFE8BFB2.
  - [X] original portfolio is constructed by all hs300 components.
  - [ ] industry weight constraint match the benchmark weight.
- [X] industry weight constraint match the benchmark weight.
- [ ] pack functions to modules.
- [ ] follow up correction.
- [ ] pack functions to modules.
- [ ] follow up correction.
* future simulation                                     :future:optimization:

 <2017-09-19 Tue> - <2017-10-01 Sun>
- [X] buy and hold mode on future backtesting.

- [X] revise pnl fitness function, rename previous pnlfitness function name to PnLAndHoldingFitness, create another two seperate functions PnLFitness and HoldingFitness respectively.

- [X] export data from database.
- [X] roll over between two contracts to get continious contract data.

- [X] create rolling strategy, return rolling position.

- [X] create continuous contracts.

- [X] learn function define, function sampling instance, wants, search strategy run on GS.
- [X] add trading volume and other value to the continuous contract.
- [X] remove unnecessary input of the parsing function.

- [X] learn creating function sampleing.

- [X] create long/short strategy, return neutral position.
- [ ] deploy portfolio optimization on function sampling workflow.
- [ ] create position signal via continous contract data.
- [ ] to get actual position from simulation position.

- [ ] calculate PnL using future rollover position, long/short position, continuous position.

- [ ] modify simulation code referred to stocks simulation.
- [ ] adjust input signatures.
- [ ] future simulation requested function:
- calendar spread trading.
buy current month expired contract.
short next month expired contract.

- rolling on constant maturity.

- trend following on dominant contract.
moving average crossing over on the continous contract.
- [X] future simulation pseudo code.
#+BEGIN_SRC python
 def OnData(self,slice):
        if not self.Portfolio.Invested:
            for chain in slice.FutureChains:
                 # Get contracts expiring no earlier than in 90 days
                contracts = filter(lambda x: x.Expiry > self.Time + timedelta(90), chain.Value)

                # if there is any contract, trade the front contract
                if len(contracts) == 0: continue
                front = sorted(contracts, key = lambda x: x.Expiry, reverse=True)[0]
                self.MarketOrder(front.Symbol , 1)
        else:
            self.Liquidate()
#+END_SRC
 requested function:
- [ ] calendar spread trading.

calculate the position value based on continious contract and actual contract

buy current month expired contract.
short next month expired contract.

- [ ] rolling on constant maturity.

calculate the position value based on continious contract and actual contract

- [ ] trend following on dominant contract.

moving average crossing over on the continous contract.
- [X] range osets.
- [X] parse future continious contract data.
- [ ] simple mode test with real data.
* Issues:
- 数据预览错误
#+BEGIN_SRC txt

Func[1EA1E9396E574AF4B6D39EC5BCCD1B61.py] Seq:3in[3CEA6605E3F63191F225D45C21395C41#1],error:Python error:
TypeError: a bytes-like object is required, not 'str'
Traceback (recent call last):
File "<string>", line 1, in <module>
File "/opt/GFTCacheServer/PythonScripts/lib/gftTools/gftIO.py", line 823, in transformOutputWithTypeGiven
return transformOutput(obj)
File "/opt/GFTCacheServer/PythonScripts/lib/gftTools/gftIO.py", line 861, in transformOutput
return _wideFrameToTuple(obj.matrix)
File "/opt/GFTCacheServer/PythonScripts/lib/gftTools/gftIO.py", line 980, in _wideFrameToTuple
ls_index = transformOFromPandas2Output(objT.index.values)
File "/opt/GFTCacheServer/PythonScripts/lib/gftTools/gftIO.py", line 332, in transformOFromPandas2Output
ret = ndOutput.astype(oDataType)

#+END_SRC
输出数字类型只能是float, 结构为TV时column name必须为GID，否则需要reset index.
- 输出string
必须是binary,不能是string
使用gftIO.gidStrArray2CharArray
* Paper Summary
#+INCLUDE: "notes/PaperSummary.org"
* Instruction on how to use emacs org mode to orginize calendar.

- insert current date
=C-c .= <2017-08-11 Fri>

- insert current date and time
=C-u C-c .= <2017-08-11 Fri 16:06>

- set deadline for an item
=C-c C-d=

- clock log in
=C-c C-x C-i=

- clock log out
=C-c C-x C-o=

- clock cancle
=C-c C-x C-q=
- org-mode
#+BEGIN_SRC text
#+TOC: headlines 3
#+INCLUDE: "./CS/Python/python2.org" :minlevel 1
#+BEGIN_COMMENT
Each major section of this document is defined in its own file.

You can jump to each file by moving the cursor on an "#+include" line
and typing =C-c '= Note: There is *no requirement* to split, but large
org-mode files can become quite slow to edit, so separate sections help
keeping things fluid...
#+END_COMMENT
#+LINK: gh    https://github.com/
#+LINK: rfc   https://tools.ietf.org/html/
#+LINK: thing https://github.com/thi-ng/
#+LINK: wiki  https://en.wikipedia.org/wiki/
#+END_SRC
