#+TITLEs: Document title
#+LANGUAGE: en
#+OPTIONS: toc:nil h:4 html-postamble:nil html-preamble:t tex:t f:t
#+OPTIONS: prop:("VERSION")
#+HTML_DOCTYPE: <!DOCTYPE html>
#+HTML_HEAD: <link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="css/style.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./styles/demo/css/style.css"/>

#+HTML: <div class="outline-2" id="meta">
| *Author* | {{{author}}} ({{{email}}})    |
| *Date*   | {{{time(%Y-%m-%d %H:%M:%S)}}} |
#+HTML: </div>

* Instruction on how to use emacs org mode to orginize calendar.

- insert current date
=C-c .= <2017-08-11 Fri>

- insert current date and time
=C-u C-c .= <2017-08-11 Fri 16:06>

- set deadline for an item
=C-c C-d=

- clock log in
=C-c C-x C-i=

- clock log out
=C-c C-x C-o=

- clock cancle
=C-c C-x C-q=
- org-mode
#+BEGIN_SRC text
#+TOC: headlines 3
#+INCLUDE: "./CS/Python/python2.org" :minlevel 1
#+BEGIN_COMMENT
Each major section of this document is defined in its own file.

You can jump to each file by moving the cursor on an "#+include" line
and typing =C-c '= Note: There is *no requirement* to split, but large
org-mode files can become quite slow to edit, so separate sections help
keeping things fluid...
#+END_COMMENT
#+LINK: gh    https://github.com/
#+LINK: rfc   https://tools.ietf.org/html/
#+LINK: thing https://github.com/thi-ng/
#+LINK: wiki  https://en.wikipedia.org/wiki/
#+END_SRC
* Workflow
1. search.
2. read pdf/code, take notes.
3. present general idea.
4. write pseudo code, set input parameters.
5. write/modify code.
6. plugging gs to run with data.
7. check result.
8. prepare presentation of all previous steps.
9. present and get feedback.
10. deploy function definition, function sampling.

* Social Science Paper Structure
** Introduction
State the contribution.
- Problem
Benefits to be gained by the research or why the problem has not been solved yet.
- Purpose
What you are trying to achieve.
- Own position
Overall view of the paper. A quick summary of the form that the parts of the research paper is going to take.
** Literature Review
- Previous research
- Contribution in more detail
Differences from what has been done before: Is it new data? A new model? A
new identication strategy? Are you answering a question more broadly/specially? Specifically
comparing how you are improving on a previous paper is useful.
- Method/Model
Exact design and methodology used to perform the research.
- Data
Describe the name and source of the data you are using and the period it covers.

Present (relevant) descriptive statistics of the data.
- Formulate
A specific question problem, or conjecture, and to describe the approach you will take to answer, solve, or test it. Often, this will take the form of an empirical hypothesis: social security depresses personal savings;
** Results
Numerical results and data, tables or graphs of data.
** Discussion
Elaborate on your findings, and explain what you found, adding your own personal interpretations.
** Conclusion
Emphasizes the importance of the results in the field, and ties it in with the previous research.


* GID table
| date             | GID                              | descriptions                                      |
| 2017/6/6	       | 64C9DC1C96FD4A0693305C87E905E039 | 组合                                              |
|                  | 69C3DC21A8E1440AA9C54442AB8A1BD3 | activeX                                           |
|                  | 528FC2E4CC73469496D330AC39E4AE50 | GROUP WEIGHT                                      |
|                  | 05E0CAE034244B75BB8CD5580CEF3F5C | 搭函数                                            |
|                  | 047D8ECFE75E44689D1D10EDF06620BF | FD, portfolio optimization                        |
|                  | 171A21059C62E84D330A4D7B53D72DDC | set target risk and target return                 |
|                  | 3A29CB69BD7879553AAF2EDDD23810F0 | 基准风格因子暴露_python                           |
|                  | E647EFFF80C540A58A2BFC7719658756 | 组合优化FR                                        |
|                  | F7960FEA8B534BE8885639B36C3816A8 | 风格因子暴露 刘鑫                                 |
|                  | A5A7240AE18DC1D9C84A3D7629EC1B63 | STYLE风格因子暴露 刘鑫                            |
|                  | 363F23CF55484C6BB57291A6543385BC | function of generate factors                      |
|                  | 0D5E7E6A76524DC8A6DA5F5AF0BFB3C3 | function of getting factor exposure               |
|                  | B58B375C952C486C80CC94F041B0EF63 | j of getting factor exposure                      |
| <2017-08-11 Fri> | B04D8EC4FC14CBDCCD9FD61B9C831955 | FI optimization                                   |
| <2017-08-11 Fri> | DB7575569F231472384F462E9A0E69AF | j stock portfolio optimization                    |
|                  | 6224F5BAEF1D4BC9BAC9FFE0244357D8 | multiple period optimization                      |
| <2017-08-14 Mon> | DB7575569F231472384F462E9A0E69AF | stocks opts with multi-period exposure constraint |
| <2017-10-17 Tue> | 2D7FFE3045D945B9A84D7CE26329C0D6 | 分类搜索（旧） (吴伟;函数定义;)                   |
|                  | 44DE982C48224407BF421AA865E9D0C0 | 搜索指标  分类搜索（旧） (吴伟;指标;)             |
|                  | A894DF1EA89F4229B04ADECB5D1A0661 | 搭函数                                            |
|                  | E9FA85212E6D4A9C879E36CD7261D84A | 添加Element (吴伟;股票因子（OTV&O是A股股票）;)    |
|                  | BA205C0589AB4F6CA1FCAF885763B927 | 分类搜索（旧）                                    |
|                  | FE1171DA00604BF797EE8D6A2AD63924 | 添加Element (吴伟;函数定义;)                      |
| <2017-10-27 Fri> | 39B85F94CA20442D99C118A6306984F7 | map homepage                                      |
| <2017-10-31 Tue> | 7D13DEA9E6C405B8176961A15D0D4860 | 总市值L                                           |
|                  | 83F882F5CB3AF07D00ADA79F77A7C4DD | A股日涨跌幅_L                                     |
|                  | DAA28C9560375A07DE91319BA0946298 | 资产收益率_TTM_PIT_L                              |
|                  | 08CFD0E436A442E5B48894CFCBACAB2D | factor return covariance                          |
|                  | 22503DED4E434CF09DE6DBAAD8BA717C | factor return specific risk                       |
| <2017-11-13 Mon> | 44CE41A497504EA2A158AC868EDFB48C | risk model compatible                             |
|                  | 1084AEFAC31C8780344A53F281B1D2EF | risk model data preprocess                        |
|                  | 58F613096423BCD84783F165C8918368 | regression                                        |
| <2017-11-14 Tue> | 566186C314C7A760C626249E168D3362 | risk model(extract result)                        |
| <2017-11-16 Thu> | 8C01AABF99EB43F28A10D2FB9E3F9F8C | black litterman function F                        |
| <2017-11-17 Fri> | 46F5B372BFEC95833468B698AF7A8120 | black litterman j                                 |
|                  | C482A7C8AAA93082803F78F03D37F51B | ROE_daily（PIT）     % change                     |
|                  | CCA1D8B93BE90E135FFBBCA59723146A | 盈利ROE_daily_PIT_L                               |
|                  | B8EA49740ACCF83BABAE7168BAA895CC | ROE（daily）                                      |
| <2018-01-16 Tue> | 392482970E904D11190D208B7C22874A | skill instance example                            |
|                  |                                  |                                                   |

* other projects
- [X] 搭建基金策略
- [X] 对基金策略进行组合优化
- [X] 分析优化后结果，把组合优化加入流程
- Hidden factor model
ICA
- portfolio optimization based on factor model
- non linear constrain on factor model

- [X] 公司帮忙照看搬家。
8 hours.

* Personal stuff
** knowledge database website
- [X] modify python content aligned with python library index.
[[http://usyiyi.cn/translate/python_352/library/index.html][python library]]
- [ ] publish org files to gitbook.
- [ ] Machine Learning
* Errand
* Deep Learning
** NaturalLanguageProcessing

*** Knowledge Graph
**** create wiki knowledge graph
DEADLINE: <2018-01-23 Tue>
:LOGBOOK:
CLOCK: [2018-01-30 Tue 19:24]--[2018-01-30 Tue 20:52] =>  1:28
CLOCK: [2018-01-30 Tue 13:11]--[2018-01-30 Tue 17:45] =>  4:34
CLOCK: [2018-01-30 Tue 10:58]--[2018-01-30 Tue 12:39] =>  1:41
CLOCK: [2018-01-29 Mon 10:36]--[2018-01-29 Mon 20:18] =>  9:42
CLOCK: [2018-01-26 Fri 09:49]--[2018-01-26 Fri 11:16] =>  1:27
CLOCK: [2018-01-25 Thu 10:32]--[2018-01-25 Thu 15:59] =>  5:27
CLOCK: [2018-01-24 Wed 13:58]--[2018-01-24 Wed 19:40] =>  5:42
CLOCK: [2018-01-23 Tue 13:47]--[2018-01-23 Tue 15:47] =>  2:00
CLOCK: [2018-01-23 Tue 09:56]--[2018-01-23 Tue 12:05] =>  2:09
CLOCK: [2018-01-22 Mon 16:45]--[2018-01-22 Mon 19:34] =>  2:49
CLOCK: [2018-01-22 Mon 13:38]--[2018-01-22 Mon 14:28] =>  0:50
CLOCK: [2018-01-22 Mon 10:00]--[2018-01-22 Mon 12:29] =>  2:29
:END:
- [ ] skill GID generating in Python.
- [ ] 把节点上传. wiki 上传了1040229 page, 381475 categories.
- [ ] 上传完备份我再建边.
- [ ] 加一个loop detection算法，现在只做了direct cycle detection algorithm.。
- [X] train word2vec model based on GID.
- [X] import edge, loop detecting for linking categories nodes.
- [X] fetching pages binary content via GID.
- [X] test response GID, same with GID saved on Chrome.
- [X] test fetching binary text with GID.
- [X] extract page to neo4j from xml file.
businessID.domain = https://zh.wikipedia.org/wiki/:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
node.url = encoded_url
- [X] import category to neo4j from sql file.
businessID.domain = https://zh.wikipedia.org/wiki/Category:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
- [X] double check GID with Shenbing after importing a small set of page.
- [X] import page from mongo to neo4j.
- [ ] backup neo4j after importing categories and page.
- [ ] importing wiki categories and page edge relation to Neo4j.
- [ ] read external sql file into pandas.
- [ ] find page title from page id.
- [X] delete edges.
- [X] importing wiki categories nodes and page nodes to Neo4j.
- [X] test importing wiki categories nodes.
**** entity relationship
:LOGBOOK:
CLOCK: [2018-02-05 Mon 09:54]
CLOCK: [2018-02-02 Fri 10:23]--[2018-02-02 Fri 17:25] =>  7:02
CLOCK: [2018-02-01 Thu 15:42]--[2018-02-01 Thu 20:26] =>  4:44
CLOCK: [2018-02-01 Thu 10:00]--[2018-02-01 Thu 11:52] =>  1:52
CLOCK: [2018-01-31 Wed 15:16]--[2018-01-31 Wed 19:29] =>  4:13
CLOCK: [2018-01-31 Wed 10:53]--[2018-01-31 Wed 12:08] =>  1:15
CLOCK: [2018-01-31 Wed 10:09]--[2018-01-31 Wed 10:52] =>  0:43
CLOCK: [2018-01-30 Tue 17:45]--[2018-01-30 Tue 18:35] =>  0:50
:END:
- [X] pseudo code of node, edge upload.
- [ ] summerize nlp library extraction result comparison in jupyter notebook.
- [ ] from survey paper -> book -> reference paper -> citation paper -> application -> open source library.
- [ ] find the difference of attirbutes not in Juyuan database, searching for useful information.
- [ ] entity extraction resolution detection like author, publisher.
- [ ] listed companies triples importing to neo4j.
- [X] extract the triple relation information.
- [X] visualization of triples.
- [ ] NER of all listed company pages content what analyst care about: 有关内容包括：主要产品，产业链，竞争对手，合作伙伴，投资方，key person(如公司跟投资人关联), 上市交易所，sentiment, 分析师评级，评论，公司重大公告.
- [ ] extract RDF company triples.
- [X] Chinese NER model is missing, searching. models are in the Chinese model jar file.
- [X] test stanford-corenlp to extract keywords and NER en.
- [X] compare nlp libraries.
- [X] extract Named Entity Recognition.
- [ ] read Q&A knowledge graph paper.
**** News summary
- [ ] get news summary from webs.
**** syntactic parsing
***** opinion extraction system
sentiment analysis.
***** information retrieval system
Q&A。
用了什么分析方法，数据。
**** Part 1: Knowledge Graph Primer
- What is a Knowledge Graph?
- Why are Knowledge Graphs Important?
- Where do Knowledge Graphs come from?
- Knowledge Representation Choices
- Problem Overview
**** Part 2: Knowledge Extraction from Text
***** NLP Fundamentals
- Tokenization, chunking
- Part-of-speech tagging
- Named entity recognition
- Dependency parsing
- Entity resolution, coreference, and linking
***** Information Extraction
- Defining knowledge domains
- Learning knowledge extractors
- Scoring extracted knowledge
- Categories of IE techniques
- Compositional models: Knowledge fusion
- IE systems in practice
**** Part 3: Knowledge Graph Construction
***** Graph construction overview
***** Probabilistic Models
- Motivation
- Graphical models
- Random walk approaches
***** Embedding Techniques
****** Relation extraction techniques
- Matrix factorization
- Embedding entity pairs
****** Graph completion techniques
- Tensor factorization
- Entity and relation embeddings
- Compositional models
**** Part 4: Critical Overview and Conclusion
- Summary
- Success stories
- Datasets, tasks, softwares
- Exciting active research
- Future research directions
*** Part-Of-Speech Tagger (POS Tagger), Named-entity recognition (NER), SEMPRE: Semantic Parsing with Execution with spacy, stanford coreNLP.
:LOGBOOK:
CLOCK: [2018-01-26 Fri 15:04]--[2018-01-26 Fri 17:11] =>  2:07
CLOCK: [2018-01-26 Fri 11:16]--[2018-01-26 Fri 13:18] =>  2:02
:END:
- [ ] read documentation.
- [ ] how to integrate NLP with gs.
- [-] Pipeline:
  - [X] choose category and depth in GS skill ->
  - [X] return pages GID ->
  - [ ] fetch paper via GID in python ->
  - [ ] parsing, tokenization -> jieba.py, preprocessing.utils.py
    - [ ] tagger(Assign part-of-speech tags), spaCy, jieba.py, coreNLP,
    - [ ] parser(Assign dependency labels), spaCy, coreNLP, snowNLP
    - [ ] NER(Detect and label named entities),spaCy, coreNLP, snowNLP
    - [ ] keywords extraction, snowNLP.py,
    - [ ] topic classification, extraction, gensim.py(LDA),
    - [ ] sentiment analysis, snowNLP.py, coreNLP,
- [ ] IEPY
*** Chinese wiki model.
- [X] train financial fields model(58+ categories).
- [X] use similarity distance to find the nearest category of target words.
- [X] similarity test on specific model.
- [X] add all pages title to jieba dict.
- [X] 中文短语处理，当短语不存在词汇库中时，拆开成词输入到模型。
- [X] preprocessing workflow.
  英文text preprocessing需要的注意一些点，及应提供的选择
  1. cut段落或文章
  2. phrase是否进行转换
  3. 停词(a, the, of, that, this, he, I...)是否保留
  4. 数字是否转为英文单词, 中间有数字的单词是否保留(th8)
  5. 提取词干（时态转换，单复数单词转换）
  6. 标点（撇号'，所有格,缩写如don’t），符号（%,#,&,?,@,\,/,",是否保留）
  7. 大小写转换（句首大写转小写，保留全部大写词，专有名词首字母大写保留）

  中文分词（主要利用结巴分词）
  - [X] 1. cut段落或文章
  - [X] 2. 去停词
  - [X] 去标点符号
  - [X] 去数字
- [ ] word2vec fast text comparison.
- [ ] fix zhwiki to mongodb words count.
- [X] compare the training results with or without stopwords.
- [X] demo code.
- [X] visualize & compare results.
- [X] create index for zhwiki.
- [X] test model.
- [X] assign wiki pages extraction task.
- [X] insert Chinese wiki to mongo, transform traditional Chinese to simple Chinese.
- [X] get rid of the stopwords.
- [X] retrie Chinese financial wiki pages from mongo and train.
*** entity relation extraction
- [ ] company relation
- [ ] analyst relation
*** Word Embedding(Word2Vec)
:LOGBOOK:
CLOCK: [2017-12-22 Fri 15:44]--[2017-12-22 Fri 19:21] =>  3:37
CLOCK: [2017-12-14 Thu 10:04]--[2017-12-14 Thu 12:01] =>  1:57
CLOCK: [2017-12-13 Wed 10:04]--[2017-12-13 Wed 15:45] =>  5:41
CLOCK: [2017-12-05 Tue 11:46]--[2017-12-05 Tue 12:04] =>  0:18
CLOCK: [2017-11-27 Mon 10:28]--[2017-11-27 Mon 12:02] =>  1:34
CLOCK: [2017-11-21 Tue 09:07]--[2017-11-21 Tue 15:05] =>  5:58
:END:
- [ ] pack pages into a corpus.
- [ ] compare cutting on paragraph and document.
- [ ] train few files to see if there's repeat training on word2vec.
- [ ] extract data from financial documents — usually PDFs — in an automated way, and to produce “better-than-human” analyses. extract data from tables and text.
- [ ] train function names based on wiki pages on functions, models, and python/matlab/sas/cpp-reference manuals, function names and function descriptions, excel formula, VBA, VB, guass, whatever software which has a function dictionary and manual.
- [ ] retrieve pages title and id under categories from mysql.
- [ ] LSA or LDA analysis on unstructured text, which will give a clustering of words on every topic.
- [ ] visualize vocabulary embedding using t-SNE which project embedding vectors into 2-D surface from an proper perspective using tensorboard locally which can ignore uploading to projector online.
- [ ] create LSTM networks on xarray data.
- [X] create test program to run word embedding, to visualize output.
- [ ] *What is fueling heavy investment in machine learning in the financial industry and how does it fit into customers’ workflows?*
  A lot of our customers’ workflows are being automated, entirely or partially. What they’re doing today is more on the cognitive side: strategy and portfolio selection, formulating the investment theses, etc. People are trying to solve many, many problems in finance using these methods, because they allow for the building of more sophisticated intelligence into trading and client facing workflows. These methods can improve efficiency, or, crucially, allow us to approach problems which heretofore were intractable – due to complicated interactions in the data, complexity of the problem, availability of data or computational resources, and so on.
- [X] provide xarray data to Zhou.
- [X] provide Sun Chinese wiki.
- [X] network Bloomberg about tensorflow.
- [X] retrieving speed test from mongodb.
- [X] test case on finance domain word embedding prediction.
- [X] dumping wiki pages to mongodb.
- [X] testing GPU server.
- [X] configuring deep learning hardware, operation system, software.
- [X] test sets simularity, A-B=C-D?, A+B=?
- [X] incremental training finance pages based via online training.
  online training can not continue missing frequency in pretrained google binary file.
- [ ] cut/training Chinese osets words into files.
- [ ] compare similarity between category and end-to-node oset element.
- [ ] compare the results from GS searching engine and word embedding.
- [ ] import xml pages to elasticsearch.
- [X] clustering categories by word embedding, osets, idea.
  To calculate the similarity matrix between all 160706 vocabulary in RAM, 160706 *160706 *4(bytes)/1024(bytes)/1014(bytes)=99491MB will be needed.
- [X] use [[http://www.cis.lmu.de/pub/phraseEmbedding.txt.bz2][phrase embedding]] as test.
  better phrasing results.
- [X] take a look at cite space iii.
- [X] test word2vec model from finance.
- [X] cut paragraph to short sentences, then phrase.
- [X] phrase text8
- [X] train phrasing sentences word2vec model.
- [X] phrase detection with google pretrained vectors.
- [X] find available library to extract wiki content.
- [X] find all page titles from level 5 finance sub-categories.
- [X] extract page section from wiki xml file.
- [ ] parse Chinese wiki, remove stopwords.
- [X] model wiki token corpus.
- [ ] [[https://github.com/ryankiros/skip-thoughts][skip-thought]].
- [X] find corporate finance/mba questions corpos.
- [ ] read A primer on Neural Network Models.
- [X] tensorflow structure.
- [X] train word2vec model.
- [X] test finding similar words from Wiki corpus.
- [X] download wiki xml file.
- [X] transfer wiki xml file to text format.
- [X] load pre-trained vector matrix, predict the context using a word based on the Skip-Gram model.
- [X] overview of word2vec, why does it work.
- [ ] video explained by Xin Rong.
- [ ] forward propagation vs backward propagation, CNN explained by Andrew Ng.
- [X] paper word2vec Parameter Learning Explained.
- [X] understand Tensorflow Word2Vec example.
- [X] build a backward propagation network.
- [ ] fi or function def from output of wants whose idea word2vec is close to target want's idea.
建一个想法，根据这个想法找到匹配的FI, or FD. 例如，建一个optimize需求，自动推荐black litterman model, or markowitz mean/variance model.
- [ ] fi and its function def whose word2vec is close to word2vec of function instances of current function def to be built.
当前FI,查找相关的下一步FI.
*** Personalized Intelligent Agents(PIA)
**** Objective:
***** Short Term
- make traditional and mature NLP model customizable visually by end users at multiple levels e.g., training data, model parameter.
- incorporate customizable NLP signals into both qualitative and quantitative investment strategies and research process.
- train specific field category model. get the related category tree, use regular expression to get responding categories from the wiki xml file.
- choose model using most related model, use wiki category relation similarity to choose model.
- Hot-word trending analysis on various topics, technology, industrial sectors, analysts comment, company announcement, Deep Learning authors comments, popular topics, papges, etc. Scrape the historical archives of web text in order to get for each post the following information: date, keywords, text.
- Pipeline: choose category and depth in GS skill -> return pages GID -> fetch paper via GID in python -> text processing -> word embedding models, entity extraction, classification, topic extraction, relation extraction, sequence to sequenc, sentiment analysis -> help users to select X,y.
****** 智能投顾
基于金融理财产品知识图谱的语义查询。知识图谱从互联网上公开信息构建，并支持如“高收益”这样的语义搜索。
****** 金融新闻Bots
实时监控各大金融新闻网站的数据，构建了一个监控平台。 根据用户输入的自然语言问题，自动推送与其相关的新闻，并标记其中和用户输入问题相关的段落。
****** 知识数据分析
通过知识图谱相关技术从招股书、年报、公司公告、券商研究报告、新闻等半结构化表格和非结构化文本数据中批量自动抽取公司的股东、子公司、供应商、客户、合作伙伴、竞争对手等信息，构建出公司的知识图谱。

在某个宏观经济事件或者企业相关事件发生的时候，券商分析师、交易员、基金公司基金经理等投资研究人员可以通过此图谱做更深层次的分析和更好的投资决策，比如在美国限制向中兴通讯出口的消息发布之后，如果我们有中兴通讯的客户供应商、合作伙伴以及竞争对手的关系图谱，就能在中兴通讯停牌的情况下快速地筛选出受影响的国际国内上市公司从而挖掘投资机会或者进行投资组合风险控制。
[[./img/knowledge_graph_company.jpg]]

***** Long Term
- context sensitive, Personalized Intelligent Agents, forming an eco-system collaborating, competing and evolving together on behalf of their owners.
- Graph2vec, 把word2vec应用在用户workflow使用序列上，根据用户使用app/func的顺序，把app/func看做单词，也是可以形成这样的序列数据，进而训练处每个app对应的向量。利用这个向量计算app之间的相似度，把真正内容相关的app聚合在一起，推荐给用户。
- 关系挖掘.
- Event-Driven Stock prediction, using sentiment analysis on the web-crawling tokens/documents with doc2vec.
https://nlp.stanford.edu/courses/cs224n/2011/reports/nccohen-aatreya-jameszjj.pdf

| 百科类问句     | 什么是ST股票 什么是向后复权                                           |
| 数据类问句     | 2010年6月26日钢铁报价是多少 三一重工2009年收益是多少                  |
| 事件类问句     | 江苏宏宝什么时候复牌 苏宁电器是哪年上市的                             |
| 逻辑推理类问句 | 预测上海能源本月底的走势 长电科技配股是利好还是利坏                   |
| 选股类问句     | 每股收益在0.2-0.3之间\且市盈率小于30的股票 浙江省市盈率小于20倍的公司 |
- NLP workflow pipeline.
**** Model
- State of the Art NL models based on DNN
  - CBOW
  - Skip gram
  - Sequence to sequence
  - LSTM
  - Convolutional Network Sentence classification
- Libraries
  - Gensim
  - SPACY
  - IEPY
  - Stanford coreNLP
  - NLTK
**** Data
***** Text Labeled with (semi) structured data
- [ ] company filings text, filing classification, event time series data (attributes manually extracted)
- [ ] finance conference scripts.
- [ ] corporate telephone comference notes, memo.
- [ ] company filings text, company industry classification
- [ ] analyst research reports, report classification, analyst ranking
- [X] wiki page titles and page text, wiki category
- [ ] textbook or CFA chapter name, chapter exercise questions, multiple choice questions
- [ ] SSRN paper keyword, category, title, abstract, data, model and conclusion
***** Text Unlabeled
- [ ] analyst research reports
- [ ] textbooks
**** Applications
- recommendation of context sensitive synonyms.
- match search keyword, event, data with theory, model.
- event-driven stock prediction.
*** Graph Convolutional Neural Network
**** Objective:
***** Short Term
- Apply CNN's efficient compositional knowledge extraction ability to graph data, more structured than natural language, image and sound
***** Long Term
- Apply RL on top of graph CNN to automatically ACTIVELY build models, workflows, contracts, investment, instead of just passive prediction without market impact
**** Model
- Spectral Graph Theory based CNN on graph data
**** Data
- Text Like Graph Data, where nodes are discretely different
 - function network in GS, model architecture
 - task network in GS, user behavior
 - supply demand product transaction network in real economy
 - buy sell financial instrument transaction network in financial market
- Image Like Graph Data, where nodes are homogeneous with continuous attributes
 - network of portfolios of different gain and loss, alpha, beta, etc
 - (social) network of analysts, traders, reporters, officials, etc of different level of influences
 - network of NN nodes with different weights
**** Applications
- Graph Classification and Identification
- Graph Simplification and Simulation

*** TODO Building the Wikipedia Knowledge Graph in Neo4j
:LOGBOOK:
CLOCK: [2017-11-15 Wed 14:01]--[2017-11-15 Wed 16:04] =>  2:03
:END:
[[file:/home/weiwu/website/leolle.github.io/CS/MachineLearning/NaturalLanguageProcessing.org][NLP]]
- [X] wiki SQL database links graph.
- [X] pulling wiki knowledge categories(id), pages(id) and relations to local csv, sql file.
**** Data dumps/Import
- methods
[[https://meta.wikimedia.org/wiki/Data_dumps]]

[[https://meta.wikimedia.org/wiki/Data_dumps/Import_examples]]

[[https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/]]
- tools
[[http://wikipapers.referata.com/wiki/List_of_visualization_tools]]

- [ ] Import into an empty wiki of el wiktionary on Linux with MySQL, or Neo4j
- [ ] create special wiki reference edge between read only text nodes
- [X] watch the youtube video
[[https://www.youtube.com/watch?v=o6wueyweC34 ]]
- [X] read Neo4j document
[[http://guides.neo4j.com/wiki]]
- [X] try Neo4j sandbox
[[https://neo4j.com/sandbox-v2/]]
- [X] create Neo4j docker.

*** Google NLP
**** Setup
- [X] 配置好GOOGLE GCLOUD PLATORM的代理
- [ ] 利用本地PYTHON处理一下中文数据提取SENSITIVITY数据。
- [ ] deep learning of bank stress in the news
- [ ] detect events based on text, obtain descriptions of events.
**** configuration:
- reports-174211
- proxy:
127.0.0.1:1080

- [X] Make sure that Python 2.7.9 or later is installed on your system.
- [X] ./google-cloud-sdk/install.sh
- [X] ./google-cloud-sdk/bin/gcloud init
- [X] pip install --upgrade google-cloud-language
- [X] export GOOGLE_APPLICATION_CREDENTIALS=/home/weiwu/projects/sentiment/reports-b56f3f2275a1.json
- [X] gcloud auth activate-service-account --key-file=/home/weiwu/projects/sentiment/reports-b56f3f2275a1.json
- [X] example:
#+BEGIN_SRC python
# auth
from google.oauth2 import service_account

credentials = service_account.Credentials.from_service_account_file(
    '/home/weiwu/projects/sentiment/reports-b56f3f2275a1.json')

scoped_credentials = credentials.with_scopes(
    ['https://www.googleapis.com/auth/cloud-platform'])

#+END_SRC

#+BEGIN_SRC python
# -*- coding: utf-8 -*-
# Imports the Google Cloud client library
from google.cloud import language

# Instantiates a client
language_client = language.Client()

# The text to analyze
text = 'Hello, world!'
#text = u'正在按照第一个网页流程走'
document = language_client.document_from_text(text)

# Detects the sentiment of the text
sentiment = document.analyze_sentiment().sentiment

print('Text: {}'.format(text))
print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))

#+END_SRC

- https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/language/movie_nl

- GCLOUD example:
#+BEGIN_SRC shell
gcloud ml language analyze-entities --content="Michelangelo Caravaggio, Italian painter, is known for 'The Calling of Saint Matthew'."
#+END_SRC
- connect to Amazon AWS EW2:
#+BEGIN_SRC shell
# 1
chmod 400 /path/my-key-pair.pem
# 2
ssh -i "amazon.pem" ec2-user@ec2-54-200-133-176.us-west-2.compute.amazonaws.com
# 3
scp -i "/home/weiwu/Downloads/amazon.pem" ~/projects/sentiment/reports-b56f3f2275a1.json  ec2-user@ec2-54-200-133-176.us-west-2.compute.amazonaws.com:~/projects/sentiment
#+END_SRC
- install Google SDK on AWS EM2
methon 1:
curl https://sdk.cloud.google.com | bash
sudo cp /usr/bin/python2.7 /usr/local/bin/python2.7
sudo `which pip` install --upgrade google-cloud-language
**** SDK
- GOOGLE NLP SDK试用，只能用PYTHON2.7，而且内容是从本地传输到GOOGLE 云计算服务，在传输过程中遇到代理麻烦，无法返回结果。
- https://cloud.google.com/sdk/docs/quickstart-linux
- key:
https://console.cloud.google.com/iam-admin/serviceaccounts/project?project=reports-174211
- 初始化SDK
  - 配置网络代理
  - 输入代理IP地址:127.0.0.1，代理端口：1080
- 授权SDK
  - gcloud auth application-default login

- auth
https://developers.google.com/identity/protocols/application-default-credentials
#+BEGIN_SRC json
{
  "type": "service_account",
  "project_id": "reports-174211",
  "private_key_id": "54818469c5e32cb8f5d55b755945e6bd7f65c747",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQC0Lz4mVUr36Y7C\n+hxLunVLm3RfBMDsJ9RZ/jICmpCprYIjYebLql0+T2DnhU7eKH2hhbHKZdUfWOvA\nDxE/DUk04eG22vsvoPlIGjN88c3EkKqEddX0llOkKeVNMAB0lN+ESEmrMC6Ewhd2\n/51dIqekDjTV5wrmZxnT1SSuJsrgA0Wbey5TjVmYBTO3hsR37sJxf6yj0+lrVqUw\np2rgkIuDrQy97jP9AYdAF6Lt1eIZvLTIC8GBpo0VjVb+s8CRhZiKGxunUu8HFymS\nK/9Li8BRJ9q/ozp4OGJszk2RQiTbFoKI64pVqggTJxRVw4pdozpzPL4cDqqjYu7y\nCakFxFStAgMBAAECggEAAL36EaKDy3IYOa2Eh3umMdsbIgINFlvn+BKVMPq4fPkU\njQyWl1ZQCdU+BwNpn/4cMwRah1ofLtYeVcfqGwT1EnqaA/ddFfo3KfNxAgMlRQ7l\nzjkFr3sdWegnRDJd/3JBL78JeKLwlMY6M64D/1HIzXhBLPQFkOMx/B6dBvpCFyVC\niE/FTJHFpN5BJni+cBtKfdKbPYGkxtNN4XdPrGtS3pg+NX8hJwtyTc9CV63YSPxG\nY7Zat5IRR+nTkHNJj/jqTLb1YolzjTSGR1vbb1KTUe+KH4ZxiYd0MVCDEEgrhvli\nmEunzCGbrIgi3K8+h77FHq2UHbVL4epd6leswEpH6QKBgQDiz/bDGXIAKszc9GsS\na4mhU/06Cw79C99rc6iMMq9O5N6qaXDlExT5IS/1xBMv8M4lQ4Qp6OqzlXP/gci3\nVOm6TzoB92hwTja5IxIlhHuCxy3+LCjN2T95dTL5QbjKtYn9IQ/SX+ZVSXxHYUvQ\nxgwZud/FIS+YE2qChYFEaev7BQKBgQDLXy++mrOHm/Kwpkf2elrra6m/jPHdJe3m\nuv7lQf9QJAjSBNDJppHiryqOKQeSJyLX7WrPi/ngUM/wFSAi/llgmLApQwAla5Xs\nPLRntxaWDK2JDUonGDkdWeR5kwyns/oomnECiS22m1+oh+2uYQHgW8DpiG4fPpAt\nJ3K6KOUziQKBgAJ3sr6So0vfK1VoD1Ym91ta95FPF32nSGn52yaTUpbwJIAEH6dO\nWtcPBl+NH4xQ+V0sa96s2wjTImzwM4fTE6vkOsRUtb0rDKtc91CTLWjR0+1E+7ip\nmWgv3zsY/PKvoO2xE2P9iRr6brC0HUgLL4yEGiwh3UuxQmJjI3HkGR4lAoGALc16\nS0s8M8aDmhVZD8HuyCavTFzJ/CyWr+hdj/SVM89pzSMYgtHo+UzKIMpM0jXOOyNV\ncvjrhgGtUzzux/LQ3liFFEXquoLIeTQSzWkGxTSFwOAlWzixZFLiih78MqjuJkbi\nPNf4W7ASkzpJ5YQjkAwaHn6DjrXgcsr3o8WiGUECgYBBLnbZLyMSkjZ0n0wYG0a7\nB4b0piRlGbSfEcKWpxoS+EJDuEgvZgn92EOHGaE0xtZvmwbWuqTKKRHXYDGG0VkR\nTf1pMyBF7uxRXDXPslZU2rUTudKMxP8HxoV8WGwxH+ltZYwsMV1iKv3xqbU6CRy2\nu5i+YnEcmQl6HFH/H8tzQg==\n-----END PRIVATE KEY-----\n",
  "client_email": "cloudnatruallanguageapi@reports-174211.iam.gserviceaccount.com",
  "client_id": "108156446460648516099",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://accounts.google.com/o/oauth2/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/cloudnatruallanguageapi%40reports-174211.iam.gserviceaccount.com"
}

#+END_SRC

- gcloud config list
#+BEGIN_SRC shell
(C:\Users\gft\Anaconda2) C:\Users\gft\Documents\Python Scripts\sentiment>gcloud
config list
[compute]
region = us-east1
zone = us-east1-d
[core]
account = victor.wuv@gmail.com
disable_usage_reporting = False
project = reports-174211
[proxy]
address = 192.168.1.164
port = 1080
type = http

Your active configuration is: [nlp1]

(C:\Users\gft\Anaconda2) C:\Users\gft\Documents\Python Scripts\sentiment>
#+END_SRC
*** read the book 《自然语言处理综论》
not easy to understand this book.
- listen to the open course
https://web.stanford.edu/class/cs224n/syllabus.html
- 寻找中文分词开源项目
LTP from Harbin Institute of Technology
http://ltp.ai/
http://ltp.readthedocs.io/zh_CN/latest/
http://www.ltp-cloud.com/intro/
- Strucuture:
分词, 词性标注, Parser
- Tools
CRF++

GIZA

Word2Vec
- Opinion Mining, Sentiment Analysis, and Opinion Spam Detection
https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
https://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf
- 目标学习：信息抽取
*** 如何在NLP领域快速学会第一个技能？
- 找到一个开源项目，比如机器翻译或者深度学习的项目。
- 理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。
- 然后再深入理解开源项目示范程序的算法。
- 自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。
- 在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。
- [ ] 词性标记集：LTP中采用863词性标注集，其各个词性含义如下表：

| Tag	 | Description	         | Example	    |    |
|-------+-----------------------+--------------+----|
| a	   | adjective	           | 美丽	       |    |
| b	   | other noun-modifier	 | 大型 ,西式	 |    |
| c	   | conjunction	         | 和,虽然	    |    |
| d	   | adverb	              | 很           | 	 |
| e	   | exclamation	         | 哎	         |    |
| g	   | morpheme	            | 茨, 甥	     |    |
| h	   | prefix	              | 阿, 伪       | 	 |
| i	   | idiom	               | 百花齐放	   |    |
| j	   | abbreviation	        | 公检法	     |    |
| k	   | suffix	              | 界, 率	     |    |
| m	   | number	              | 一, 第一	   |    |
| n	   | general noun	        | 苹果	       |    |
| nd	  | direction noun	      | 右侧	       |    |
| nh	  | person name	         | 杜甫, 汤姆   |    |
| ni	  | organization name	   | 保险公司     |    |
| nl	  | location noun	       | 城郊         |    |
| ns	  | geographical name	   | 北京         |    |
| nt	  | temporal noun	       | 近日, 明代   |    |
| nz	  | other proper   noun	 | 诺贝尔奖     |    |
| o	   | onomatopoeia	        | 哗啦         |    |
| p	   | preposition	         | 在, 把       |    |
| q	   | quantity	            | 个           |    |
| u	   | auxiliary	           | 的, 地       |    |
| v	   | verb	                | 跑, 学习     |    |
| wp	  | punctuation	         | ，。！       |    |
| ws	  | foreign words	       | CPU          |    |
| x	   | non-lexeme	          | 萄, 翱       |    |
- [ ] 命名实体识别
命名实体识别 (Named Entity Recognition, NER) 是在句子的词序列中定位并识别人名、地名、机构名等实体的任务。 如之前的例子，命名实体识别的结果是：

国务院 (机构名) 总理李克强 (人名) 调研上海外高桥 (地名) 时提出，支持上海 (地名) 积极探索新机制。
命名实体识别对于挖掘文本中的实体进而对其进行分析有很重要的作用。
- [ ] 依存句法分析
依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关 系。
- [ ] 语义角色标注
语义角色标注 (Semantic Role Labeling, SRL) 是一种浅层的语义分析技术，标注句子中某些短语为给定谓词的论元 (语义角色) ，如施事、受事、时间和地点等。其能够对问答系统、信息抽取和机器翻译等应用产生推动作用。
- [ ] 查看Google Trende API.
** GPU server hardware comparison
- [X] server vs desktop vs cloud
- [X] hardware purchase.
Case	美商海盗船 AIR540 USB3.0 中塔侧透 钢铝混合 游戏散热机箱 支持36CM水冷 黑色	AIR540+AX1500i电源	1000
CPU	i7 7700k		2500
RAM	美商海盗船32G频率3000		3300
GPU	MSI/微星 GTX1080TI AERO 11G公版GPU渲染计算游戏显卡深度学习	X2	12800
Power	以太坊1600W6卡6P供电静音ETH显卡矿机机箱电源		600
Storage	Intel/英特尔 600P 512G M.2 PCI-E 3.0X4 NVME SSD固态硬盘		1250
	Seagate/希捷 ST1000DM010 1T新酷鱼1tb台式机械电脑硬盘		300
Mother board	Asus/华硕 X99-E WS/USB 3.1工作站主板 单路 2011针 4路全速交火		4000
			25750


Case	美商海盗船 AIR540 USB3.0 中塔侧透 钢铝混合 游戏散热机箱 支持36CM水冷 黑色
CPU	i7 6800k
RAM	16G 2400 DDR4
GPU	GTX
Power	1000W
Storage	Intel/英特尔 600P 256G M.2 PCI-E 3.0X4 NVME SSD固态硬盘
Mother board	微星X99A SLI Plus	只支持2路SLI
		GTX 1080	14500
		GTX 1080 Ti	16700
		GTX Titan XP	21200
		GTX 1080x2	19000
		GTX 1080 Tix2	23600
		GTX Titan XPx2	32800

** Machine Learning Basics
[[file:/home/weiwu/website/leolle.github.io/CS/MachineLearning/DeepLearning.org][Deep Learning]]
** Other's recommendation
- [ ] pytorch.
- [ ] stuff from Zhupengjia, Liguofu.
- [X] Yoshua paper Deep Learning.
- [X] Pytorch, Keras, tensorflow, cuda. [[https://keras.io/layers/convolutional/]], [[http://pytorch.org/]]
Tensorflow is a mathmatical library including neural network, Keras is a framework based on Tensorflow.
 - Lower Level: This is where frameworks like Tensorflow, MXNet, Theano, and PyTorch sit. This is the level where mathematical operations like Generalized Matrix-Matrix multiplication and Neural Network primitives like Convolutional operations are implemented.
 - Higher Level: This is where frameworks like Keras sit. At this Level, the lower level primitives are used to implement Neural Network abstraction like Layers and models. Generally, at this level other helpful APIs like model saving and model training are also implemented.
- [X] Deep Learning book, author Ian Goodfellow.
- [ ] Vational Autoencoder(VAE) all factor data, then feed the output to various ML models on GS.
- [X] Neural Language Processing(NLP), 结巴分词.
** Basics
*** ANN
*** CNN
*** RNN
* Machine Learning
Principle: research and project oriented, adjusting speed based on progress.

Spend at most 1 week to learn all those, then pick one to go deeper.
** Week 1/2
*** Logistic Regression
*** Regularization
*** Classification
**** Linear Discriminant Analysis

**** Comparison
*** Decision tree and random forest
*** Boosting
*** XGBoost
*** SVM
*** Clustering
*** Bayesian Network
*** LDA
*** HMM
*** Neural Network
** Studying method:
1. model assumption, input data, application, detail.
* Risk model
DEADLINE: <2017-11-14 Tue>
:LOGBOOK:
CLOCK: [2017-11-14 Tue 10:57]--[2017-11-14 Tue 19:30] =>  8:33
:END:
** removing risk model/portfolio optimization for loop
- [ ] remove risk model for loop.
- [X] calculate factor return and residual(specific risk).
  - [X] stock return should add one more column constraint.
  - [X] extract factor exposure and stock return as regression model input X and Y.
  - [X] apply regression function on groupby.
  - [X] change regression input parameter name.
  - [X] regression day by day in a for loop.
- [X] load xarray data.
- read the barra&axioma documents about risk model.
- create charts from the risk model report.
* GS
** Research workflow
DEADLINE: <2017-11-06 Mon>
:LOGBOOK:
CLOCK: [2017-11-06 Mon 10:25]--[2017-11-07 Tue 10:03] => 23:38

#+END:

:END:
1. [X] Search SSRN and find topics
2. [X] Highlight those topics to create topics idea
3. [X] Create topics OSET and Want based on topics idea
   1. Such want can be under project want or under agent want such as SSRN, Journal
4. [X] Create a Want workflow to research the topics, e.g.,
   1. find top school and authors of that topics
   2. find top papers chronologically in that topics
5. [X] Search top papers and websites under the topics
   1. meet the wants in research workflow with top author, school, etc
6. [X] Download those papers and Meet the Want(s) of that topics
7. [X] Create an idea workflow for such topics
   1. define agent behavior under study
   2. define data
   3. models
   4. results
   5. related topics
8. [ ] Create wants of different oset (doc, idea, func instance, func def, oset) for idea workflow
9. [ ] highlights in related papers to meet above wants, create additional f def and f inst oset, then follow GS workflow to create f def, f instance, etc
10. [ ] define function sampling.

** organize OSet structure
DEADLINE: <2017-10-23 Mon>
[[https://docs.google.com/document/d/1eRArN_yGxYnEEbA-AMDAhGOSGSFsL_CvBstbuJ7v4lE/edit]]
- [-] 从Handbook Of Equity Market Anomalies提取因子，策略，转化为能产生alpha的需求。
  - [ ] Analyst-related anomalies
  - [X] GS技术分析因子分类
  - [ ] Accural anomalies
  - [ ] Post-earnings announcement drift
  - [X] GS函数定义机器学习分类，到需求tree
  - [ ] 建立动量需求，搜动量因子满足需求
  - [ ] 盈余惯性
  - [X] 分析师预测和盈利预测
- [ ] 原有因子指标加入新建OSet
- [X] add machine learning functions to statistical inference
- [X] add abnormal factors on GS
- [X] add computer age statistical inference function content on GS
- [ ] strategies
- [X] stocks
- [X] f def
- [X] abnormal factors
** follow GS workflow

* TODO optimization

** black-litterman model
DEADLINE: <2017-11-01 Wed>
- [X] calculate unconstrainted optimized weight.
bug: inaccurate view will cause extreme result.
- [ ] translate ROE to views.
look ahead 3 months strategy.
- [ ] Omega should be calculated via 2 methods, one is to calculate Omega using LC and Calibration Factor, the other is integrating tilt to new weight w.
- [ ] write/modify code.

- [ ] plugging gs to run with data.
- [ ] check result.
- [ ] prepare presentation of all previous steps.
- [ ] present and get feedback.
- [ ] deploy function definition, function sampling.
<2017-10-01 Sun> - <2017-10-31 Tue>
**- [X] simply use stocks in a sector instead of all sector classes that ignore sector market capitalization.
- [X] prepare data on GS.
- [X] create idea map on GS.
- [X] write pseudo code.
- [X] prepapre industial historical price data, data problem on class market capitalization.
- [X] read pdf/code, take notes.
- [X] present general idea.
vivid views on expected return.
explain exact meaning with each parameter with cases.
search more industrial use cases.
view details.
- [X] write pseudo code, set input parameters.
*** why most of the cases are on assets classes optimization, not on individual stocks.
1. The data points of individual asset are not enough, and are not stable.

2. Sector classes perform stably for a long time.
** cvxportfolio
- [ ] 查看cvxportfolio的使用说明。
- [ ] 在开发的帮助下安装cvxportfolio library，调试。
- [ ] 调试cvxportfolio library，把代码换用该库。
- [ ] 在gs上调试使用cvxpy和cvxportfolio的因子中性程序。
** cvxpy
- [ ] research on soft constraint using cvxpy.
- [ ] 搭建workflow。
- [ ] 多期优化。
** archive
<2017-07-21 Fri> - <2017-09-18 Mon>
- [ ] construnct FS, FR, workflow.
- [ ] soft constraint.
- [ ] worse-case risk analysis.
- [X] add excess single returns and excess cumulative returns on PNLFitness.
- [X] move functions defines osets.
- [X] read future simulation code.
- [X] pack constraint input parameters.
- [ ] DONE multi-period optimization using cvxportfolio.
- [ ] 用多期因子做一下回测看看效果，没有看到在OBJECTIVE FUNCTION里面加TRADE COST LOSS FUNCTION的做法，RETURN, RISK和 COST 不在同一个数量级的.
- [ ] 继续在CVXPORTFOLIO上改用RISK MODEL.
- [ ] 周末把每一期按单期优化处理后得到了新的权重，没有加入预测，上午跑一下看看效果。
- [ ] 把RISK MODEL加到CVXPORTFOLIO。
- [X] GS说明文档，结合以前学到的东西，WORKFLOW, J, FR, FI, 边，等等，重新理解一下。
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] 组合优化当hard constraint无法达成时，relax constraint，结果同时输出fitness。
- [X] mode 参数变成一个枚举值。
- [ ] gs输入signature需要指定 oset.
oset组织不完整，postpone.
- [X] risk model api.
- [-] risk model api test.
  - [X] change factor return dataframe name to b char array.
  - [ ] risk model api function.
    - factor exposure(panel)
      size(1536*8*2885).
      items: datetime index
      major axis: factors
      minor axis: symbols
    - sigma(panel)
      size(1536*35*2885)
    - specific risk(panel)
      panel size is too large for calculation(1536*2885*2885).
    - covariance matrix(panel)
    - factor return(dataframe)
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] test the api according to my own requirement.
  - [ ] risk model class function.
    - factor exposure(panel)
      size(1536*8*2885).
      items: datetime index
      major axis: factors
      minor axis: symbols
    - sigma(panel)
      size(1536*35*2885)
    - specific risk(panel)
      panel size is too large for calculation(1536*2885*2885).
    - covariance matrix(panel)
    - factor return(dataframe)
- [ ] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [ ] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] which industry does the benchmark return come form?
using the robust linear regression to estimate the coefficient and do the ranking.
consider use the lasso regression maybe a better choice.
- [X] 将新写的api代码应用到gs.
- [X] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [X] 将 group constaint 的条件泛化。
- [ ] select assets by returns and volatility according to target mode.
- [ ] Paper on industry classification.
- [X] 将非模型的处理逻辑，如asset constriant，group constraint, exposure constraint抽取与剥离出来，通过app定义输入与输出，编写RISK MODEL API，从 risk model 中取数据的逻辑泛化。
- [X] 将 group constaint 的条件泛化。
- [X] test on gs.
- [X] select assets by returns and volatility according to target mode.
- [X] soft constraint based on penalty function.
- [ ] Paper on industry classification.
1. portfolio optimization
- [ ] write pseudo code.
- [ ] fix const variable issue for target mode naming.
- [ ] write fd for asset constraint on gs.

2. soft constraint
- [ ] write pseudo code.
- [ ] move soft constraint test on gs.

3. industry classification paper
finish this paper and write summary.
find a dateset for industry, product, financial statement, etc, extract nouns from text matching these classes.
duplicate classification algorithm.

- [X] fix const variable issue for target mode naming.
- [X] write fd for asset constraint on gs.
- [X] write pseudo code.
- [ ] move soft constraint test on gs.
- [X] finish text-based network industry classification(TBNIC) and summarizing.
- [X] deploy the constraint function on gs.
- [ ] collect product noun words from description using NLP.
- [ ] get factors list GID
- [ ] review optimization function map, functions signature, pseudo code.
- [ ] collect product noun words from description using NLP.
- [X] programe the pair input data structure.
- [X] make this structure available on GS.
- [X] OTVV的数据结构是通过上传还是写python function还在试验看哪种方式更高效，不过我自己已经把这种函数写出来了。
- [ ] put the constraint GS, review code.
- [X] setup the constraint, covariance matrix, sigma, delta on GS.
- [ ] add industry to portfolio bug: output of this function will ignore assets whose industry information is missing.
- [X] 数据传入没有问题，在计算时出现bug，估计是传入数据dataframe column的顺序出现问题，debug。
fix by fillna with 0 in the factor exposure dataframe.
- [X] deploy the whole function on Jupyter and gs.
- [X] we have a position limit option, length is x.
asset information is a diagonal matrix coming from the input portfolio, whose size is n*n, weight variable should be an m*1 vector, so select m assets from asset information dataframe first.
we can create two constraint with the formula (wT*M1)T>= V_1, (wT*M1)T <= V_2.
- [X] calculate the benchmark factor exposure as the reference j to the exposure constraint input.
- [ ] debug the optimization with exposure constraint.
BUG: asColumnTab will get wrong data if the content in the GftTable is a dataframe with datetimeindex.
- [X] debug the optimization with exposure constraint.
- [X] 程序已经调通，加入factor exposure constraint找到最优解
  1. check the calculation of the factor exposure.
  2. 放松约束条件.
  3. [X] bug found: group constraint loading dataframe index order should be aligned within calculation.
- [-] test:
  - [X] original portfolio is constructed by 59 assets in a single period. gid: 3C9221A6176B231A095969B6BFE8BFB2.
  - [ ] original portfolio is constructed by all hs300 components.
  - [ ] industry weight constraint match the benchmark weight.
- [X] profile the program.
90% of program running time is spent on loading data and decompresssing data.
- [ ] present &review project.
- [ ] portfolio construction with risk-parity model.
- [-] test:
  - [X] original portfolio is constructed by 59 assets in a single period. gid: 3C9221A6176B231A095969B6BFE8BFB2.
  - [X] original portfolio is constructed by all hs300 components.
  - [ ] industry weight constraint match the benchmark weight.
- [X] industry weight constraint match the benchmark weight.
- [ ] pack functions to modules.
- [ ] follow up correction.
- [ ] pack functions to modules.
- [ ] follow up correction.
* TODO future simulation                                :future:optimization:

 <2017-09-19 Tue> - <2017-10-01 Sun>
- [X] buy and hold mode on future backtesting.

- [X] revise pnl fitness function, rename previous pnlfitness function name to PnLAndHoldingFitness, create another two seperate functions PnLFitness and HoldingFitness respectively.

- [X] export data from database.
- [X] roll over between two contracts to get continious contract data.

- [X] create rolling strategy, return rolling position.

- [X] create continuous contracts.

- [X] learn function define, function sampling instance, wants, search strategy run on GS.
- [X] add trading volume and other value to the continuous contract.
- [X] remove unnecessary input of the parsing function.

- [X] learn creating function sampleing.

- [X] create long/short strategy, return neutral position.
- [ ] deploy portfolio optimization on function sampling workflow.
- [ ] create position signal via continous contract data.
- [ ] to get actual position from simulation position.

- [ ] calculate PnL using future rollover position, long/short position, continuous position.

- [ ] modify simulation code referred to stocks simulation.
- [ ] adjust input signatures.
- [ ] future simulation requested function:
- calendar spread trading.
buy current month expired contract.
short next month expired contract.

- rolling on constant maturity.

- trend following on dominant contract.
moving average crossing over on the continous contract.
- [X] future simulation pseudo code.
#+BEGIN_SRC python
 def OnData(self,slice):
        if not self.Portfolio.Invested:
            for chain in slice.FutureChains:
                 # Get contracts expiring no earlier than in 90 days
                contracts = filter(lambda x: x.Expiry > self.Time + timedelta(90), chain.Value)

                # if there is any contract, trade the front contract
                if len(contracts) == 0: continue
                front = sorted(contracts, key = lambda x: x.Expiry, reverse=True)[0]
                self.MarketOrder(front.Symbol , 1)
        else:
            self.Liquidate()
#+END_SRC
 requested function:
- [ ] calendar spread trading.

calculate the position value based on continious contract and actual contract

buy current month expired contract.
short next month expired contract.

- [ ] rolling on constant maturity.

calculate the position value based on continious contract and actual contract

- [ ] trend following on dominant contract.

moving average crossing over on the continous contract.
- [X] range osets.
- [X] parse future continious contract data.
- [ ] simple mode test with real data.
* Paper Summary
#+INCLUDE: "notes/PaperSummary.org"
