#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: NaturalLanguageProcessing
#+DATE: <2017-07-25 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+SETUPFILE: ../../configOrg/level2.org

* Overview
[[file:./nlp/NLP.html][Stanford NLP CS224n notes]]
- Document level:
The task at this level is to classify whether a whole opinion document expresses a positive or negative sentiment.
- Sentence level:
The task at this level goes to the sentences and determines whether each sentence expressed a positive, negative, or neutral opinion.

This level of analysis is closely related to subjectivity classification (Wiebe, Bruce and O'Hara, 1999), which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions.

- Entity and Aspect level:
It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion).

- regular opinions and comparative opinions
A regular opinion expresses a sentiment only on an particular entity or an aspect of the entity, e.g., “Coke tastes very good,” which expresses a positive sentiment on the aspect taste of Coke. A comparative opinion compares multiple entities based on some of their shared aspects, e.g., “Coke tastes better than Pepsi,” which compares Coke and Pepsi based on their tastes (an aspect) and expresses a preference for Coke.

** Sentiment Lexicon and Its Issues
- lexicon
opinions, positive and negative.
- issue
spam review
- language structure:
words -> sentences -> paragraphs -> documents.
is your input a string text, or a list of strings, or a list of lists composing by string.
** Neural Network Architectures
Feed-forward networks and Recurrent Recursive networks.
** Feature Representation
- Input X:
When dealing with natural language, the input x encodes features such as words, part-of-speech tags or other linguistic information. Perhaps the biggest jump when moving from sparse-input linear models to neural-network based models is to stop representing each feature as a unique dimension (the so called one-hot representation) and representing them instead as dense vectors. That is, each core feature is embedded into a d dimensional space, and represented as a vector in that space. The embeddings (the vector representation of each core feature) can then be trained like the other parameter of the function NN.

The main benefit of the dense representations is in generalization power: if we believe
some features may provide similar clues, it is worthwhile to provide a representation that
is able to capture these similarities.

Probabilities: Dog(10 times) vs Cat(several times), almost no connection from probability. But very similar from dense vectors.


** Word Embedding
- Similarity:
We represent word meaning similarity using the meaning of the context words.
*** Increamental training with Gensim.
#+BEGIN_SRC python
model = gensim.models.Word2Vec.load('/tmp/model/path')
model.train(more_sentences)
#+END_SRC
*** Several factors influence the quality of the word vectors:
**** amount and quality of the training data
**** size of the vectors
**** training algorithm
* Definition of Sentiment Analysis
** Structure
Due to a large collection of opinions on the Web, some form of summary of opinions is needed (Hu and Liu, 2004).
** Opinion Defintion
- Observation: An opinion consists of two key components: a target g and a sentiment s on the target.

- Definition (Opinion): An opinion is a quadruple, (g, s, h, t), where g is the opinion (or sentiment) target, s is the sentiment about the target, h is the opinion holder and t is the time when the opinion was expressed.

- Definition (entity): An entity e is a product, service, topic, issue, person, organization, or event. It is described with a pair, e: (T, W), where T is a hierarchy of parts, sub-parts, and so on, and W is a set of attributes of e.

- Definition (opinion): An opinion is a quintuple, (ei, aij, sijkl, hk, tl), where ei is the name of an entity, aij is an aspect of ei, sijkl is the sentiment on aspect aij of entity ei, hk is the opinion holder, and tl is the time when the opinion is expressed by hk. The sentiment sijkl is positive, negative, or neutral, or expressed with different strength/intensity levels, e.g., 1 to 5 stars as used by most review sits on the Web.
* Language Technology Processing
[http://ltp.ai/docs/index.html]
** features:
- 针对单一自然语言处理任务，生成统计机器学习模型的工具
- 针对单一自然语言处理任务，调用模型进行分析的编程接口
- 使用流水线方式将各个分析工具结合起来，形成一套统一的中文自然语言处理系统
- 系统可调用的，用于中文语言处理的模型文件
- 针对单一自然语言处理任务，基于云端的编程接口
* Building the Wikipedia Knowledge Graph in Neo4j
- guide:
[[http://guides.neo4j.com/wiki]]

- main topic classification:
[[https://en.wikipedia.org/wiki/Category:Main_topic_classifications]]
** Login
#+BEGIN_SRC neo4j
:server connect
#+END_SRC
#+BEGIN_SRC text
host:
bolt://ws-10-0-1-111-33640.neo4jsandbox.com:443

username:
neo4j

pwd:
darts-quota-alternation

#+END_SRC

** Approach 1: Loading a reduced subset incrementally through the MediaWiki API
- wiki dumps:

[[https://jesusbarrasa.wordpress.com/2016/08/03/quickgraph2-how-is-wikipedias-knowledge-organised/]]

[[https://jesusbarrasa.wordpress.com/2017/04/26/quickgraph6-building-the-wikipedia-knowledge-graph-in-neo4j-qg2-revisited/]]

https://github.com/jbarrasa/datasets/blob/master/wikipedia/wiki-guide.adoc

- Wiki API JSON format:
[[https://www.mediawiki.org/wiki/API:Categorymembers]]
#+BEGIN_SRC json
{
    "batchcomplete": "",
    "continue": {
        "cmcontinue": "page|2b273f2f3d29272b3b0445434d2f414d37273d0117018f16|55503653",
        "continue": "-||"
    },
    "query": {
        "categorymembers": [
            {
                "pageid": 22939,
                "ns": 0,
                "title": "Physics"
            },
            {
                "pageid": 3445246,
                "ns": 0,
                "title": "Glossary of classical physics"
            },
            {
                "pageid": 24489,
                "ns": 0,
                "title": "Outline of physics"
            },
            {
                "pageid": 1653925,
                "ns": 100,
                "title": "Portal:Physics"
            },
            {
                "pageid": 50926902,
                "ns": 0,
                "title": "Action angle coordinates"
            },
            {
                "pageid": 9079863,
                "ns": 0,
                "title": "Aerometer"
            },
            {
                "pageid": 52657328,
                "ns": 0,
                "title": "Bayesian model of computational anatomy"
            },
            {
                "pageid": 49342572,
                "ns": 0,
                "title": "Group actions in computational anatomy"
            },
            {
                "pageid": 50724262,
                "ns": 0,
                "title": "Blasius\u2013Chaplygin formula"
            },
            {
                "pageid": 33327002,
                "ns": 0,
                "title": "Cabbeling"
            }
        ]
    }
}
#+END_SRC

- Clause of building wikipedia graph
#+BEGIN_SRC neo4j
//Loading the data into Neo4j
//prepare the DB with a few indexes to accelerate the ingestion and querying of the data:
CREATE INDEX ON :Category(catId)
CREATE INDEX ON :Category(catName)
CREATE INDEX ON :Page(pageTitle)

//Loading a reduced subset incrementally through the MediaWiki API
//create the Wikipedia Knowledge Graph about Databases.
//create the root category: Databases.
CREATE (c:Category:RootCategory {catId: 0, catName: 'Databases', subcatsFetched : false, pagesFetched : false, level: 0 })

//iteratively load the next level of subcategories to a depth of our choice.
UNWIND range(0,3) as level
CALL apoc.cypher.doIt("
MATCH (c:Category { subcatsFetched: false, level: $level})
CALL apoc.load.json('https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtype=subcat&cmtitle=Category apoc.text.urlencode(c.catName) + '&cmprop=ids%7Ctitle&cmlimit=500')
YIELD value as results
UNWIND results.query.categorymembers AS subcat
MERGE (sc:Category {catId: subcat.pageid})
ON CREATE SET sc.catName = substring(subcat.title,9),
 sc.subcatsFetched = false,
 sc.pagesFetched = false,
 sc.level = $level + 1
WITH sc,c
CALL apoc.create.addLabels(sc,['Level' + ($level + 1) + 'Category']) YIELD node
MERGE (sc)-[:SUBCAT_OF]->(c)
WITH DISTINCT c
SET c.subcatsFetched = true", { level: level }) YIELD value
RETURN value
//load the pages in a similar way
UNWIND range(0,4) as level
CALL apoc.cypher.doIt("
MATCH (c:Category { pagesFetched: false, level: $level })
CALL apoc.load.json('https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtype=page&cmtitle=Category apoc.text.urlencode(c.catName) + '&cmprop=ids%7Ctitle&cmlimit=500')
YIELD value as results
UNWIND results.query.categorymembers AS page
MERGE (p:Page {pageId: page.pageid})
ON CREATE SET p.pageTitle = page.title, p.pageUrl = 'http://en.wikipedia.org/wiki/' + apoc.text.urlencode(replace(page.title, ' ', '_'))
WITH p,c
MERGE (p)-[:IN_CATEGORY]->(c)
WITH DISTINCT c
SET c.pagesFetched = true", { level: level }) yield value
return value

#+END_SRC
** Approach 2: Batch loading the data with LOAD CSV from an SQL dump
- basics
- wiki page structure example:
[[https://en.wikipedia.org/wiki/Category:Graph_databases]]

Category:Graph databases
From Wikipedia, the free encyclopedia

Subcategories
This category has only the following subcategory.
R
► Resource Description Framework‎ (5 C, 24 P)

Pages in category "Graph databases"
The following 15 pages are in this category, out of 15 total. This list may not reflect recent changes (learn more).

Graph database
A
AllegroGraph
ArangoDB
C
Cypher Query Language
D
DataStax
Sparksee (graph database)
F
FlockDB
G
GRAKN.AI
I
InfiniteGraph
L
Linkurious
M
Mulgara (software)
N
Neo4j
O
Oracle Spatial and Graph
OrientDB
S
Sones GraphDB

Categories: Types of databasesGraph theory

- category content sample:
| category id | auto incremental index | category name               | page count | sub-category count |
| "895945",   | "3",                   | "Computer_storage_devices", | "86",      | "10"               |

- relation sample:
| from category | to category(supercategory) |
| "28169972",   | "51326333"                 |

- import csv to neo4j
#+BEGIN_SRC cypher
USING PERIODIC COMMIT 10000
LOAD CSV FROM "https://github.com/jbarrasa/datasets/blob/master/wikipedia/data/cats.csv?raw=true" as row
CREATE (c:Category { catId: row[0]})
SET c.catName = row[2], c.pageCount = toInt(row[3]), c.subcatCount = toInt(row[4])

USING PERIODIC COMMIT 10000
LOAD CSV FROM "https://github.com/jbarrasa/datasets/blob/master/wikipedia/data/rels.csv?raw=true" as row
MATCH (from:Category { catId: row[0]})
MATCH (to:Category { catId: row[1]})
CREATE (from)-[:SUBCAT_OF]->(to)
#+END_SRC

- regenerating fresh csv files:
1. Start by downloading the latest DB dumps from the Wikipedia downloads page.
2. For the category hierarchy, you’ll only need the following tables: category, categorylinks and page.
3. Load them in your DBMS.
4. Generate the categories CSV file by running the following SQL.
#+BEGIN_SRC sql
select p.page_id as PAGE_ID, c.cat_id as CAT_ID, cast(c.cat_title as nCHAR) as CAT_TITLE , c.cat_pages as CAT_PAGES_COUNT, c.cat_subcats as CAT_SUBCAT_COUNT
into outfile '/Users/jbarrasa/Applications/neo4j-enterprise-3.1.2/import/wiki/cats.csv' fields terminated by ',' enclosed by '"' escaped by '\\' lines terminated by '\n'
from test.category c, test.page p
where c.cat_title = p.page_title
and p.page_namespace = 14
#+END_SRC
5. Generate the relationships file by running the following SQL
#+BEGIN_SRC sql
select p.page_id as FROM_PAGE_ID, p2.page_id as TO_PAGE_ID
into outfile '/Users/jbarrasa/Applications/neo4j-enterprise-3.1.2/import/wiki/rels.csv' fields terminated by ',' enclosed by '"' escaped by '\\' lines terminated by '\n'
from test.category c, test.page p , test.categorylinks l, test.category c2, test.page p2
where l.cl_type = 'subcat'
and c.cat_title = p.page_title
and p.page_namespace = 14
and l.cl_from = p.page_id
and l.cl_to = c2.cat_title
and c2.cat_title = p2.cat_title
and p2.page_namespace = 14
#+END_SRC

*** Data dumps/Import
**** structure:
- category:
| cat_id | cat_title                  | cat_pages | cat_subcats | cat_files |
|      3 | 'Computer_storage_devices' |        88 |          10 |         0 |

- category links:
|  cl_from | cl_to                                    | cl_sortke                                                      | cl_timestamp          | cl_sortkey_prefix  | cl_collation       | cl_type('page','subcat','file') |
| 55706942 | 'NA-importance_NA-Class_Russia_articles' | '^R<82>,<BF>^DIOKK7\'A^D+CAKM7MOM7CA\'=^D+I7K7K^A#^A<84><8F> ' | '2017-11-04 07:25:01' | 'Adonis, Rochelle' | 'uca-default-u-kn' | 'page'                          |

- pages:
| page_id | page_namespace | page_title            | page_restrictions | page_counter | page_is_redirect | page_is_new | page_random           | page_touched     | page_links_updated | page_latest | page_len | page_content_model | page_lang |
| '10'    | '0'            | 'AccessibleComputing' | ?                 | '0'          | '1'              | '0'         | '0.33167112649574004' | '20171002144257' | '20171003005845'   | '767284433' | '124'    | 'wikitext'         | NULL      |

- page links:
|  pl_from | pl_namespace | pl_title                | pl_from_namespace |
| 42886934 |            0 | '!Women_Art_Revolution' |                 0 |

**** methods
[[https://meta.wikimedia.org/wiki/Data_dumps]]

[[https://meta.wikimedia.org/wiki/Data_dumps/Import_examples]]

[[https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/]]
- tools
[[http://wikipapers.referata.com/wiki/List_of_visualization_tools]]

- Import into an empty wiki of el wiktionary on Linux with MySQL[edit]
MediaWiki version: 1.20

This wiki was chosen because it uses a non-latin1 character set, has a reasonable number of articles but isn't huge, and relies on only a small number of extensions.

I chose to import only the current pages, with User or Talk pages, because most folks who set up local mirrors want the article content and not the revision history or the discussion pages.

Before the import[edit]
I downloaded the dumps for a given day. I got all the sql.gz files, the stub-articles.xml.gz file, and the pages-articles.xml.bz2 file from http://download.wikimedia.org/elwiktionary/ even though I knew there would be a few of those sql files I wouldn't need.
I installed the prerequisites for MediaWiki, including MySQL, PHP 5, Apache, php-mysql, php-intl, ImageMagick and rsvg (see the manual).
I downloaded MediaWiki 1.20 and unpacked it into /var/www/html/elwikt (your location may vary).
I installed MediaWiki 1.20 on my laptop, with the following settings:
el for my language and the wiki language
MySQL database type
localhost for hostname (hey, it's a local install on my laptop :-P)
elwikt for database name
no database table prefix
root db username and password for the database username and password for install
a different user name and password for the database account for web access, with 'create if it does not exist' checked
InnoDB table format
Binary character set
Disable media uploads
use InstantCommons
I selected the extensions I wanted installed via the installer, some of them not being necessary but I thought they would be useful to have if I did decide to locally edit:
ConfirmEdit
Gadgets
Nuke
ParserFunctions
RenameUser
Vector
WikiEditor
I generated page, revision and text sql files from the stub and page content XML files, using mwxml2sql via the command mwxml2sql -s elwiktionary-blahblah-stub-articles.xml.gz -t elwiktionary-blahblah-pages-articles.xml.bz2 -f elwikt-pages-current-sql.gz -m 1.20
I converted all the sql files to tab delimited files using sql2txt (same repo as previous step) via the command zcat elwiktionary-blahdate-blahtable.sql.gz | sql2txt | gzip > elwiktionary-blahdate-blahtable.tabs.gz. Actually that's a lie, I wrote a tiny bash script to do them all for me. I skipped the following downloaded files:
site_stats - I didn't want or need these, the numbers would be wrong anyways
user_groups - Not needed for displaying page content
old_image and image - using InstantCommons
page - generated from XML files instead
I converted the page, revision and text table files that were generated from the XML files, to tab delimited, using a command similar to the above step
The actual import[edit]
Note: maybe using charset 'binary' here would be better!

I imported all of the above files into MySQL, doing the following:
#+BEGIN_SRC sql
mysql -u root -p
mysql>use elwikt
mysql>SET autocommit=0;
mysql>SET foreign_key_checks=0;
mysql>SET unique_checks=0;
mysql>SET character_set_client = utf8;
# unpacked the tab delimited file
mysql>TRUNCATE TABLE tablenamehere;
mysql>LOAD DATA INFILE path-to-tab-delim-file-for-table-here FIELDS OPTIONALLY ENCLOSED BY '\'';
repeated this for all tab delim files
mysql>exit;

# or https://meta.wikimedia.org/wiki/Data_dumps/Import_examples/catswiki_bash_script
echo "TRUNCATE TABLE $table ; " | mysql -u root -pnotverysecure enwiki
mysql -u root -pnotverysecure enwiki
#+END_SRC
After the import[edit]
Since this is a wiktionary, I updated the LocalSettings.php file so that page titles need not start with a capital letter, adding $wgCapitalLinks = false; to the file
Since this wiki has extra namespaces beyond the standard ones defined by MediaWiki, I added those to LocalSettings.php. You can find such namespaces by looking at the first few lines of the stubs XML file. Lines added: $wgExtraNamespaces = 'Παράρτημα'; and $wgExtraNamespaces = 'Συζήτηση_παραρτήματος';.
The namespace for the project and for project discussion are typically special localized names. I added those to LocalSettings.php, finding the names in the stub XML file at the beginning: $wgMetaNamespace = 'Βικιλεξικό'; and $wgMetaNamespaceTalk = 'Συζήτηση_βικιλεξικού';
I installed tidy and added the following lines to LocalSettings.php to reflect that: $wgUseTidy = true; and $wgTidyBin = '/usr/bin/tidy';. No configuration file was necessary; one is provided as part of MediaWiki and used by default.
I set up the interwiki cache cdb file, by using fixup-interwikis.py via the command python fixup-interwikis.py --localsettings /var/www/html/elwikt/LocalSettings.php --sitetype wiktionary and then added $wgInterwikiCache = "$IP/cache/interwiki.cdb" to the LocalSettings.php file. (See mw:Interwiki_cache/Setup_for_your_own_wiki for info.)
That was it. This was enough to let me view (most) pages without errors.

* TensorBoard: Embedding Visualization
** Load data
Step 1: Load a TSV file of vectors.
Example of 3 vectors with dimension 4:
0.1\t0.2\t0.5\t0.9
0.2\t0.1\t5.0\t0.2
0.4\t0.1\t7.0\t0.8


Step 2 (optional): Load a TSV file of metadata.
Example of 3 data points and 2 columns.
Note: If there is more than one column, the first row will be parsed as column labels.
Pokémon\tSpecies
Wartortle\tTurtle
Venusaur\tSeed
Charmeleon\tFlame

** Projections
The Embedding Projector has three methods of reducing the dimensionality of a data set: two linear and one nonlinear. Each method can be used to create either a two- or three-dimensional view.

- [[http://setosa.io/ev/principal-component-analysis/][Principal Component Analysis]]
A straightforward technique for reducing dimensions is Principal Component Analysis (PCA). The Embedding Projector computes the top 10 principal components. The menu lets you project those components onto any combination of two or three. PCA is a linear projection, often effective at examining global geometry.

- [[https://distill.pub/2016/misread-tsne/][t-SNE]]
A popular non-linear dimensionality reduction technique is t-SNE. The Embedding Projector offers both two- and three-dimensional t-SNE views. Layout is performed client-side animating every step of the algorithm. Because t-SNE often preserves some local structure, it is useful for exploring local neighborhoods and finding clusters. Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. See this great article for how to use t-SNE effectively.
