#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: interview
#+DATE: <2018-09-26 Wed>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+STARTUP: latexpreview
#+SETUPFILE: ../../configOrg/level2.org
* 逻辑斯特回归为什么要对特征进行离散化
  @严林，本题解析来源：https://www.zhihu.com/question/31989952

  在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

  0. 离散特征的增加和减少都很容易，易于模型的快速迭代；

  1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

  2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

  3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；

  4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

  5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；

  6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

  李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

* 多重共线性

** 什么是多重共线性？
回归中的多重共线性是一个当模型中一些预测变量与其他预测变量相关时发生的条件。严重的多重共线性可能会产生问题，因为它可以增大回归系数的方差，使它们变得不稳定。以下是不稳定系数导致的一些后果：
即使预测变量和响应之间存在显著关系，系数也可能看起来并不显著。
高度相关的预测变量的系数在样本之间差异很大。
从模型中去除任何高度相关的项都将大幅影响其他高度相关项的估计系数。高度相关项的系数甚至会包含错误的符号。

** 要度量多重共线性，
可以检查预测变量的相关性结构。您也可以查看方差膨胀因子 (VIF)。VIF 用于在您的预测变量相关时，度量估计回归系数的方差增加的幅度。如果所有 VIF 都为 1，则不存在多重共线性，但如果有些 VIF 大于 1，则预测变量为相关。VIF 大于 5 时，该项的回归系数的估计结果不理想。

** 多重共线性的纠正方法
如果要对多项式进行拟合，请将预测变量值减去预测变量的均值。

- 从模型中删除那些高度相关的预测变量。由于它们提供了冗余信息，因此删除它们通常不会显著减少 R2。

- Stepwise Regression
考虑使用逐步回归、最佳子集回归或数据集的专门知识来删除这些变量。

- 使用偏最小二乘或主成分分析。这些方法可以将预测变量的数量减少为更小的不相关分量集。
- 增加样本容量

* Newton's method
is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function. It is one example of a root-finding algorithm.
$$x:f(x)=0$$

The method starts with a function f defined over the real numbers x, the function's derivative f ′, and an initial guess x0 for a root of the function f. If the function satisfies the assumptions made in the derivation of the formula and the initial guess is close, then a better approximation x1 is

$$x_{1}=x_{0}-{\frac {f(x_{0})}{f'(x_{0})}}$$

...

$$x_{n+1}=x_{n}-{\frac {f(x_{n})}{f'(x_{n})}}$$

* 中心极限定理
采样的时候，随着采样数量的增加，采样的CDF会趋于某个分布，正态，或者其它。

intuitive example: 弹珠在盒子里面的分布。

* PCA
- maximize the variance of compression.
$$x=U\SigmaV$$
first k columns of U.
$$z=U_{reduce}^T*x$$
- minimize the reconstruction error.

* 判定模型和生成模型
假设你现在有一个分类问题，x是特征，y是类标记。用生成模型学习一个联合概率分布P（x，y），而用判别模型学习一个条件概率分布P（y|x）。

有监督机器学习方法可以分为生成方法和判别方法（常见的生成方法有LDA主题模型、朴素贝叶斯算法和隐式马尔科夫模型等，常见的判别方法有SVM、LR等），生成方法学习出的是生成模型，判别方法学习出的是判别模型。

* activation functions
[[./Images/activation_function.png]]

* Backpropagation
利用链式求导法则来更新权重，一次正向传播，一次反向传播，算误差和再更新权重。

* 朴素贝叶斯基本原理和预测过程
https://en.wikipedia.org/wiki/Naive_Bayes_classifier

Document classification
Here is a worked example of naive Bayesian classification to the document classification problem. Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as

{\displaystyle p(w_{i}\mid C)\,} {\displaystyle p(w_{i}\mid C)\,}
(For this treatment, we simplify things further by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)

Then the probability that a given document D contains all of the words {\displaystyle w_{i}} w_{i}, given a class C, is

{\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,} {\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}
The question that we desire to answer is: "what is the probability that a given document D belongs to a given class C?" In other words, what is {\displaystyle p(C\mid D)\,} {\displaystyle p(C\mid D)\,}?

Now by definition

{\displaystyle p(D\mid C)={p(D\cap C) \over p(C)}} {\displaystyle p(D\mid C)={p(D\cap C) \over p(C)}}
and

{\displaystyle p(C\mid D)={p(D\cap C) \over p(D)}} {\displaystyle p(C\mid D)={p(D\cap C) \over p(D)}}
Bayes' theorem manipulates these into a statement of probability in terms of likelihood.

{\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}} {\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}
Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other;

{\displaystyle p(D\mid S)=\prod _{i}p(w_{i}\mid S)\,} {\displaystyle p(D\mid S)=\prod _{i}p(w_{i}\mid S)\,}
and

{\displaystyle p(D\mid \neg S)=\prod _{i}p(w_{i}\mid \neg S)\,} {\displaystyle p(D\mid \neg S)=\prod _{i}p(w_{i}\mid \neg S)\,}
Using the Bayesian result above, we can write:

{\displaystyle p(S\mid D)={p(S) \over p(D)}\,\prod _{i}p(w_{i}\mid S)} {\displaystyle p(S\mid D)={p(S) \over p(D)}\,\prod _{i}p(w_{i}\mid S)}
{\displaystyle p(\neg S\mid D)={p(\neg S) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg S)} {\displaystyle p(\neg S\mid D)={p(\neg S) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg S)}
Dividing one by the other gives:

{\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S)\,\prod _{i}p(w_{i}\mid S) \over p(\neg S)\,\prod _{i}p(w_{i}\mid \neg S)}} {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S)\,\prod _{i}p(w_{i}\mid S) \over p(\neg S)\,\prod _{i}p(w_{i}\mid \neg S)}}
Which can be re-factored as:

{\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S) \over p(\neg S)}\,\prod _{i}{p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}} {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S) \over p(\neg S)}\,\prod _{i}{p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}
Thus, the probability ratio p(S | D) / p(¬S | D) can be expressed in terms of a series of likelihood ratios. The actual probability p(S | D) can be easily computed from log (p(S | D) / p(¬S | D)) based on the observation that p(S | D) + p(¬S | D) = 1.

Taking the logarithm of all these ratios, we have:

{\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}=\ln {p(S) \over p(\neg S)}+\sum _{i}\ln {p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}} {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}=\ln {p(S) \over p(\neg S)}+\sum _{i}\ln {p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}
(This technique of "log-likelihood ratios" is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)

Finally, the document can be classified as follows. It is spam if {\displaystyle p(S\mid D)>p(\neg S\mid D)} {\displaystyle p(S\mid D)>p(\neg S\mid D)} (i. e., {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}>0} {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}>0}), otherwise it is not spam.

* LR推导
https://blog.csdn.net/puqutogether/article/details/43191099
