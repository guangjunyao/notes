#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: DeepLearning
#+DATE: <2017-10-29 Sun>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+SETUPFILE: ../../configOrg/level2.org

[[https://docs.google.com/document/d/1NitqVZyU1zZYRUQmj-0c2Pq3qso7jcbF9XekZlC-w04/edit]]
* Basic prerequisites
** maximum likelihood estimation
- Summary
Maximum likelihood is a general and powerful technique for learning statistical models, i.e. fitting the parameters to data. The maximum likelihood parameters are the ones under which the observed data has the highest probability. It is widely used in practice, and techniques such as Bayesian parameter estimation are closely related to maximum likelihood.
- Context
This concept has the prerequisites:
 - random variables
 - independent random variables (The data are generally assumed to be independent draws from a distribution.)
 - optimization problems (Maximum likelihood is formulated as an optimization problem.)
 - Gaussian distribution (Fitting a Gaussian distribution is an instructive example of maximum likelihood estimation.)
** terminology:
- tensor
a mathematical object analogous to but more general than a vector, represented by an array of components that are functions of the coordinates of a space.
* 深度学习

深度学习（英语：deep learning）是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。
表征学习的目标是寻求更好的表示方法并创建更好的模型来从大规模未标记数据中学习这些表示方法。表达方式类似神经科学的进步，并松散地创建在类似神经系统中的信息处理和通信模式的理解上，如神经编码，试图定义拉动神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。
* 学习方法
- Step 1: 学习机器学习基础
- Step 2: 深入学习
- Step 3: 选择一个区域并进一步深入
  - 计算机视觉(Computer Vision):
  - 自然语言处理(NLP)：
  - 记忆网络(RNN-LSTM)
  - 深度加强学习(RDL):
  - 生成模型(GAN):
- Step 4: 建立项目
* Critical interview questions
- 传统机器学习考察点：
1、bias与variance的含义，并结合ensemble method问哪种方法降低bias，哪种方法降低variance
2、lr与svm的区别与联系
3、gbdt与adaboost的区别与联系
4、手推svm，svm麻雀虽小五脏俱全
5、pca与lda的区别与联系，并推导
6、白化的原理与作用
7、给一个算法，例如lr，问这个算法的model、evaluate、optimization分别是啥
- 深度学习考察点：
1、手推bp
2、梯度消失/爆炸原因，以及解决方法
随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。

随着网络层数增加，“梯度消失”现象更加严重。具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。

Hinton利用预训练方法缓解了局部最优解问题，为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。去年出现的高速公路网络(highway network)和深度残差学习（deep residual learning）进一步避免了梯度消失，网络层数达到了前所未有的一百多层（深度残差学习：152层）

全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。


3、bn的原理，与白化的联系
[[http://blog.csdn.net/fate_fjh/article/details/53375881]]
Batch Normalization是由google提出的一种训练优化方法。参考论文：Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift[1]
个人觉得BN层的作用是加快网络学习速率，论文中提及其它的优点都是这个优点的副产品。
网上对BN解释详细的不多，大多从原理上解释，没有说出实际使用的过程，这里从what, why, how三个角度去解释BN。

- What is BN

Normalization是数据标准化（归一化，规范化），Batch 可以理解为批量，加起来就是批量标准化。

- Why is BN

解决的问题是梯度消失与梯度爆炸。

关于梯度消失，以sigmoid函数为例子，sigmoid函数使得输出在[0,1]之间。

事实上x到了一定大小，经过sigmoid函数的输出范围就很小了.

如果输入很大，其对应的斜率就很小，我们知道，其斜率（梯度）在反向传播中是权值学习速率。所以就会出现如下的问题，
在深度网络中，如果网络的激活输出很大，其梯度就很小，学习速率就很慢。假设每层学习梯度都小于最大值0.25，网络有n层，因为链式求导的原因，第一层的梯度小于0.25的n次方，所以学习速率就慢，对于最后一层只需对自身求导1次，梯度就大，学习速率就快。
这会造成的影响是在一个很大的深度网络中，浅层基本不学习，权值变化小，后面几层一直在学习，结果就是，后面几层基本可以表示整个网络，失去了深度的意义。

关于梯度爆炸，根据链式求导法，
第一层偏移量的梯度=激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n
假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加

-How to use BN

4、防止过拟合有哪些方法(regularization):
- weight decay:

$$J(W)= MSE_{train}+\lambda\omega^T\omega$$
where $\lambda$ is a value chosen ahead of time that controls the strength of our preference for smaller weights. When $\lambda=0$, we impose no preference, and larger$\lambda$ forces the weights to become smaller. Minimizing J(w) results in a choice of weights that make a tradeoff between fitting the training data and being small.


To get the optimized $\lambda$, splitting data set to train set and validation set.
$$\lambda={10^{-2}, 10^{-1.5}, 10^{-1}, ..., 10, 10^{1.5}, 10^{2}}$$

- restrict parameter values
putting extra constraints on models such as adding restrictions on the parameter values.

- soft constraint
add extra terms in the objective function that can be thought of as corresponding to a soft constraint on the parameter values.
5、dnn、cnn、rnn的区别与联系
DNN是一个大类，CNN是一个典型的空间上深度的神经网络，RNN是在时间上深度的神经网络。

为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式.

图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出题主所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。

样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。对了适应这种需求，就出现了大家所说的另一种神经网络结构——循环神经网络RNN。


6、机器学习与深度学习的联系
7、batch size大小会怎么影响收敛速度
- 最优化考察点：
1、sgd、momentum、rmsprop、adam区别与联系
2、深度学习为什么不用二阶优化
3、拉格朗日乘子法、对偶问题、kkt条件
- coding考察点： 排序、双指针、dp、贪心、分治、递归、回溯、字符串、树、链表、trie、bfs、dfs等等
- 第一种是广撒网地问一些老生常谈的DL中没有标准答案的问题，比如过拟合怎么办？样本偏斜怎么办？
drop out，data augmentation， weight decay(常用的weight decay有哪些？怎么处理weight decay的权重).L1，L2。让你比较为什么要两种weight decay，区别在哪里。比如如果你讲L1零点不可导才用L2，那么立马问你SMOOTHL1。如果你都说明白了，就问你为什么weight decay能够一定程度解决过拟合？如果你说到了L0和稀疏性。接着就来问你为什么稀疏性有效？

* Footnotes

[1] https://arxiv.org/pdf/1502.03167v3.pdf
