#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: MachineLearning
#+DATE: <2017-05-16 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+STARTUP: latexpreview

* 机器学习与数据挖掘

** 问题
- 分类
分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。分类问题也被称为监督式学习(supervised classification)，根据已知训练区提供的样本，通过计算选择特征参数，建立判别函数以对样本进行的分类。 与之相对的称为非监督式学习(unsupervised classification)，也叫做聚类分析。
- 聚类
聚类分析（英语：Cluster analysis，亦称为群集分析）是对于统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。
- 回归
回归分析（英语：Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。
回归分析是建立因变数$ {\displaystyle Y} Y$（或称依变数，反应变数）与自变数 ${\displaystyle X} X$（或称独变数，解释变数）之间关系的模型。简单线性回归使用一个自变量$ {\displaystyle X} X$，复回归使用超过一个自变量$（ {\displaystyle X_{1},X_{2}...X_{i}} X_{1},X_{2}...X_{i}）$。

  - 模型
线性回归 简单回归 普通最小二乘法 多项式回归 一般线性模型

广义线性模式 离散选择 逻辑回归 多项罗吉特 混合罗吉特 波比 多项式波比 排序性模型 有序波比 泊松回归

等级线性模型 固定效应 随机效应 混合模型

非线性回归 非参数 半参数 稳健 分位数回归 保序回归 主成分 最小角 局部 分段
含误差变量

  - 估计

最小二乘法 普通最小二乘法 线性 偏最小二乘回归 总体 广义 加权 非线性 非负 重复再加权 岭回归 LASSO

最小绝对值导数法 贝叶斯 贝叶斯多元
  - 背景
回归模型检验 平均响应和预测响应 误差和残差 拟合优度 学生化残差 高斯－马尔可夫定理
- 异常检测
在数据挖掘中，异常检测（英语：anomaly detection）对不匹配预期模式或数据集中其他项目的项目、事件或观测值的识别。 通常异常项目会转变成银行欺诈、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。
特别是在检测滥用与网络入侵时，有趣性对象往往不是罕见对象，但却是超出预料的突发活动。这种模式不遵循通常统计定义中把异常点看作是罕见对象，于是许多异常检测方法（特别是无监督的方法）将对此类数据失效，除非进行了合适的聚集。相反，聚类分析算法可能可以检测出这些模式形成的微聚类。
有三大类异常检测算法。 在假设数据集中大多数实例都是正常的前提下，无监督异常检测方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。监督式异常检测方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的统计分类问题的关键区别是异常检测的内在不均衡性）。半监督式异常检测方法根据一个给定的正常训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。
关联规则 强化学习 结构预测 特征学习 在线学习 半监督学习 语法归纳
** 监督学习(分类 · 回归)
- 决策树
决策论中 （如风险管理），决策树（Decision tree）由一个决策图和可能的结果（包括资源成本和风险）组成， 用来创建到达目标的规划。决策树建立并用来辅助决策，是一种特殊的树结构。决策树是一个利用像树一样的图形或决策模型的决策支持工具，包括随机事件结果，资源代价和实用性。它是一个算法显示的方法。决策树经常在运筹学中使用，特别是在决策分析中，它帮助确定一个能最可能达到目标的策略。如果在实际中，决策不得不在没有完备知识的情况下被在线采用，一个决策树应该平行概率模型作为最佳的选择模型或在线选择模型算法。决策树的另一个使用是作为计算条件概率的描述性手段。
- 表征（装袋, 提升，随机森林）
- k-NN
- 线性回归
- 朴素贝叶斯
- 神经网络
http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html
- 逻辑回归
- 感知器
- 支持向量机（SVM）
- 相关向量机（RVM）
** 聚类

- BIRCH
- 层次
- k平均
- 期望最大化（EM）
- DBSCAN
- OPTICS
- 均值飘移
** 降维
- 因子分析
- CCA
- ICA
- LDA
- NMF
- PCA
- LASSO
- t-SNE
** 结构预测

- 概率图模型（贝叶斯网络，CRF, HMM）
** 异常检测
- k-NN
- 局部离群因子
** 神经网络

-自编码
- 深度学习
- 多层感知机
- RNN
- 受限玻尔兹曼机
- SOM
- CNN
** 理论

-偏差/方差困境
- 计算学习理论
- 经验风险最小化
- PAC学习
- 统计学习
- VC理论
* Week 1

** What is machine learning
Study of algorithms that
- improve their performance P
- at some task T
training data set, validation data set, test data set.
- with experience E

** Well defined machine learning problem
- supervised learning

Fitting some data to a function or function approximation.

- unsupervised learning

Figuring out what the data is without any feedback. For instance, if we were given many data points, we could group them by similarity, or perhaps determine which variables are better than others.

- Reinforcement Learning

** Decision tree learning
$$H = {H|h: X \to Y}$$

*** Top-Down induction of DTree
- A \to the best decision attribute for next node.
- Assign A as decision attribute for node.
- For each value of A, create new descendant of node.
- Sort training examples to leaf nodes.
- If training examples perfectly classified, then STOP, Else iterate over new leaf nodes.

*** Entropy
Entropy H(X) of a random variable X:
$$H(X) = -\Sum{P(X=i)log_2*P(X=i)}$$

** Course logistics
- Linear Regression
- Logistic Regression
- Neural Networks
- Support Vector Machines
- K-means Clustering
- Principal Components Analysis
- Anomaly Detection
- Collaborative Filtering
- Object Recognition

** Model Representation
To establish notation for future use, we’ll use x(i) to denote the “input” variables (living area in this example),
also called input features, and y(i) to denote the “output” or target variable that we are trying to predict (price).
(x(i),y(i)) is called a training example.
m—is called a training set.

** Cost Function
$J(\Theta_1,\Theta_2)$
contour is the bow projected on the 2D surface.
A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line.

** Linear Regression with One Variable
- <2017-08-14 Mon>
linear regression with one variable is also called simple regression. The X variable is called the predictor, Y is called the dependent.
- some statistics:
  - t-statistics
    off value according to the $/epsilon$.
  - p value
    the probability of the hypothesis that $/beta_1$ is 0.
  - $R^2$
    the confidence level that $/beta_1$ is approximately estimated.
  - RSS
    residual sum of squares
  - RSE
    residual standard error.

** Linear Algebra Review
- Vector
- Matrix
-
* Week 2 Linear Regression with Multiple Variables
- Gradient Descent:
Taking the derivative (the tangential line to a function) of our cost function.
The slope of the tangent is the derivative at that point and it will give us a direction to move towards.
We make steps down the cost function in the direction with the steepest descent.
The size of each step is determined by the parameter α, which is called the learning rate.
- Algorithm:
$$\Theta_j = \Theta_j + \Alpha\Derivative{J(\Theta_0,\Theta_1)}$$
Update simutaneously:
$$Temp_0 := \Theta_0 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
$$Temp_1 := \Theta_1 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
- normalization
$$\theta_0 = \theta_0 - \alpha\partial{J(\theta)}{\theta}$$
* Week 3
** Logistic Regression
** Regularization
** Classification
*** Linear Discriminant Analysis

*** Comparison
* Week 4
** Neural Networks: Representation
* Week 5
** Neural Networks: Learning
* Week 6
** Advice for Applying Machine Learning
** Machine Learning System Design
* Week 7
** Support Vector Machine
Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression.
Given a set of training examples, each marked as belonging to one of two categories,
an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.

*** Maximal Margin Classifier

*** Support Vector Classifiers

*** Support Vector Machines
#+begin_src emacs-lisp :tangle yes
from sklearn import svm
training_X = target
training_y = target names
svm_model = svm.SVC(gamma=0.01, C=100.)
svm_model.fit(training_X, training_y)
predicts = svm_model.predict(test_X)
from sklearn.metrics import accuracy_score
accuracy_score(y_true, predicts)

#+end_src
* Week 8
** Unsupervised Learning
** Dimensionality Reduction
* Week 9
** Anomaly Detection
** Recommender Systems
* Week 10
** Large Scale Machine Learning
* Week 11
** Application Example: Photo OCR
* Bayesian network
[[file:./BayesianNetworkDescribingYourData.org][BayesianNetworkDescribingYourData]]

* Tree-Based Methods

** Decision Trees

** Bagging, Random Forests, Boosting

* Unsupervised Learning

** Principal Components Analysis
- first principal component
- second principal component

** Clustering Methods
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.

*** K-Means Clustering
which group of stocks could possibly move up in the next trading period.

*** K-Nearest Neighbors(KNN)

*** Hierarchical Clustering
* Resampling Methods

** Cross-Validation

** The Bootstrap

* Google Natural Language Processing

* Outline of Machine Learning
