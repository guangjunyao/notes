#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: MachineLearning
#+DATE: <2017-05-16 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)


* Week 1

** What is machine learning
Study of algorithms that
- improve their performance P
- at some task T
- with experience E

** Well defined machine learning problem

** Decision tree learning
$$H = {H|h: X \to Y}$$

*** Top-Down induction of DTree
- A \to the best decision attribute for next node.
- Assign A as decision attribute for node.
- For each value of A, create new descendant of node.
- Sort training examples to leaf nodes.
- If training examples perfectly classified, then STOP, Else iterate over new leaf nodes.

*** Entropy
Entropy H(X) of a random variable X:
$$H(X) = -\Sum{P(X=i)log_2*P(X=i)}$$

** Course logistics
- Linear Regression
- Logistic Regression
- Neural Networks
- Support Vector Machines
- K-means Clustering
- Principal Components
- Analysis
- Anomaly Detection
- Collaborative Filtering
- Object Recognition

** Model Representation
To establish notation for future use, we’ll use x(i) to denote the “input” variables (living area in this example),
also called input features, and y(i) to denote the “output” or target variable that we are trying to predict (price).
(x(i),y(i)) is called a training example.
m—is called a training set.

** Cost Function

** Linear Regression with One Variable

** Linear Algebra Review

* Week 2 Linear Regression with Multiple Variables

* Week 3
** Logistic Regression
** Regularization
* Week 4
** Neural Networks: Representation
* Week 5
** Neural Networks: Learning
* Week 6
** Advice for Applying Machine Learning
** Machine Learning System Design
* Week 7
** Support Vector Machines
* Week 8
** Unsupervised Learning
** Dimensionality Reduction
* Week 9
** Anomaly Detection
** Recommender Systems
* Week 10
** Large Scale Machine Learning
* Week 11
** Application Example: Photo OCR
