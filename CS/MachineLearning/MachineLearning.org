#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: MachineLearning
#+DATE: <2017-05-16 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+STARTUP: latexpreview

* Week 1

** What is machine learning
Study of algorithms that
- improve their performance P
- at some task T
- with experience E

** Well defined machine learning problem
- supervised learning
- unsupervised learning

** Decision tree learning
$$H = {H|h: X \to Y}$$

*** Top-Down induction of DTree
- A \to the best decision attribute for next node.
- Assign A as decision attribute for node.
- For each value of A, create new descendant of node.
- Sort training examples to leaf nodes.
- If training examples perfectly classified, then STOP, Else iterate over new leaf nodes.

*** Entropy
Entropy H(X) of a random variable X:
$$H(X) = -\Sum{P(X=i)log_2*P(X=i)}$$

** Course logistics
- Linear Regression
- Logistic Regression
- Neural Networks
- Support Vector Machines
- K-means Clustering
- Principal Components Analysis
- Anomaly Detection
- Collaborative Filtering
- Object Recognition

** Model Representation
To establish notation for future use, we’ll use x(i) to denote the “input” variables (living area in this example),
also called input features, and y(i) to denote the “output” or target variable that we are trying to predict (price).
(x(i),y(i)) is called a training example.
m—is called a training set.

** Cost Function
$J(\Theta_1,\Theta_2)$
contour is the bow projected on the 2D surface.
A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line.

** Linear Regression with One Variable

** Linear Algebra Review
- Vector
- Matrix
-
* Week 2 Linear Regression with Multiple Variables
- Gradient Descent:
Taking the derivative (the tangential line to a function) of our cost function.
The slope of the tangent is the derivative at that point and it will give us a direction to move towards.
We make steps down the cost function in the direction with the steepest descent.
The size of each step is determined by the parameter α, which is called the learning rate.
- Algorithm:
$$\Theta_j = \Theta_j + \Alpha\Derivative{J(\Theta_0,\Theta_1)}$$
Update simutaneously:
$$Temp_0 := \Theta_0 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
$$Temp_1 := \Theta_1 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
- normalization
$$\theta_0 = \theta_0 - \alpha\partial{J(\theta)}{\theta}$$
* Week 3
** Logistic Regression
** Regularization
** Classification
*** Linear Discriminant Analysis

*** Comparison
* Week 4
** Neural Networks: Representation
* Week 5
** Neural Networks: Learning
* Week 6
** Advice for Applying Machine Learning
** Machine Learning System Design
* Week 7
** Support Vector Machine
Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression.
Given a set of training examples, each marked as belonging to one of two categories,
an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.

*** Maximal Margin Classifier

*** Support Vector Classifiers

*** Support Vector Machines
* Week 8
** Unsupervised Learning
** Dimensionality Reduction
* Week 9
** Anomaly Detection
** Recommender Systems
* Week 10
** Large Scale Machine Learning
* Week 11
** Application Example: Photo OCR
* Tree-Based Methods

** Decision Trees

** Bagging, Random Forests, Boosting

* Unsupervised Learning

** Principal Components Analysis

** Clustering Methods
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.

*** K-Means Clustering

*** Hierarchical Clustering
* Resampling Methods

** Cross-Validation

** The Bootstrap
