#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: MachineLearning
#+DATE: <2017-05-16 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)


* Week 1

** What is machine learning
Study of algorithms that
- improve their performance P
- at some task T
- with experience E

** Well defined machine learning problem

** Decision tree learning
$$H = {H|h: X \to Y}$$

*** Top-Down induction of DTree
- A \to the best decision attribute for next node.
- Assign A as decision attribute for node.
- For each value of A, create new descendant of node.
- Sort training examples to leaf nodes.
- If training examples perfectly classified, then STOP, Else iterate over new leaf nodes.

*** Entropy
Entropy H(X) of a random variable X:
$$H(X) = -\Sum{P(X=i)log_2*P(X=i)}$$

** Course logistics
- Linear Regression
- Logistic Regression
- Neural Networks
- Support Vector Machines
- K-means Clustering
- Principal Components
- Analysis
- Anomaly Detection
- Collaborative Filtering
- Object Recognition

** Linear Regression with One Variable

** Linear Algebra Review

* Week 2 Linear Regression with Multiple Variables

* Week 3
** Logistic Regression
** Regularization
* Week 4
** Neural Networks: Representation
* Week 5
** Neural Networks: Learning
* Week 6
** Advice for Applying Machine Learning
** Machine Learning System Design
* Week 7
** Support Vector Machines
* Week 8
** Unsupervised Learning
** Dimensionality Reduction
* Week 9
** Anomaly Detection
** Recommender Systems
* Week 10
** Large Scale Machine Learning
* Week 11
** Application Example: Photo OCR
