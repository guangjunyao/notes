#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: BasicModule
#+DATE: <2017-09-23 Sat>
#+AUTHORs: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)


* Linear Regression with Multiple Variables
- Gradient Descent:
Taking the derivative (the tangential line to a function) of our cost function.
The slope of the tangent is the derivative at that point and it will give us a direction to move towards.
We make steps down the cost function in the direction with the steepest descent.
The size of each step is determined by the parameter Î±, which is called the learning rate.
- Algorithm:
$$\Theta_j = \Theta_j + \Alpha\Derivative{J(\Theta_0,\Theta_1)}$$
Update simutaneously:
$$Temp_0 := \Theta_0 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
$$Temp_1 := \Theta_1 - \Alpha\Derivative{J(\Theta_0,\Theta_1)} $$
- normalization
$$\theta_0 = \theta_0 - \alpha\partial{J(\theta)}{\theta}$$

* Polynomial regression

* Taylor expansion:
#+BEGIN_SRC python
# -*- coding: utf-8 -*-
import numpy as np
import math
import pprint


def cal_e_small(x):
    n = 10
    f = np.arange(1, n+1).cumprod()
    b = np.array([x]*n).cumprod()
    return np.sum(b/f) + 1


def cal_e(x):
    reverse = False
    if x < 0:
        x = -x
        reverse = True
    ln2 = 0.69314718055994529
    c = x / ln2
    a = int(c + 0.5)
    b = x - a * ln2
    y = (2 ** a) * cal_e_small(b)
    if reverse:
        return 1 / y
    return y


t1 = np.linspace(-2, 0, 10, endpoint=False)
t2 = np.linspace(0, 2, 20)
t = np.concatenate((t1, t2))
print(t)
y = np.empty_like(t)
for i, x in enumerate(t):
    y[i] = cal_e(x)
    print('e^', x, ' = ', y[i], ', appx = \t', math.exp(x))

#+END_SRC

#+RESULT:
: e^ -2.0  =  0.135335283237 , appx = 	 0.1353352832366127
: e^ -1.8  =  0.165298888222 , appx = 	 0.16529888822158653
: e^ -1.6  =  0.201896517995 , appx = 	 0.20189651799465538
: e^ -1.4  =  0.246596963942 , appx = 	 0.2465969639416065
: e^ -1.2  =  0.301194211912 , appx = 	 0.30119421191220214
: e^ -1.0  =  0.367879441171 , appx = 	 0.36787944117144233
: e^ -0.8  =  0.449328964117 , appx = 	 0.4493289641172217
: e^ -0.6  =  0.548811636094 , appx = 	 0.5488116360940265
: e^ -0.4  =  0.670320046036 , appx = 	 0.6703200460356393
: e^ -0.2  =  0.818730753078 , appx = 	 0.8187307530779819
: e^ 0.0  =  1.0 , appx = 	 1.0
: e^ 0.105263157895  =  1.11100294108 , appx = 	 1.1110029410844708
: e^ 0.210526315789  =  1.2343275351 , appx = 	 1.2343275350983443
: e^ 0.315789473684  =  1.37134152176 , appx = 	 1.3713415217558058
: e^ 0.421052631579  =  1.5235644639 , appx = 	 1.523564463901954
: e^ 0.526315789474  =  1.69268460033 , appx = 	 1.692684600326856
: e^ 0.631578947368  =  1.88057756929 , appx = 	 1.8805775692915292
: e^ 0.736842105263  =  2.08932721042 , appx = 	 2.089327210420374
: e^ 0.842105263158  =  2.32124867566 , appx = 	 2.3212486756648487
: e^ 0.947368421053  =  2.57891410565 , appx = 	 2.57891410565208
: e^ 1.05263157895  =  2.86518115618 , appx = 	 2.8651811561836884
: e^ 1.15789473684  =  3.18322469126 , appx = 	 3.1832246912598827
: e^ 1.26315789474  =  3.53657199412 , appx = 	 3.5365719941224363
: e^ 1.36842105263  =  3.92914188683 , appx = 	 3.929141886826998
: e^ 1.47368421053  =  4.3652881922 , appx = 	 4.365288192202982
: e^ 1.57894736842  =  4.84984802022 , appx = 	 4.849848020218827
: e^ 1.68421052632  =  5.38819541428 , appx = 	 5.388195414275814
: e^ 1.78947368421  =  5.9863009524 , appx = 	 5.986300952398287
: e^ 1.89473684211  =  6.65079796433 , appx = 	 6.650797964331267
: e^ 2.0  =  7.38905609893 , appx = 	 7.38905609893065

*
