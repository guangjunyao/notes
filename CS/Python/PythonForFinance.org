#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: PythonForFinance
#+DATE: <2017-05-11 Thu>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)

#+todo

* TODO

- [X] 算法与数据结构
- [ ] 机器学习
- [ ] Hadoop
- [ ] Hbase
- [ ] 图形数据库, Tableau
- [ ] HANA
- [ ] Message Queue
- [ ] Tomcat

* Introductory Examples

** Implied Volatilities

** Monte Carlo Simulation

** Technical Analysis

* Data Visualization

** Two-Dementional Plotting

** Financial plots

** 3D plotting

* Financial Time Series

** pandas

** Financial Data

** Regression Analysis
[[file:./py4fi/sentiment.html][sentiment]]

** High-Frequency Data

* Performance Python
[[file:./py4fi/PerformanceOfPythonParadigms.html][PerformancePython]]

* Mathematical Tools
[[file:./py4fi/Regression.html][Regression]]

** Approximation

*** Regression

*** Interpolation

** Convex Optimization

*** Global Optimization

*** Local Optimization

*** Constrained Optimization

** Integration

*** Numerical Integration

*** Integration by simulation

** Symbolic Computation

* Stochastics

** Random Numbers

** Simulation

*** Random Variables

*** Stochastic Processes

*** Variance Reduction

*** Valuation

**** European options

**** American options

*** Risk Measure

**** VaR

**** Credit Value Adjustments

* Statistics

** Normality Tests

*** Benchmark Case

*** Real-World data

** Portfolio Optimization
[[file:./py4fi/Optimization.html][Optimization]]

*** Quadratic function
In algebra, a quadratic function, a quadratic polynomial, a polynomial of degree 2, or simply a quadratic, is a polynomial function in one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables x, y, and z contains exclusively terms x2, y2, z2, xy, xz, yz, x, y, z, and a constant:

{\displaystyle f(x,y,z)=ax^{2}+by^{2}+cz^{2}+dxy+exz+fyz+gx+hy+iz+j,} f(x,y,z)=ax^{2}+by^{2}+cz^{2}+dxy+exz+fyz+gx+hy+iz+j,
with at least one of the coefficients a, b, c, d, e, or f of the second-degree terms being non-zero.

*** Quadratic programming
- Quadratic programming (QP) is the process of solving a special type of mathematical optimization problem—specifically, a (linearly constrained) quadratic optimization problem, that is, the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables. Quadratic programming is a particular type of nonlinear programming.
*** Nonlinear programming
the process of solving an optimization problem defined by a system of equalities and inequalities, collectively termed constraints, over a set of unknown real variables, along with an objective function to be maximized or minimized, where some of the constraints or the objective function are nonlinear.
*** Equality constraints
Quadratic programming is particularly simple when Q is positive definite and there are only equality constraints; specifically, the solution process is linear. By using Lagrange multipliers and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem

{\displaystyle {\text{Minimize}}\quad {\tfrac {1}{2}}\mathbf {x} ^{\mathrm {T} }Q\mathbf {x} +\mathbf {c} ^{\mathrm {T} }\mathbf {x} } {\text{Minimize}}\quad {\tfrac {1}{2}}\mathbf {x} ^{\mathrm {T} }Q\mathbf {x} +\mathbf {c} ^{\mathrm {T} }\mathbf {x}
{\displaystyle {\text{subject to}}\quad E\mathbf {x} =\mathbf {d} } {\text{subject to}}\quad E\mathbf {x} =\mathbf {d}
is given by the linear system

{\displaystyle {\begin{bmatrix}Q&-E^{T}\\E&0\end{bmatrix}}{\begin{bmatrix}\mathbf {x} \\\lambda \end{bmatrix}}={\begin{bmatrix}-\mathbf {c} \\\mathbf {d} \end{bmatrix}}} {\displaystyle {\begin{bmatrix}Q&-E^{T}\\E&0\end{bmatrix}}{\begin{bmatrix}\mathbf {x} \\\lambda \end{bmatrix}}={\begin{bmatrix}-\mathbf {c} \\\mathbf {d} \end{bmatrix}}}
where {\displaystyle \lambda } \lambda  is a set of Lagrange multipliers which come out of the solution alongside {\displaystyle \mathbf {x} } \mathbf {x} .



OLS assumptions:
- Linear in Parameters
- Random Sample of n Observations
- Zero Conditional Mean
- No Perfect Collinearity
- Homoskedasticity

Pitfall of Modern Portfolio Theory(MPT):
- It is based on the historical return performance. If the political visk shows up or the fund changes policy or manager, the corporate or the fund performance may change.

Black–Litterman model:
It is a mathematical model for portfolio allocation developed in 1990 at Goldman Sachs by Fischer Black and Robert Litterman, and published in 1992. It seeks to overcome problems that institutional investors have encountered in applying modern portfolio theory in practice, although the covariances of a few assets can be adequately estimated, it is difficult to come up with reasonable estimates of expected returns. The model starts with the equilibrium assumption that the asset allocation of a representative agent should be proportional to the market values of the available assets, and then modifies that to take into account the 'views' (i.e., the specific opinions about asset returns) of the investor in question to arrive at a bespoke asset allocation.

**** steps:
input: weights, percentage return, percentage volatility, constraints, boundaries.
percentage return = np.sum(rets.mean() * weights) * 252
rets = np.log(data / data.shift(1))
constraints = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})

calculate:
#+BEGIN_SRC python
def statistics(weights):
    ''' Return portfolio statistics.

    Parameters
    ==========
    weights : array-like
        weights for different securities in portfolio

    Returns
    =======
    pret : float
        expected portfolio return
    pvol : float
        expected portfolio volatility
    pret / pvol : float
        Sharpe ratio for rf=0
    '''
    weights = np.array(weights)
    pret = np.sum(rets.mean() * weights) * 252
    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))
    return np.array([pret, pvol, pret / pvol])
def min_func_sharpe(weights):
    return -statistics(weights)[2]
opts = sco.minimize(min_func_sharpe, noa * [1. / noa,], method='SLSQP',
                       bounds=bnds, constraints=cons)
#+END_SRC

output:
#+BEGIN_SRC python
opts
Out[22]:
     fun: -0.89964063622932411
     jac: array([  3.65152955e-05,   2.00167218e+00,  -1.04084611e-04,
         3.82214785e-05,   7.63027400e-01,   0.00000000e+00])
 message: 'Optimization terminated successfully.'
    nfev: 63
     nit: 9
    njev: 9
  status: 0
 success: True
       x: array([  3.16847434e-01,   8.62049147e-16,   2.64774759e-01,
         4.18377806e-01,   0.00000000e+00])
#+END_SRC

**** algorithms for minimizing with constraints:
scipy.optimize.minimize
scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)


*** Efficient frontier

*** Capital Market Line

**** 3 scenarios:
- solve minimum risk for maximum return above target .
Here we find the portfolio that minimizes
the return variance (which is associated with the risk of the portfolio) subject to achieving a minimum acceptable mean return rmin, and satisfying the portfolio
budget and no-shorting constraints.
# solves the Quadratic Programming, where x is the allocation of the portfolio:
# minimize   x'Px + q'x
# subject to Gx <= h
# -c_1*w_1 - c_2*w_2 - ... - c_n*w_n <= -target return
#            Ax == b
# w_1 + w_2 + ... + w_n == b
- solve maximum return for risk under target.

- solve for minimum risk.
- solve for maximum return.
** Principal Component Analysis
[[file:./py4fi/PCA.html][PCA]]

*** The DAX index and its 30 stocks

*** Applying PCA

*** Constructing a PCA Index

** Bayesian Regression
[[file:./py4fi/BayesFormula.html][Bayes]]
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.

*** Bayes's formula

*** PyMC3

* Valuation Framework

** Fundamental Theorem of Asset pricing

** Risk-Neutral discounting

*** modeling and handling dates

*** constant short rate

*** Market environment

* Simulation of Financial Models

** Random Number Generation

** Generic Simulation Class

** Geometric Brownian Motion

** Jump Diffusion

** Square-Root Diffusion

* Derivatives Valuation

** Generic Valuation Class

** European Exercise

** American Excercise

*** Least-Square Monte Carlo

* Portfolio Valuation

** Derivatives positions

** Derivatives portfolio

* Volatility Options

** The VSTOXX Data

*** VSTOXX Index Data

*** VSTOXX Futures Data

*** VSTOXX Options Data

** Model Calibration

** American Options on the VSTOXX

* 非结构化数据可视化

* 最优化算法（锥优化、随机优化优先）

** Gradient descent
In optimization, gradient method is an algorithm to solve problems of the form $$min \f(x)$$.

*** Gradient descent
Gradient descent is a first-order iterative optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.

Limitations: For some of the above examples, gradient descent is relatively slow close to the minimum: technically, its asymptotic rate of convergence is inferior to many other methods. For poorly conditioned convex problems, gradient descent increasingly 'zigzags' as the gradients point nearly orthogonally to the shortest direction to a minimum point. For more details, see the comments below.

For non-differentiable functions, gradient methods are ill-defined.

*** Conjugate gradient method
In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite.
$$Ax=b, u_tAv=0$$

** Stochastic optimization
Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involve random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.

*** Random search
Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.

The name "random search" is attributed to Rastrigin who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search-space, which are sampled from a hypersphere surrounding the current position.

The basic RS algorithm can then be described as:

Initialize x with a random position in the search-space.
Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:
Sample a new position y from the hypersphere of a given radius surrounding the current position x (see e.g. Marsaglia's technique for sampling a hypersphere.)
If f(y) < f(x) then move to the new position by setting x = y

*** Bayesian optimization
Bayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.



* Graphical Models, e.g.,
	* Conditional Random Fields
	* Bayesian Networks
* Genetic Algorithm
In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.
The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.



* First order and Propositional Rule Based Systems, e.g.,
	* Tractable Markov Logic
	* Prolog
	* Lifted Inverse Deduction Algorithms
* Recurrent Nets, e.g.,
	* LSTM
* Natural language processing, e.g.
	* Auto text generation
	* Auto Text Summary
* Reinforcement Learning
* Decision Trees (ensambles)
** 数据处理：离散化
离散化指把连续型数据切分为若干“段”，也称bin，是数据分析中常用的手段。切分的原则有等距，等频，优化，或根据数据特点而定。在营销数据挖掘中，离散化得到普遍采用。究其原因，有这样几点：
- 算法需要。例如决策树，NaiveBayes等算法本身不能直接使用连续型变量，连续型数据只有经离散处理后才能进入算法引擎。
- 离散化可以有效地克服数据中隐藏的缺陷：使模型结果更加稳定。例如，数据中的极端值是影响模型效果的一个重要因素。极端值导致模型参数过高或过低，或导致模型被虚假现象“迷惑”，把原来不存在的关系作为重要模式来学习。而离散化，尤其是等距离散，可以有效地减弱极端值和异常值的影响.
- 有利于对非线性关系进行诊断和描述：对连续型数据进行离散处理后，自变量和目标变量之间的关系变得清晰化。如果两者之间是非线性关系，可以重新定义离散后变量每段的取值，如采取0，1的形式， 由一个变量派生为多个哑变量，分别确定每段和目标变量间的联系。这样做，虽然减少了模型的自由度，但可以大大提高模型的灵活度。
- 等距:将连续型变量的取值范围均匀划成n等份，每份的间距相等。例如，客户订阅刊物的时间是一个连续型变量，可以从几天到几年。采取等距切分可以把1年以下的客户划分成一组，1-2年的客户为一组，2-3年为一组..，以此类分，组距都是一年。
- 等频:把观察点均匀分为n等份，每份内包含的观察点数相同。还取上面的例子，设该杂志订户共有5万人，等频分段需要先把订户按订阅时间按顺序排列，排列好后可以按5000人一组，把全部订户均匀分为十段。
- 离散化处理不免要损失一部分信息。很显然，对连续型数据进行分段后，同一个段内的观察点之间的差异便消失了。
** 随机森林
随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。决策树是一种基本的分类器，一般是将特征分为两类（决策树也可以用来回归，不过本文中暂且不表）。构建好的决策树呈树形结构，可以认为是if-then规则的集合，主要优点是模型具有可读性，分类速度快。
*** 随机森林的构建过程
**** 数据的随机选取：
- 从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
- 第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。
- 最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。
**** 待选特征的随机选取
与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。
**** Random Forest的具体使用-sklearn
以上介绍了随机森林的工作原理，那么在python环境下，我们可以利用python环境下的sklearn包来帮助我们完成任务。举个小例子：
　　特征是通过收盘价数据计算的SMA，WMA，MOM指标，训练样本的特征是从2007-1-4到2016-6-2中截止前一天的SMA，WMA，MOM指标，训练样本的标类别是2007-1-4日到2016-6-2中每一天的涨跌情况，涨了就是True，跌了就是False，测试样本是2016-6-3日的三个指标以及涨跌情况。我们可以判定之后判断结果是正确还是错误，如果通过Random Forest判断的结果和当天的涨跌情况相符，则输出True，如果判断结果和当天的涨跌情况不符，则输出False。
#+BEGIN_SRC python
import talib
from jqdata import *
test_stock = '399300.XSHE'
start_date = datetime.date(2007, 1, 4)
end_date = datetime.date(2016, 6, 3)
trading_days = get_all_trade_days()
start_date_index = trading_days.index(start_date)
end_date_index = trading_days.index(end_date)
x_all = []
y_all = []

for index in range(start_date_index, end_date_index):    # 得到计算指标的所有数据    start_day = trading_days[index - 30]    end_day = trading_days[index]    stock_data = get_price(test_stock, start_date=start_day, end_date=end_day, frequency='daily', fields=['close'])    close_prices = stock_data['close'].values        #通过数据计算指标    # -2是保证获取的数据是昨天的，-1就是通过今天的数据计算出来的指标    sma_data = talib.SMA(close_prices)[-2]     wma_data = talib.WMA(close_prices)[-2]    mom_data = talib.MOM(close_prices)[-2]        features = []    features.append(sma_data)    features.append(wma_data)    features.append(mom_data)        label = False    if close_prices[-1] > close_prices[-2]:        label = True    x_all.append(features)    y_all.append(label) # 准备算法需要用到的数据
 x_train = x_all[: -1]
 y_train = y_all[: -1]
 x_test = x_all[-1]
 y_test = y_all[-1]
 print('data done')

 输出：
data done

from sklearn.ensemble import RandomForestClassifier
#开始利用机器学习算法计算，括号里面的n_estimators就是森林中包含的树的数目啦
clf = RandomForestClassifier(n_estimators=10)
#训练的代码clf.fit(x_train, y_train)
#
得到测试结果的代码prediction = clf.predict(x_test)
# 看看预测对了没print(prediction == y_test)
print('all done')
输出：
[ True]all done
#+END_SRC
* Instance Based Learning
	* SVM
	* k-nearest neighbor
	* Amazon Netflix Recommendation system
* Times Series Analysis, e.g.,
	* Co-integration
	* VAR
* Ux design and Psychology
* Track
#+CAPTION: 标题区域
#+ATTR_HTML: border="1" rules="all" frame="border"
#+begining_src org
| programming | level |
|-------------+-------|
| Lisp        |     1 |
| VBA         |     3 |
| C/C++       |     6 |
| SQL         |     5 |
| Matlab      |     5 |
| R           |     4 |
| Python      |     7 |

| Machine Learning | Models                                           | level |
| Neural Networks  | Convolutional neural network                     |     0 |
|                  | long short-term memory                           |     0 |
|                  | Autoencoder                                      |     0 |
|                  | Bayesian networks                                |     1 |
|                  | PCA                                              |     5 |
|                  | K-Means                                          |     1 |
|                  | SVM                                              |     1 |
| Optimization     | Linear OLS(mean variance)                        |     4 |
|                  | Genetic Algorithm                                |     0 |
|                  | h(params,x)函数：hypothesis                      |     0 |
|                  | J(params,x,y)函数：cost function                 |     0 |
|                  | grad(params,x,y)函数：Gradient Descent           |     1 |
| Time Series      | autoregressive(AR)                               |     1 |
|                  | moving average (MA)                              |     2 |
|                  | autoregressive moving average (ARMA)             |     1 |
|                  | autoregressive integrated moving average (ARIMA) |     1 |
#+end_src
