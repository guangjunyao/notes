<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-09-29 Fri 20:13 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="weiwu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "5em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "left",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. <span class="todo TODO">TODO</span> </a></li>
<li><a href="#orgheadline5">2. Introductory Examples</a>
<ul>
<li><a href="#orgheadline2">2.1. Implied Volatilities</a></li>
<li><a href="#orgheadline3">2.2. Monte Carlo Simulation</a></li>
<li><a href="#orgheadline4">2.3. Technical Analysis</a></li>
</ul>
</li>
<li><a href="#orgheadline9">3. Data Visualization</a>
<ul>
<li><a href="#orgheadline6">3.1. Two-Dementional Plotting</a></li>
<li><a href="#orgheadline7">3.2. Financial plots</a></li>
<li><a href="#orgheadline8">3.3. 3D plotting</a></li>
</ul>
</li>
<li><a href="#orgheadline14">4. Financial Time Series</a>
<ul>
<li><a href="#orgheadline10">4.1. pandas</a></li>
<li><a href="#orgheadline11">4.2. Financial Data</a></li>
<li><a href="#orgheadline12">4.3. Regression Analysis</a></li>
<li><a href="#orgheadline13">4.4. High-Frequency Data</a></li>
</ul>
</li>
<li><a href="#orgheadline15">5. Performance Python</a></li>
<li><a href="#orgheadline44">6. Mathematical Tools</a>
<ul>
<li><a href="#orgheadline35">6.1. Approximation</a>
<ul>
<li><a href="#orgheadline28">6.1.1. Regression</a></li>
<li><a href="#orgheadline29">6.1.2. Interpolation</a></li>
<li><a href="#orgheadline34">6.1.3. Clustering</a></li>
</ul>
</li>
<li><a href="#orgheadline39">6.2. Convex Optimization</a>
<ul>
<li><a href="#orgheadline36">6.2.1. Global Optimization</a></li>
<li><a href="#orgheadline37">6.2.2. Local Optimization</a></li>
<li><a href="#orgheadline38">6.2.3. Constrained Optimization</a></li>
</ul>
</li>
<li><a href="#orgheadline42">6.3. Integration</a>
<ul>
<li><a href="#orgheadline40">6.3.1. Numerical Integration</a></li>
<li><a href="#orgheadline41">6.3.2. Integration by simulation</a></li>
</ul>
</li>
<li><a href="#orgheadline43">6.4. Symbolic Computation</a></li>
</ul>
</li>
<li><a href="#orgheadline56">7. Stochastics</a>
<ul>
<li><a href="#orgheadline45">7.1. Random Numbers</a></li>
<li><a href="#orgheadline55">7.2. Simulation</a>
<ul>
<li><a href="#orgheadline46">7.2.1. Random Variables</a></li>
<li><a href="#orgheadline47">7.2.2. Stochastic Processes</a></li>
<li><a href="#orgheadline48">7.2.3. Variance Reduction</a></li>
<li><a href="#orgheadline51">7.2.4. Valuation</a></li>
<li><a href="#orgheadline54">7.2.5. Risk Measure</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline73">8. Statistics</a>
<ul>
<li><a href="#orgheadline59">8.1. Normality Tests</a>
<ul>
<li><a href="#orgheadline57">8.1.1. Benchmark Case</a></li>
<li><a href="#orgheadline58">8.1.2. Real-World data</a></li>
</ul>
</li>
<li><a href="#orgheadline65">8.2. Portfolio Optimization</a>
<ul>
<li><a href="#orgheadline62">8.2.1. Efficient frontier</a></li>
<li><a href="#orgheadline64">8.2.2. Capital Market Line</a></li>
</ul>
</li>
<li><a href="#orgheadline69">8.3. Principal Component Analysis</a>
<ul>
<li><a href="#orgheadline66">8.3.1. The DAX index and its 30 stocks</a></li>
<li><a href="#orgheadline67">8.3.2. Applying PCA</a></li>
<li><a href="#orgheadline68">8.3.3. Constructing a PCA Index</a></li>
</ul>
</li>
<li><a href="#orgheadline72">8.4. Bayesian Regression</a>
<ul>
<li><a href="#orgheadline70">8.4.1. Bayes's formula</a></li>
<li><a href="#orgheadline71">8.4.2. PyMC3</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline79">9. Valuation Framework</a>
<ul>
<li><a href="#orgheadline74">9.1. Fundamental Theorem of Asset pricing</a></li>
<li><a href="#orgheadline78">9.2. Risk-Neutral discounting</a>
<ul>
<li><a href="#orgheadline75">9.2.1. modeling and handling dates</a></li>
<li><a href="#orgheadline76">9.2.2. constant short rate</a></li>
<li><a href="#orgheadline77">9.2.3. Market environment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline85">10. Simulation of Financial Models</a>
<ul>
<li><a href="#orgheadline80">10.1. Random Number Generation</a></li>
<li><a href="#orgheadline81">10.2. Generic Simulation Class</a></li>
<li><a href="#orgheadline82">10.3. Geometric Brownian Motion</a></li>
<li><a href="#orgheadline83">10.4. Jump Diffusion</a></li>
<li><a href="#orgheadline84">10.5. Square-Root Diffusion</a></li>
</ul>
</li>
<li><a href="#orgheadline90">11. Derivatives Valuation</a>
<ul>
<li><a href="#orgheadline86">11.1. Generic Valuation Class</a></li>
<li><a href="#orgheadline87">11.2. European Exercise</a></li>
<li><a href="#orgheadline89">11.3. American Excercise</a>
<ul>
<li><a href="#orgheadline88">11.3.1. Least-Square Monte Carlo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline93">12. Portfolio Valuation</a>
<ul>
<li><a href="#orgheadline91">12.1. Derivatives positions</a></li>
<li><a href="#orgheadline92">12.2. Derivatives portfolio</a></li>
</ul>
</li>
<li><a href="#orgheadline100">13. Volatility Options</a>
<ul>
<li><a href="#orgheadline97">13.1. The VSTOXX Data</a>
<ul>
<li><a href="#orgheadline94">13.1.1. VSTOXX Index Data</a></li>
<li><a href="#orgheadline95">13.1.2. VSTOXX Futures Data</a></li>
<li><a href="#orgheadline96">13.1.3. VSTOXX Options Data</a></li>
</ul>
</li>
<li><a href="#orgheadline98">13.2. Model Calibration</a></li>
<li><a href="#orgheadline99">13.3. American Options on the VSTOXX</a></li>
</ul>
</li>
<li><a href="#orgheadline101">14. 非结构化数据可视化</a></li>
<li><a href="#orgheadline108">15. 最优化算法（锥优化、随机优化优先）</a>
<ul>
<li><a href="#orgheadline104">15.1. Gradient descent</a>
<ul>
<li><a href="#orgheadline102">15.1.1. Gradient descent</a></li>
<li><a href="#orgheadline103">15.1.2. Conjugate gradient method</a></li>
</ul>
</li>
<li><a href="#orgheadline107">15.2. Stochastic optimization</a>
<ul>
<li><a href="#orgheadline105">15.2.1. Random search</a></li>
<li><a href="#orgheadline106">15.2.2. Bayesian optimization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline109">16. Graphical Models, e.g.,</a></li>
<li><a href="#orgheadline110">17. Genetic Algorithm</a></li>
<li><a href="#orgheadline111">18. First order and Propositional Rule Based Systems, e.g.,</a></li>
<li><a href="#orgheadline112">19. Recurrent Nets, e.g.,</a></li>
<li><a href="#orgheadline113">20. Natural language processing, e.g.</a></li>
<li><a href="#orgheadline114">21. Reinforcement Learning</a></li>
<li><a href="#orgheadline121">22. Decision Trees (ensambles)</a>
<ul>
<li><a href="#orgheadline115">22.1. 数据处理：离散化</a></li>
<li><a href="#orgheadline120">22.2. 随机森林</a>
<ul>
<li><a href="#orgheadline119">22.2.1. 随机森林的构建过程</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline122">23. Instance Based Learning</a></li>
<li><a href="#orgheadline123">24. Times Series Analysis, e.g.,</a></li>
<li><a href="#orgheadline124">25. Ux design and Psychology</a></li>
<li><a href="#orgheadline125">26. Track</a></li>
</ul>
</div>
</div>
<p>
#+todo
</p>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1"><span class="section-number-2">1</span> <span class="todo TODO">TODO</span> </h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li class="on"><code>[X]</code> 算法与数据结构</li>
<li class="off"><code>[&#xa0;]</code> 机器学习</li>
<li class="off"><code>[&#xa0;]</code> Hadoop</li>
<li class="off"><code>[&#xa0;]</code> Hbase</li>
<li class="off"><code>[&#xa0;]</code> 图形数据库, Tableau</li>
<li class="off"><code>[&#xa0;]</code> HANA</li>
<li class="off"><code>[&#xa0;]</code> Message Queue</li>
<li class="off"><code>[&#xa0;]</code> Tomcat</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-2">
<h2 id="orgheadline5"><span class="section-number-2">2</span> Introductory Examples</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2"><span class="section-number-3">2.1</span> Implied Volatilities</h3>
</div>

<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">2.2</span> Monte Carlo Simulation</h3>
</div>

<div id="outline-container-orgheadline4" class="outline-3">
<h3 id="orgheadline4"><span class="section-number-3">2.3</span> Technical Analysis</h3>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-2">
<h2 id="orgheadline9"><span class="section-number-2">3</span> Data Visualization</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-orgheadline6" class="outline-3">
<h3 id="orgheadline6"><span class="section-number-3">3.1</span> Two-Dementional Plotting</h3>
</div>

<div id="outline-container-orgheadline7" class="outline-3">
<h3 id="orgheadline7"><span class="section-number-3">3.2</span> Financial plots</h3>
</div>

<div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">3.3</span> 3D plotting</h3>
</div>
</div>

<div id="outline-container-orgheadline14" class="outline-2">
<h2 id="orgheadline14"><span class="section-number-2">4</span> Financial Time Series</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10"><span class="section-number-3">4.1</span> pandas</h3>
</div>

<div id="outline-container-orgheadline11" class="outline-3">
<h3 id="orgheadline11"><span class="section-number-3">4.2</span> Financial Data</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>data preparation &amp; parsing</li>
</ul>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">from</span> scipy.stats <span style="color: #F92672;">import</span> mstats
<span style="color: #FD971F;">df_factor</span> = pd.Series<span style="color: #AE81FF;">(</span>index=idx, data=mstats.winsorize<span style="color: #66D9EF;">(</span>df_factor, limits=<span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">025</span><span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">df_factor.apply(mstats.winorize())</span>
<span style="color: #FD971F;">start_date</span> = df_factor.index<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">0</span><span style="color: #AE81FF;">]</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">remove outlier data by quantile</span>
<span style="color: #75715E;">#</span><span style="color: #75715E;">df_factor[df_factor&gt;np.percentile(df_factor, 90)] = np.percentile(df_factor, 90)</span>

<span style="color: #75715E;"># </span><span style="color: #75715E;">remove by z score</span>
<span style="color: #75715E;">#</span><span style="color: #75715E;">df_factor = pd.Series(index=idx, df_factor[(np.abs(stats.zscore(df_factor)) &lt; 3).all()])</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgheadline12" class="outline-3">
<h3 id="orgheadline12"><span class="section-number-3">4.3</span> Regression Analysis</h3>
<div class="outline-text-3" id="text-4-3">
<p>
<a href="./py4fi/sentiment.html">sentiment</a>
</p>
</div>
</div>

<div id="outline-container-orgheadline13" class="outline-3">
<h3 id="orgheadline13"><span class="section-number-3">4.4</span> High-Frequency Data</h3>
</div>
</div>

<div id="outline-container-orgheadline15" class="outline-2">
<h2 id="orgheadline15"><span class="section-number-2">5</span> Performance Python</h2>
<div class="outline-text-2" id="text-5">
<p>
<a href="./py4fi/PerformanceOfPythonParadigms.html">PerformancePython</a>
</p>
</div>
</div>

<div id="outline-container-orgheadline44" class="outline-2">
<h2 id="orgheadline44"><span class="section-number-2">6</span> Mathematical Tools</h2>
<div class="outline-text-2" id="text-6">
<p>
<a href="./py4fi/Regression.html">Regression</a>
</p>
</div>

<div id="outline-container-orgheadline35" class="outline-3">
<h3 id="orgheadline35"><span class="section-number-3">6.1</span> Approximation</h3>
<div class="outline-text-3" id="text-6-1">
</div><div id="outline-container-orgheadline28" class="outline-4">
<h4 id="orgheadline28"><span class="section-number-4">6.1.1</span> Regression</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
\[standard error = \d{standard deviation}{\sqrt{N}}\]
</p>
<ul class="org-ul">
<li>The standard error is a measure of sample variability.</li>
<li>population standard deviation. \(\sqrt{N-1}\)</li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline25"></a>linear regression<br  /><div class="outline-text-5" id="text-6-1-1-1">
<ul class="org-ul">
<li>linear regression deals with estimating a variable from another variable.</li>
</ul>
<p>
\[ Y = Xk + \epsilon\]
</p>
</div>

<ol class="org-ol"><li><a id="orgheadline16"></a>The dependent variable (Dep. Variable) states the name of the variable that is fitted.  <br  /></li>
<li><a id="orgheadline17"></a>Model states what model we used in the fit; except OLS, there are several other models such as   weighted least squares (WLS).<br  /></li>
<li><a id="orgheadline18"></a>The number of observations (No. Observations) are listed and the degrees of freedom of the residuals (Df Residuals), that is, the number of observations (59) minus the parameters determined through the fitting 2 (k and   k).  <br  /></li>
<li><a id="orgheadline19"></a>Df Model shows how many parameters were determined (except the constant, that is, intercept).<br  /></li>
<li><a id="orgheadline20"></a>The table to the right of the top table shows you information on how well the model fits the data.<br  /></li>
<li><a id="orgheadline21"></a>R-squared was covered before; here, the adjusted R-square value (Adj. R-squared) is also listed and this is the R-square value corrected for the number of data points and degrees of freedom.<br  /></li>
<li><a id="orgheadline22"></a>The   F-statistic number gives you an estimate of how significant the fit is. Practically, it is the mean squared error of the model divided by the mean squared error of the residuals.<br  /></li>
<li><a id="orgheadline23"></a>The next value, Prob (F-statistic), gives you the probability to get the F-statistic value if the null hypothesis is true, that is, the variables are not related.<br  /></li>
<li><a id="orgheadline24"></a>After this, three sets of   log-likelihood function values follow: the value of the log-likelihood value of the fit, the   Akaike Information Criterion (AIC), and   Bayes Information Criterion (BIC). The AIC and BIC are various ways of adjusting the log-likelihood function for the number of observations and model type.<br  /></li></ol></li>
<li><a id="orgheadline26"></a>multiple regression<br  /><div class="outline-text-5" id="text-6-1-1-2">
<ul class="org-ul">
<li>a variable is estimated from two or more others.</li>
</ul>
</div></li>
<li><a id="orgheadline27"></a>logistic regression<br  /><div class="outline-text-5" id="text-6-1-1-3">
<p>
Logistic regression fits models to one or more discrete variables, which are sometimes binary(that is, can only take the values 0 or 1).
</p>
<ul class="org-ul">
<li>One of the main differences between binary logistic and linear regression is that in binary logistic regression, we are fitting the probability of an outcome given a measured (discrete or continuous) variable,</li>
<li>while linear regression models deal with characterizing the dependency of two or more continuous variables on each other.</li>
<li>Logistic regression gives the probability of an occurrence given some observed variable( s). Probability is sometimes expressed as P( Y | X) and read as   Probability that the value is Y given the variable X.</li>
</ul>
<p>
\[ln(p/(1-p)) = m + kx\]
</p>
</div></li></ol>
</div>
<div id="outline-container-orgheadline29" class="outline-4">
<h4 id="orgheadline29"><span class="section-number-4">6.1.2</span> Interpolation</h4>
</div>
<div id="outline-container-orgheadline34" class="outline-4">
<h4 id="orgheadline34"><span class="section-number-4">6.1.3</span> Clustering</h4>
<div class="outline-text-4" id="text-6-1-3">
</div><ol class="org-ol"><li><a id="orgheadline30"></a>K-means nearest neighbor algorithm for cluster finding.<br  /><div class="outline-text-5" id="text-6-1-3-1">
<p>
The K-means algorithm is also referred to as vector quantization. What the algorithm does is finds the cluster (centroid) positions that minimize the distances to all points in the cluster.
</p>
</div></li>
<li><a id="orgheadline33"></a>hierarchical clustering.<br  /><div class="outline-text-5" id="text-6-1-3-2">
<p>
Hierarchical clustering is connectivity-based clustering. It assumes that the clusters are connected, or in another word, linked.
</p>
</div>
<ol class="org-ol"><li><a id="orgheadline31"></a>Agglomerative clustering starts out with each point in its own cluster and then merges the two clusters with the lowest dissimilarity, that is, the bottom-up approach<br  /></li>
<li><a id="orgheadline32"></a>Divisive clustering is, as the name suggests, a top-down approach where we start out with one single cluster that is divided into smaller and smaller clusters<br  /><div class="outline-text-6" id="text-6-1-3-2-2">
<p>
Magnus Vilhelm Persson. Mastering Python Data Analysis (Kindle Locations 2335-2338). Packt Publishing. Kindle Edition.
</p>
</div></li></ol></li></ol>
</div>
</div>
<div id="outline-container-orgheadline39" class="outline-3">
<h3 id="orgheadline39"><span class="section-number-3">6.2</span> Convex Optimization</h3>
<div class="outline-text-3" id="text-6-2">
</div><div id="outline-container-orgheadline36" class="outline-4">
<h4 id="orgheadline36"><span class="section-number-4">6.2.1</span> Global Optimization</h4>
</div>

<div id="outline-container-orgheadline37" class="outline-4">
<h4 id="orgheadline37"><span class="section-number-4">6.2.2</span> Local Optimization</h4>
</div>

<div id="outline-container-orgheadline38" class="outline-4">
<h4 id="orgheadline38"><span class="section-number-4">6.2.3</span> Constrained Optimization</h4>
</div>
</div>

<div id="outline-container-orgheadline42" class="outline-3">
<h3 id="orgheadline42"><span class="section-number-3">6.3</span> Integration</h3>
<div class="outline-text-3" id="text-6-3">
</div><div id="outline-container-orgheadline40" class="outline-4">
<h4 id="orgheadline40"><span class="section-number-4">6.3.1</span> Numerical Integration</h4>
</div>

<div id="outline-container-orgheadline41" class="outline-4">
<h4 id="orgheadline41"><span class="section-number-4">6.3.2</span> Integration by simulation</h4>
</div>
</div>

<div id="outline-container-orgheadline43" class="outline-3">
<h3 id="orgheadline43"><span class="section-number-3">6.4</span> Symbolic Computation</h3>
</div>
</div>

<div id="outline-container-orgheadline56" class="outline-2">
<h2 id="orgheadline56"><span class="section-number-2">7</span> Stochastics</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgheadline45" class="outline-3">
<h3 id="orgheadline45"><span class="section-number-3">7.1</span> Random Numbers</h3>
</div>

<div id="outline-container-orgheadline55" class="outline-3">
<h3 id="orgheadline55"><span class="section-number-3">7.2</span> Simulation</h3>
<div class="outline-text-3" id="text-7-2">
</div><div id="outline-container-orgheadline46" class="outline-4">
<h4 id="orgheadline46"><span class="section-number-4">7.2.1</span> Random Variables</h4>
</div>

<div id="outline-container-orgheadline47" class="outline-4">
<h4 id="orgheadline47"><span class="section-number-4">7.2.2</span> Stochastic Processes</h4>
</div>

<div id="outline-container-orgheadline48" class="outline-4">
<h4 id="orgheadline48"><span class="section-number-4">7.2.3</span> Variance Reduction</h4>
</div>

<div id="outline-container-orgheadline51" class="outline-4">
<h4 id="orgheadline51"><span class="section-number-4">7.2.4</span> Valuation</h4>
<div class="outline-text-4" id="text-7-2-4">
</div><ol class="org-ol"><li><a id="orgheadline49"></a>European options<br  /></li>

<li><a id="orgheadline50"></a>American options<br  /></li></ol>
</div>

<div id="outline-container-orgheadline54" class="outline-4">
<h4 id="orgheadline54"><span class="section-number-4">7.2.5</span> Risk Measure</h4>
<div class="outline-text-4" id="text-7-2-5">
</div><ol class="org-ol"><li><a id="orgheadline52"></a>VaR<br  /></li>

<li><a id="orgheadline53"></a>Credit Value Adjustments<br  /></li></ol>
</div>
</div>
</div>

<div id="outline-container-orgheadline73" class="outline-2">
<h2 id="orgheadline73"><span class="section-number-2">8</span> Statistics</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-orgheadline59" class="outline-3">
<h3 id="orgheadline59"><span class="section-number-3">8.1</span> Normality Tests</h3>
<div class="outline-text-3" id="text-8-1">
</div><div id="outline-container-orgheadline57" class="outline-4">
<h4 id="orgheadline57"><span class="section-number-4">8.1.1</span> Benchmark Case</h4>
</div>

<div id="outline-container-orgheadline58" class="outline-4">
<h4 id="orgheadline58"><span class="section-number-4">8.1.2</span> Real-World data</h4>
</div>
</div>

<div id="outline-container-orgheadline65" class="outline-3">
<h3 id="orgheadline65"><span class="section-number-3">8.2</span> Portfolio Optimization</h3>
<div class="outline-text-3" id="text-8-2">
</div><ol class="org-ol"><li><a id="orgheadline60"></a>steps:<br  /><div class="outline-text-5" id="text-8-2-0-1">
<p>
input: weights, percentage return, percentage volatility, constraints, boundaries.
percentage return = np.sum(rets.mean() * weights) * 252
rets = np.log(data / data.shift(1))
constraints = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})
</p>

<p>
calculate:
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">def</span> <span style="color: #A6E22E;">statistics</span><span style="color: #AE81FF;">(</span>weights<span style="color: #AE81FF;">)</span>:
    <span style="color: #E6DB74;">''' Return portfolio statistics.</span>

<span style="color: #E6DB74;">    Parameters</span>
<span style="color: #E6DB74;">    ==========</span>
<span style="color: #E6DB74;">    weights : array-like</span>
<span style="color: #E6DB74;">        weights for different securities in portfolio</span>

<span style="color: #E6DB74;">    Returns</span>
<span style="color: #E6DB74;">    =======</span>
<span style="color: #E6DB74;">    pret : float</span>
<span style="color: #E6DB74;">        expected portfolio return</span>
<span style="color: #E6DB74;">    pvol : float</span>
<span style="color: #E6DB74;">        expected portfolio volatility</span>
<span style="color: #E6DB74;">    pret / pvol : float</span>
<span style="color: #E6DB74;">        Sharpe ratio for rf=0</span>
<span style="color: #E6DB74;">    '''</span>
    <span style="color: #FD971F;">weights</span> = np.array<span style="color: #AE81FF;">(</span>weights<span style="color: #AE81FF;">)</span>
    <span style="color: #FD971F;">pret</span> = np.<span style="color: #F92672;">sum</span><span style="color: #AE81FF;">(</span>rets.mean<span style="color: #66D9EF;">()</span> * weights<span style="color: #AE81FF;">)</span> * <span style="color: #AE81FF;">252</span>
    <span style="color: #FD971F;">pvol</span> = np.sqrt<span style="color: #AE81FF;">(</span>np.dot<span style="color: #66D9EF;">(</span>weights.T, np.dot<span style="color: #A6E22E;">(</span>rets.cov<span style="color: #E6DB74;">()</span> * <span style="color: #AE81FF;">252</span>, weights<span style="color: #A6E22E;">)</span><span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
    <span style="color: #F92672;">return</span> np.array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>pret, pvol, pret / pvol<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #F92672;">def</span> <span style="color: #A6E22E;">min_func_sharpe</span><span style="color: #AE81FF;">(</span>weights<span style="color: #AE81FF;">)</span>:
    <span style="color: #F92672;">return</span> -statistics<span style="color: #AE81FF;">(</span>weights<span style="color: #AE81FF;">)[</span><span style="color: #AE81FF;">2</span><span style="color: #AE81FF;">]</span>
<span style="color: #FD971F;">opts</span> = sco.minimize<span style="color: #AE81FF;">(</span>min_func_sharpe, noa * <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">1</span>. / noa,<span style="color: #66D9EF;">]</span>, method=<span style="color: #E6DB74;">'SLSQP'</span>,
                       bounds=bnds, constraints=cons<span style="color: #AE81FF;">)</span>
</pre>
</div>

<p>
output:
</p>
<div class="org-src-container">

<pre class="src src-python">opts
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">22</span><span style="color: #AE81FF;">]</span>:
     fun: -<span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">89964063622932411</span>
     jac: array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>  <span style="color: #AE81FF;">3</span>.<span style="color: #AE81FF;">65152955e</span>-<span style="color: #AE81FF;">05</span>,   <span style="color: #AE81FF;">2</span>.<span style="color: #AE81FF;">00167218e</span>+<span style="color: #AE81FF;">00</span>,  -<span style="color: #AE81FF;">1</span>.<span style="color: #AE81FF;">04084611e</span>-<span style="color: #AE81FF;">04</span>,
         <span style="color: #AE81FF;">3</span>.<span style="color: #AE81FF;">82214785e</span>-<span style="color: #AE81FF;">05</span>,   <span style="color: #AE81FF;">7</span>.<span style="color: #AE81FF;">63027400e</span>-<span style="color: #AE81FF;">01</span>,   <span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">00000000e</span>+<span style="color: #AE81FF;">00</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
 message: <span style="color: #E6DB74;">'Optimization terminated successfully.'</span>
    nfev: <span style="color: #AE81FF;">63</span>
     nit: <span style="color: #AE81FF;">9</span>
    njev: <span style="color: #AE81FF;">9</span>
  status: <span style="color: #AE81FF;">0</span>
 success: <span style="color: #AE81FF;">True</span>
       x: array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>  <span style="color: #AE81FF;">3</span>.<span style="color: #AE81FF;">16847434e</span>-<span style="color: #AE81FF;">01</span>,   <span style="color: #AE81FF;">8</span>.<span style="color: #AE81FF;">62049147e</span>-<span style="color: #AE81FF;">16</span>,   <span style="color: #AE81FF;">2</span>.<span style="color: #AE81FF;">64774759e</span>-<span style="color: #AE81FF;">01</span>,
         <span style="color: #AE81FF;">4</span>.<span style="color: #AE81FF;">18377806e</span>-<span style="color: #AE81FF;">01</span>,   <span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">00000000e</span>+<span style="color: #AE81FF;">00</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
</div></li>

<li><a id="orgheadline61"></a>algorithms for minimizing with constraints:<br  /><div class="outline-text-5" id="text-8-2-0-2">
<p>
scipy.optimize.minimize
scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
</p>
</div></li>


<div id="outline-container-orgheadline62" class="outline-4">
<h4 id="orgheadline62"><span class="section-number-4">8.2.1</span> Efficient frontier</h4>
</div>

<div id="outline-container-orgheadline64" class="outline-4">
<h4 id="orgheadline64"><span class="section-number-4">8.2.2</span> Capital Market Line</h4>
<div class="outline-text-4" id="text-8-2-2">
</div><ol class="org-ol"><li><a id="orgheadline63"></a>3 scenarios:<br  /><div class="outline-text-5" id="text-8-2-2-1">
<ul class="org-ul">
<li>solve minimum risk for maximum return above target .</li>
</ul>
<p>
Here we find the portfolio that minimizes
the return variance (which is associated with the risk of the portfolio) subject to achieving a minimum acceptable mean return rmin, and satisfying the portfolio
budget and no-shorting constraints.
</p>

<ul class="org-ul">
<li>solve maximum return for risk under target.</li>

<li>solve for minimum risk.</li>
<li>solve for maximum return.</li>
</ul>
</div></li></ol>
</div>
</div>
<div id="outline-container-orgheadline69" class="outline-3">
<h3 id="orgheadline69"><span class="section-number-3">8.3</span> Principal Component Analysis</h3>
<div class="outline-text-3" id="text-8-3">
<p>
<a href="./py4fi/PCA.html">PCA</a>
</p>
</div>

<div id="outline-container-orgheadline66" class="outline-4">
<h4 id="orgheadline66"><span class="section-number-4">8.3.1</span> The DAX index and its 30 stocks</h4>
</div>

<div id="outline-container-orgheadline67" class="outline-4">
<h4 id="orgheadline67"><span class="section-number-4">8.3.2</span> Applying PCA</h4>
</div>

<div id="outline-container-orgheadline68" class="outline-4">
<h4 id="orgheadline68"><span class="section-number-4">8.3.3</span> Constructing a PCA Index</h4>
</div>
</div>

<div id="outline-container-orgheadline72" class="outline-3">
<h3 id="orgheadline72"><span class="section-number-3">8.4</span> Bayesian Regression</h3>
<div class="outline-text-3" id="text-8-4">
<p>
<a href="./py4fi/BayesFormula.html">Bayes</a>
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
</p>
</div>

<div id="outline-container-orgheadline70" class="outline-4">
<h4 id="orgheadline70"><span class="section-number-4">8.4.1</span> Bayes's formula</h4>
</div>

<div id="outline-container-orgheadline71" class="outline-4">
<h4 id="orgheadline71"><span class="section-number-4">8.4.2</span> PyMC3</h4>
</div>
</div>
</div>

<div id="outline-container-orgheadline79" class="outline-2">
<h2 id="orgheadline79"><span class="section-number-2">9</span> Valuation Framework</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-orgheadline74" class="outline-3">
<h3 id="orgheadline74"><span class="section-number-3">9.1</span> Fundamental Theorem of Asset pricing</h3>
</div>

<div id="outline-container-orgheadline78" class="outline-3">
<h3 id="orgheadline78"><span class="section-number-3">9.2</span> Risk-Neutral discounting</h3>
<div class="outline-text-3" id="text-9-2">
</div><div id="outline-container-orgheadline75" class="outline-4">
<h4 id="orgheadline75"><span class="section-number-4">9.2.1</span> modeling and handling dates</h4>
</div>

<div id="outline-container-orgheadline76" class="outline-4">
<h4 id="orgheadline76"><span class="section-number-4">9.2.2</span> constant short rate</h4>
</div>

<div id="outline-container-orgheadline77" class="outline-4">
<h4 id="orgheadline77"><span class="section-number-4">9.2.3</span> Market environment</h4>
</div>
</div>
</div>

<div id="outline-container-orgheadline85" class="outline-2">
<h2 id="orgheadline85"><span class="section-number-2">10</span> Simulation of Financial Models</h2>
<div class="outline-text-2" id="text-10">
</div><div id="outline-container-orgheadline80" class="outline-3">
<h3 id="orgheadline80"><span class="section-number-3">10.1</span> Random Number Generation</h3>
</div>

<div id="outline-container-orgheadline81" class="outline-3">
<h3 id="orgheadline81"><span class="section-number-3">10.2</span> Generic Simulation Class</h3>
</div>

<div id="outline-container-orgheadline82" class="outline-3">
<h3 id="orgheadline82"><span class="section-number-3">10.3</span> Geometric Brownian Motion</h3>
</div>

<div id="outline-container-orgheadline83" class="outline-3">
<h3 id="orgheadline83"><span class="section-number-3">10.4</span> Jump Diffusion</h3>
</div>

<div id="outline-container-orgheadline84" class="outline-3">
<h3 id="orgheadline84"><span class="section-number-3">10.5</span> Square-Root Diffusion</h3>
</div>
</div>

<div id="outline-container-orgheadline90" class="outline-2">
<h2 id="orgheadline90"><span class="section-number-2">11</span> Derivatives Valuation</h2>
<div class="outline-text-2" id="text-11">
</div><div id="outline-container-orgheadline86" class="outline-3">
<h3 id="orgheadline86"><span class="section-number-3">11.1</span> Generic Valuation Class</h3>
</div>

<div id="outline-container-orgheadline87" class="outline-3">
<h3 id="orgheadline87"><span class="section-number-3">11.2</span> European Exercise</h3>
</div>

<div id="outline-container-orgheadline89" class="outline-3">
<h3 id="orgheadline89"><span class="section-number-3">11.3</span> American Excercise</h3>
<div class="outline-text-3" id="text-11-3">
</div><div id="outline-container-orgheadline88" class="outline-4">
<h4 id="orgheadline88"><span class="section-number-4">11.3.1</span> Least-Square Monte Carlo</h4>
</div>
</div>
</div>

<div id="outline-container-orgheadline93" class="outline-2">
<h2 id="orgheadline93"><span class="section-number-2">12</span> Portfolio Valuation</h2>
<div class="outline-text-2" id="text-12">
</div><div id="outline-container-orgheadline91" class="outline-3">
<h3 id="orgheadline91"><span class="section-number-3">12.1</span> Derivatives positions</h3>
</div>

<div id="outline-container-orgheadline92" class="outline-3">
<h3 id="orgheadline92"><span class="section-number-3">12.2</span> Derivatives portfolio</h3>
</div>
</div>

<div id="outline-container-orgheadline100" class="outline-2">
<h2 id="orgheadline100"><span class="section-number-2">13</span> Volatility Options</h2>
<div class="outline-text-2" id="text-13">
</div><div id="outline-container-orgheadline97" class="outline-3">
<h3 id="orgheadline97"><span class="section-number-3">13.1</span> The VSTOXX Data</h3>
<div class="outline-text-3" id="text-13-1">
</div><div id="outline-container-orgheadline94" class="outline-4">
<h4 id="orgheadline94"><span class="section-number-4">13.1.1</span> VSTOXX Index Data</h4>
</div>

<div id="outline-container-orgheadline95" class="outline-4">
<h4 id="orgheadline95"><span class="section-number-4">13.1.2</span> VSTOXX Futures Data</h4>
</div>

<div id="outline-container-orgheadline96" class="outline-4">
<h4 id="orgheadline96"><span class="section-number-4">13.1.3</span> VSTOXX Options Data</h4>
</div>
</div>

<div id="outline-container-orgheadline98" class="outline-3">
<h3 id="orgheadline98"><span class="section-number-3">13.2</span> Model Calibration</h3>
</div>

<div id="outline-container-orgheadline99" class="outline-3">
<h3 id="orgheadline99"><span class="section-number-3">13.3</span> American Options on the VSTOXX</h3>
</div>
</div>

<div id="outline-container-orgheadline101" class="outline-2">
<h2 id="orgheadline101"><span class="section-number-2">14</span> 非结构化数据可视化</h2>
</div>

<div id="outline-container-orgheadline108" class="outline-2">
<h2 id="orgheadline108"><span class="section-number-2">15</span> 最优化算法（锥优化、随机优化优先）</h2>
<div class="outline-text-2" id="text-15">
</div><div id="outline-container-orgheadline104" class="outline-3">
<h3 id="orgheadline104"><span class="section-number-3">15.1</span> Gradient descent</h3>
<div class="outline-text-3" id="text-15-1">
<p>
In optimization, gradient method is an algorithm to solve problems of the form \[min \f(x)\].
</p>
</div>

<div id="outline-container-orgheadline102" class="outline-4">
<h4 id="orgheadline102"><span class="section-number-4">15.1.1</span> Gradient descent</h4>
<div class="outline-text-4" id="text-15-1-1">
<p>
Gradient descent is a first-order iterative optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.
</p>

<p>
Limitations: For some of the above examples, gradient descent is relatively slow close to the minimum: technically, its asymptotic rate of convergence is inferior to many other methods. For poorly conditioned convex problems, gradient descent increasingly 'zigzags' as the gradients point nearly orthogonally to the shortest direction to a minimum point. For more details, see the comments below.
</p>

<p>
For non-differentiable functions, gradient methods are ill-defined.
</p>
</div>
</div>

<div id="outline-container-orgheadline103" class="outline-4">
<h4 id="orgheadline103"><span class="section-number-4">15.1.2</span> Conjugate gradient method</h4>
<div class="outline-text-4" id="text-15-1-2">
<p>
In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite.
\[Ax=b, u_tAv=0\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline107" class="outline-3">
<h3 id="orgheadline107"><span class="section-number-3">15.2</span> Stochastic optimization</h3>
<div class="outline-text-3" id="text-15-2">
<p>
Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involve random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.
</p>
</div>

<div id="outline-container-orgheadline105" class="outline-4">
<h4 id="orgheadline105"><span class="section-number-4">15.2.1</span> Random search</h4>
<div class="outline-text-4" id="text-15-2-1">
<p>
Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.
</p>

<p>
The name "random search" is attributed to Rastrigin who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search-space, which are sampled from a hypersphere surrounding the current position.
</p>

<p>
The basic RS algorithm can then be described as:
</p>

<p>
Initialize x with a random position in the search-space.
Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:
Sample a new position y from the hypersphere of a given radius surrounding the current position x (see e.g. Marsaglia's technique for sampling a hypersphere.)
If f(y) &lt; f(x) then move to the new position by setting x = y
</p>
</div>
</div>

<div id="outline-container-orgheadline106" class="outline-4">
<h4 id="orgheadline106"><span class="section-number-4">15.2.2</span> Bayesian optimization</h4>
<div class="outline-text-4" id="text-15-2-2">
<p>
Bayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-orgheadline109" class="outline-2">
<h2 id="orgheadline109"><span class="section-number-2">16</span> Graphical Models, e.g.,</h2>
<div class="outline-text-2" id="text-16">
<ul class="org-ul">
<li>Conditional Random Fields</li>
<li>Bayesian Networks</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline110" class="outline-2">
<h2 id="orgheadline110"><span class="section-number-2">17</span> Genetic Algorithm</h2>
<div class="outline-text-2" id="text-17">
<p>
In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.
The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.
</p>
</div>
</div>



<div id="outline-container-orgheadline111" class="outline-2">
<h2 id="orgheadline111"><span class="section-number-2">18</span> First order and Propositional Rule Based Systems, e.g.,</h2>
<div class="outline-text-2" id="text-18">
<ul class="org-ul">
<li>Tractable Markov Logic</li>
<li>Prolog</li>
<li>Lifted Inverse Deduction Algorithms</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline112" class="outline-2">
<h2 id="orgheadline112"><span class="section-number-2">19</span> Recurrent Nets, e.g.,</h2>
<div class="outline-text-2" id="text-19">
<ul class="org-ul">
<li>LSTM</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline113" class="outline-2">
<h2 id="orgheadline113"><span class="section-number-2">20</span> Natural language processing, e.g.</h2>
<div class="outline-text-2" id="text-20">
<ul class="org-ul">
<li>Auto text generation</li>
<li>Auto Text Summary</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline114" class="outline-2">
<h2 id="orgheadline114"><span class="section-number-2">21</span> Reinforcement Learning</h2>
</div>
<div id="outline-container-orgheadline121" class="outline-2">
<h2 id="orgheadline121"><span class="section-number-2">22</span> Decision Trees (ensambles)</h2>
<div class="outline-text-2" id="text-22">
</div><div id="outline-container-orgheadline115" class="outline-3">
<h3 id="orgheadline115"><span class="section-number-3">22.1</span> 数据处理：离散化</h3>
<div class="outline-text-3" id="text-22-1">
<p>
离散化指把连续型数据切分为若干“段”，也称bin，是数据分析中常用的手段。切分的原则有等距，等频，优化，或根据数据特点而定。在营销数据挖掘中，离散化得到普遍采用。究其原因，有这样几点：
</p>
<ul class="org-ul">
<li>算法需要。例如决策树，NaiveBayes等算法本身不能直接使用连续型变量，连续型数据只有经离散处理后才能进入算法引擎。</li>
<li>离散化可以有效地克服数据中隐藏的缺陷：使模型结果更加稳定。例如，数据中的极端值是影响模型效果的一个重要因素。极端值导致模型参数过高或过低，或导致模型被虚假现象“迷惑”，把原来不存在的关系作为重要模式来学习。而离散化，尤其是等距离散，可以有效地减弱极端值和异常值的影响.</li>
<li>有利于对非线性关系进行诊断和描述：对连续型数据进行离散处理后，自变量和目标变量之间的关系变得清晰化。如果两者之间是非线性关系，可以重新定义离散后变量每段的取值，如采取0，1的形式， 由一个变量派生为多个哑变量，分别确定每段和目标变量间的联系。这样做，虽然减少了模型的自由度，但可以大大提高模型的灵活度。</li>
<li>等距:将连续型变量的取值范围均匀划成n等份，每份的间距相等。例如，客户订阅刊物的时间是一个连续型变量，可以从几天到几年。采取等距切分可以把1年以下的客户划分成一组，1-2年的客户为一组，2-3年为一组..，以此类分，组距都是一年。</li>
<li>等频:把观察点均匀分为n等份，每份内包含的观察点数相同。还取上面的例子，设该杂志订户共有5万人，等频分段需要先把订户按订阅时间按顺序排列，排列好后可以按5000人一组，把全部订户均匀分为十段。</li>
<li>离散化处理不免要损失一部分信息。很显然，对连续型数据进行分段后，同一个段内的观察点之间的差异便消失了。</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline120" class="outline-3">
<h3 id="orgheadline120"><span class="section-number-3">22.2</span> 随机森林</h3>
<div class="outline-text-3" id="text-22-2">
<p>
随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。决策树是一种基本的分类器，一般是将特征分为两类（决策树也可以用来回归，不过本文中暂且不表）。构建好的决策树呈树形结构，可以认为是if-then规则的集合，主要优点是模型具有可读性，分类速度快。
</p>
</div>
<div id="outline-container-orgheadline119" class="outline-4">
<h4 id="orgheadline119"><span class="section-number-4">22.2.1</span> 随机森林的构建过程</h4>
<div class="outline-text-4" id="text-22-2-1">
</div><ol class="org-ol"><li><a id="orgheadline116"></a>数据的随机选取：<br  /><div class="outline-text-5" id="text-22-2-1-1">
<ul class="org-ul">
<li>从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。</li>
<li>第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。</li>
<li>最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</li>
</ul>
</div></li>
<li><a id="orgheadline117"></a>待选特征的随机选取<br  /><div class="outline-text-5" id="text-22-2-1-2">
<p>
与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。
</p>
</div></li>
<li><a id="orgheadline118"></a>Random Forest的具体使用-sklearn<br  /><div class="outline-text-5" id="text-22-2-1-3">
<p>
以上介绍了随机森林的工作原理，那么在python环境下，我们可以利用python环境下的sklearn包来帮助我们完成任务。举个小例子：
　　特征是通过收盘价数据计算的SMA，WMA，MOM指标，训练样本的特征是从2007-1-4到2016-6-2中截止前一天的SMA，WMA，MOM指标，训练样本的标类别是2007-1-4日到2016-6-2中每一天的涨跌情况，涨了就是True，跌了就是False，测试样本是2016-6-3日的三个指标以及涨跌情况。我们可以判定之后判断结果是正确还是错误，如果通过Random Forest判断的结果和当天的涨跌情况相符，则输出True，如果判断结果和当天的涨跌情况不符，则输出False。
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">import</span> talib
<span style="color: #F92672;">from</span> jqdata <span style="color: #F92672;">import</span> *
<span style="color: #FD971F;">test_stock</span> = <span style="color: #E6DB74;">'399300.XSHE'</span>
<span style="color: #FD971F;">start_date</span> = datetime.date<span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">2007</span>, <span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">4</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">end_date</span> = datetime.date<span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">2016</span>, <span style="color: #AE81FF;">6</span>, <span style="color: #AE81FF;">3</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">trading_days</span> = get_all_trade_days<span style="color: #AE81FF;">()</span>
<span style="color: #FD971F;">start_date_index</span> = trading_days.index<span style="color: #AE81FF;">(</span>start_date<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">end_date_index</span> = trading_days.index<span style="color: #AE81FF;">(</span>end_date<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">x_all</span> = <span style="color: #AE81FF;">[]</span>
<span style="color: #FD971F;">y_all</span> = <span style="color: #AE81FF;">[]</span>

<span style="color: #F92672;">for</span> index <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span><span style="color: #AE81FF;">(</span>start_date_index, end_date_index<span style="color: #AE81FF;">)</span>:    <span style="color: #75715E;"># </span><span style="color: #75715E;">&#24471;&#21040;&#35745;&#31639;&#25351;&#26631;&#30340;&#25152;&#26377;&#25968;&#25454;    start_day = trading_days[index - 30]    end_day = trading_days[index]    stock_data = get_price(test_stock, start_date=start_day, end_date=end_day, frequency='daily', fields=['close'])    close_prices = stock_data['close'].values        #&#36890;&#36807;&#25968;&#25454;&#35745;&#31639;&#25351;&#26631;    # -2&#26159;&#20445;&#35777;&#33719;&#21462;&#30340;&#25968;&#25454;&#26159;&#26152;&#22825;&#30340;&#65292;-1&#23601;&#26159;&#36890;&#36807;&#20170;&#22825;&#30340;&#25968;&#25454;&#35745;&#31639;&#20986;&#26469;&#30340;&#25351;&#26631;    sma_data = talib.SMA(close_prices)[-2]     wma_data = talib.WMA(close_prices)[-2]    mom_data = talib.MOM(close_prices)[-2]        features = []    features.append(sma_data)    features.append(wma_data)    features.append(mom_data)        label = False    if close_prices[-1] &gt; close_prices[-2]:        label = True    x_all.append(features)    y_all.append(label) # &#20934;&#22791;&#31639;&#27861;&#38656;&#35201;&#29992;&#21040;&#30340;&#25968;&#25454;</span>
 <span style="color: #FD971F;">x_train</span> = x_all<span style="color: #AE81FF;">[</span>: -<span style="color: #AE81FF;">1</span><span style="color: #AE81FF;">]</span>
 <span style="color: #FD971F;">y_train</span> = y_all<span style="color: #AE81FF;">[</span>: -<span style="color: #AE81FF;">1</span><span style="color: #AE81FF;">]</span>
 <span style="color: #FD971F;">x_test</span> = x_all<span style="color: #AE81FF;">[</span>-<span style="color: #AE81FF;">1</span><span style="color: #AE81FF;">]</span>
 <span style="color: #FD971F;">y_test</span> = y_all<span style="color: #AE81FF;">[</span>-<span style="color: #AE81FF;">1</span><span style="color: #AE81FF;">]</span>
 <span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span><span style="color: #E6DB74;">'data done'</span><span style="color: #AE81FF;">)</span>

 &#36755;&#20986;&#65306;
data done

<span style="color: #F92672;">from</span> sklearn.ensemble <span style="color: #F92672;">import</span> RandomForestClassifier
<span style="color: #75715E;">#</span><span style="color: #75715E;">&#24320;&#22987;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#65292;&#25324;&#21495;&#37324;&#38754;&#30340;n_estimators&#23601;&#26159;&#26862;&#26519;&#20013;&#21253;&#21547;&#30340;&#26641;&#30340;&#25968;&#30446;&#21862;</span>
<span style="color: #FD971F;">clf</span> = RandomForestClassifier<span style="color: #AE81FF;">(</span>n_estimators=<span style="color: #AE81FF;">10</span><span style="color: #AE81FF;">)</span>
<span style="color: #75715E;">#</span><span style="color: #75715E;">&#35757;&#32451;&#30340;&#20195;&#30721;clf.fit(x_train, y_train)</span>
<span style="color: #75715E;">#</span>
<span style="color: #FD971F;">&#24471;&#21040;&#27979;&#35797;&#32467;&#26524;&#30340;&#20195;&#30721;prediction</span> = clf.predict<span style="color: #AE81FF;">(</span>x_test<span style="color: #AE81FF;">)</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">&#30475;&#30475;&#39044;&#27979;&#23545;&#20102;&#27809;print(prediction == y_test)</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span><span style="color: #E6DB74;">'all done'</span><span style="color: #AE81FF;">)</span>
&#36755;&#20986;&#65306;
<span style="color: #AE81FF;">[</span> <span style="color: #AE81FF;">True</span><span style="color: #AE81FF;">]</span><span style="color: #F92672;">all</span> done
</pre>
</div>
</div></li></ol>
</div>
</div>
</div>
<div id="outline-container-orgheadline122" class="outline-2">
<h2 id="orgheadline122"><span class="section-number-2">23</span> Instance Based Learning</h2>
<div class="outline-text-2" id="text-23">
<ul class="org-ul">
<li>SVM</li>
<li>k-nearest neighbor</li>
<li>Amazon Netflix Recommendation system</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline123" class="outline-2">
<h2 id="orgheadline123"><span class="section-number-2">24</span> Times Series Analysis, e.g.,</h2>
<div class="outline-text-2" id="text-24">
<ul class="org-ul">
<li>Co-integration</li>
<li>VAR</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline124" class="outline-2">
<h2 id="orgheadline124"><span class="section-number-2">25</span> Ux design and Psychology</h2>
</div>
<div id="outline-container-orgheadline125" class="outline-2">
<h2 id="orgheadline125"><span class="section-number-2">26</span> Track</h2>
<div class="outline-text-2" id="text-26">
<div class="org-src-container">
<label class="org-src-name">标题区域</label>
<pre class="src src-org"><span style="color: #A6E22E;">| programming | level |</span>
<span style="color: #A6E22E;">|-------------+-------|</span>
<span style="color: #A6E22E;">| Lisp        |     1 |</span>
<span style="color: #A6E22E;">| VBA         |     3 |</span>
<span style="color: #A6E22E;">| C/C++       |     6 |</span>
<span style="color: #A6E22E;">| SQL         |     5 |</span>
<span style="color: #A6E22E;">| Matlab      |     5 |</span>
<span style="color: #A6E22E;">| R           |     4 |</span>
<span style="color: #A6E22E;">| Python      |     7 |</span>

<span style="color: #A6E22E;">| Machine Learning | Models                                           | level |</span>
<span style="color: #A6E22E;">| Neural Networks  | Convolutional neural network                     |     0 |</span>
<span style="color: #A6E22E;">|                  | long short-term memory                           |     0 |</span>
<span style="color: #A6E22E;">|                  | Autoencoder                                      |     0 |</span>
<span style="color: #A6E22E;">|                  | Bayesian networks                                |     1 |</span>
<span style="color: #A6E22E;">|                  | PCA                                              |     5 |</span>
<span style="color: #A6E22E;">|                  | K-Means                                          |     1 |</span>
<span style="color: #A6E22E;">|                  | SVM                                              |     1 |</span>
<span style="color: #A6E22E;">| Optimization     | Linear OLS(mean variance)                        |     4 |</span>
<span style="color: #A6E22E;">|                  | Genetic Algorithm                                |     0 |</span>
<span style="color: #A6E22E;">|                  | h(params,x)&#20989;&#25968;&#65306;hypothesis                      |     0 |</span>
<span style="color: #A6E22E;">|                  | J(params,x,y)&#20989;&#25968;&#65306;cost function                 |     0 |</span>
<span style="color: #A6E22E;">|                  | grad(params,x,y)&#20989;&#25968;&#65306;Gradient Descent           |     1 |</span>
<span style="color: #A6E22E;">| Time Series      | autoregressive(AR)                               |     1 |</span>
<span style="color: #A6E22E;">|                  | moving average (MA)                              |     2 |</span>
<span style="color: #A6E22E;">|                  | autoregressive moving average (ARMA)             |     1 |</span>
<span style="color: #A6E22E;">|                  | autoregressive integrated moving average (ARIMA) |     1 |</span>
</pre>
</div>
</div>
</div>
</div>
</body>
</html>
