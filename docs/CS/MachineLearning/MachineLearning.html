<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-11-22 Wed 19:34 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="weiwu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../styles/demo/css/demo.css"/>
<link href="https://fonts.proxy.ustclug.org/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "5em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "left",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline57">1. 机器学习与数据挖掘</a>
<ul>
<li><a href="#orgheadline8">1.1. 问题</a>
<ul>
<li><a href="#orgheadline1">1.1.1. 分类</a></li>
<li><a href="#orgheadline2">1.1.2. 聚类</a></li>
<li><a href="#orgheadline7">1.1.3. 回归</a></li>
</ul>
</li>
<li><a href="#orgheadline19">1.2. 监督学习(分类 · 回归)</a>
<ul>
<li><a href="#orgheadline9">1.2.1. 决策树</a></li>
<li><a href="#orgheadline10">1.2.2. 表征（装袋, 提升，随机森林）</a></li>
<li><a href="#orgheadline11">1.2.3. k-NN</a></li>
<li><a href="#orgheadline12">1.2.4. 线性回归</a></li>
<li><a href="#orgheadline13">1.2.5. 朴素贝叶斯</a></li>
<li><a href="#orgheadline14">1.2.6. 神经网络</a></li>
<li><a href="#orgheadline15">1.2.7. 逻辑回归</a></li>
<li><a href="#orgheadline16">1.2.8. 感知器</a></li>
<li><a href="#orgheadline17">1.2.9. 支持向量机（SVM）</a></li>
<li><a href="#orgheadline18">1.2.10. 相关向量机（RVM）</a></li>
</ul>
</li>
<li><a href="#orgheadline27">1.3. 聚类</a>
<ul>
<li><a href="#orgheadline20">1.3.1. BIRCH</a></li>
<li><a href="#orgheadline21">1.3.2. 层次</a></li>
<li><a href="#orgheadline22">1.3.3. k平均</a></li>
<li><a href="#orgheadline23">1.3.4. 期望最大化（EM）</a></li>
<li><a href="#orgheadline24">1.3.5. DBSCAN</a></li>
<li><a href="#orgheadline25">1.3.6. OPTICS</a></li>
<li><a href="#orgheadline26">1.3.7. 均值飘移</a></li>
</ul>
</li>
<li><a href="#orgheadline36">1.4. 降维</a>
<ul>
<li><a href="#orgheadline28">1.4.1. 因子分析</a></li>
<li><a href="#orgheadline29">1.4.2. CCA</a></li>
<li><a href="#orgheadline30">1.4.3. ICA</a></li>
<li><a href="#orgheadline31">1.4.4. LDA</a></li>
<li><a href="#orgheadline32">1.4.5. NMF</a></li>
<li><a href="#orgheadline33">1.4.6. PCA</a></li>
<li><a href="#orgheadline34">1.4.7. LASSO</a></li>
<li><a href="#orgheadline35">1.4.8. t-SNE</a></li>
</ul>
</li>
<li><a href="#orgheadline38">1.5. 结构预测</a>
<ul>
<li><a href="#orgheadline37">1.5.1. 概率图模型（贝叶斯网络，CRF, HMM）</a></li>
</ul>
</li>
<li><a href="#orgheadline41">1.6. 异常检测</a>
<ul>
<li><a href="#orgheadline39">1.6.1. k-NN</a></li>
<li><a href="#orgheadline40">1.6.2. 局部离群因子</a></li>
</ul>
</li>
<li><a href="#orgheadline49">1.7. 神经网络</a>
<ul>
<li><a href="#orgheadline42">1.7.1. 自编码</a></li>
<li><a href="#orgheadline43">1.7.2. 深度学习</a></li>
<li><a href="#orgheadline44">1.7.3. 多层感知机</a></li>
<li><a href="#orgheadline45">1.7.4. RNN</a></li>
<li><a href="#orgheadline46">1.7.5. 受限玻尔兹曼机</a></li>
<li><a href="#orgheadline47">1.7.6. SOM</a></li>
<li><a href="#orgheadline48">1.7.7. CNN</a></li>
</ul>
</li>
<li><a href="#orgheadline56">1.8. 理论</a>
<ul>
<li><a href="#orgheadline50">1.8.1. 偏差/方差困境</a></li>
<li><a href="#orgheadline51">1.8.2. 计算学习理论</a></li>
<li><a href="#orgheadline52">1.8.3. 经验风险最小化</a></li>
<li><a href="#orgheadline53">1.8.4. PAC学习</a></li>
<li><a href="#orgheadline54">1.8.5. 统计学习</a></li>
<li><a href="#orgheadline55">1.8.6. VC理论</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline69">2. Week 1</a>
<ul>
<li><a href="#orgheadline58">2.1. 基础库使用1</a></li>
<li><a href="#orgheadline59">2.2. 基础库使用2</a></li>
<li><a href="#orgheadline60">2.3. What is machine learning</a></li>
<li><a href="#orgheadline63">2.4. Well defined machine learning problem</a>
<ul>
<li><a href="#orgheadline61">2.4.1. Top-Down induction of DTree</a></li>
<li><a href="#orgheadline62">2.4.2. Entropy</a></li>
</ul>
</li>
<li><a href="#orgheadline64">2.5. Course logistics</a></li>
<li><a href="#orgheadline65">2.6. Model Representation</a></li>
<li><a href="#orgheadline66">2.7. Cost Function</a></li>
<li><a href="#orgheadline67">2.8. Linear Regression with One Variable</a></li>
<li><a href="#orgheadline68">2.9. Linear Algebra Review</a></li>
</ul>
</li>
<li><a href="#orgheadline70">3. Week 2 Linear Regression with Multiple Variables</a></li>
<li><a href="#orgheadline72">4. Week 5</a>
<ul>
<li><a href="#orgheadline71">4.1. Neural Networks: Learning</a></li>
</ul>
</li>
<li><a href="#orgheadline75">5. Week 6</a>
<ul>
<li><a href="#orgheadline73">5.1. Advice for Applying Machine Learning</a></li>
<li><a href="#orgheadline74">5.2. Machine Learning System Design</a></li>
</ul>
</li>
<li><a href="#orgheadline80">6. Week 7</a>
<ul>
<li><a href="#orgheadline79">6.1. Support Vector Machine</a>
<ul>
<li><a href="#orgheadline76">6.1.1. Maximal Margin Classifier</a></li>
<li><a href="#orgheadline77">6.1.2. Support Vector Classifiers</a></li>
<li><a href="#orgheadline78">6.1.3. Support Vector Machines</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline83">7. Week 8</a>
<ul>
<li><a href="#orgheadline81">7.1. Unsupervised Learning</a></li>
<li><a href="#orgheadline82">7.2. Dimensionality Reduction</a></li>
</ul>
</li>
<li><a href="#orgheadline86">8. Week 9</a>
<ul>
<li><a href="#orgheadline84">8.1. Anomaly Detection</a></li>
<li><a href="#orgheadline85">8.2. Recommender Systems</a></li>
</ul>
</li>
<li><a href="#orgheadline88">9. Week 10</a>
<ul>
<li><a href="#orgheadline87">9.1. Large Scale Machine Learning</a></li>
</ul>
</li>
<li><a href="#orgheadline90">10. Week 11</a>
<ul>
<li><a href="#orgheadline89">10.1. Application Example: Photo OCR</a></li>
</ul>
</li>
<li><a href="#orgheadline91">11. Bayesian network</a></li>
<li><a href="#orgheadline94">12. Tree-Based Methods</a>
<ul>
<li><a href="#orgheadline92">12.1. Decision Trees</a></li>
<li><a href="#orgheadline93">12.2. Bagging, Random Forests, Boosting</a></li>
</ul>
</li>
<li><a href="#orgheadline100">13. Unsupervised Learning</a>
<ul>
<li><a href="#orgheadline95">13.1. Principal Components Analysis</a></li>
<li><a href="#orgheadline99">13.2. Clustering Methods</a>
<ul>
<li><a href="#orgheadline96">13.2.1. K-Means Clustering</a></li>
<li><a href="#orgheadline97">13.2.2. K-Nearest Neighbors(KNN)</a></li>
<li><a href="#orgheadline98">13.2.3. Hierarchical Clustering</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline103">14. Resampling Methods</a>
<ul>
<li><a href="#orgheadline101">14.1. Cross-Validation</a></li>
<li><a href="#orgheadline102">14.2. The Bootstrap</a></li>
</ul>
</li>
<li><a href="#orgheadline104">15. Google Natural Language Processing</a></li>
<li><a href="#orgheadline105">16. Outline of Machine Learning</a></li>
<li><a href="#orgheadline110">17. Scikit-lean API</a>
<ul>
<li><a href="#orgheadline106">17.1. basics of the API</a></li>
<li><a href="#orgheadline109">17.2. feature engineering</a>
<ul>
<li><a href="#orgheadline107">17.2.1. Imputation of Missing Data</a></li>
<li><a href="#orgheadline108">17.2.2. Feature Pipelines</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline57" class="outline-2">
<h2 id="orgheadline57"><span class="section-number-2">1</span> 机器学习与数据挖掘</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">1.1</span> 问题</h3>
<div class="outline-text-3" id="text-1-1">
</div><div id="outline-container-orgheadline1" class="outline-4">
<h4 id="orgheadline1"><span class="section-number-4">1.1.1</span> 分类</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。分类问题也被称为监督式学习(supervised classification)，根据已知训练区提供的样本，通过计算选择特征参数，建立判别函数以对样本进行的分类。 与之相对的称为非监督式学习(unsupervised classification)，也叫做聚类分析。
</p>
</div>
</div>
<div id="outline-container-orgheadline2" class="outline-4">
<h4 id="orgheadline2"><span class="section-number-4">1.1.2</span> 聚类</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
聚类分析（英语：Cluster analysis，亦称为群集分析）是对于统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。
</p>
</div>
</div>
<div id="outline-container-orgheadline7" class="outline-4">
<h4 id="orgheadline7"><span class="section-number-4">1.1.3</span> 回归</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
回归分析（英语：Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。
回归分析是建立因变数\( {\displaystyle Y} Y\)（或称依变数，反应变数）与自变数 \({\displaystyle X} X\)（或称独变数，解释变数）之间关系的模型。简单线性回归使用一个自变量$ {\displaystyle X} X\(，复回归使用超过一个自变量\)（ {\displaystyle X<sub>1</sub>,X<sub>2</sub>&#x2026;X<sub>i</sub>} X<sub>1</sub>,X<sub>2</sub>&#x2026;X<sub>i</sub>）$。
</p>
</div>

<ol class="org-ol"><li><a id="orgheadline3"></a>模型<br  /><div class="outline-text-5" id="text-1-1-3-1">
<p>
线性回归 简单回归 普通最小二乘法 多项式回归 一般线性模型
</p>

<p>
广义线性模式 离散选择 逻辑回归 多项罗吉特 混合罗吉特 波比 多项式波比 排序性模型 有序波比 泊松回归
</p>

<p>
等级线性模型 固定效应 随机效应 混合模型
</p>

<p>
非线性回归 非参数 半参数 稳健 分位数回归 保序回归 主成分 最小角 局部 分段
含误差变量
</p>
</div></li>

<li><a id="orgheadline4"></a>估计<br  /><div class="outline-text-5" id="text-1-1-3-2">
<p>
最小二乘法 普通最小二乘法 线性 偏最小二乘回归 总体 广义 加权 非线性 非负 重复再加权 岭回归 LASSO
</p>

<p>
最小绝对值导数法 贝叶斯 贝叶斯多元
</p>
</div></li>
<li><a id="orgheadline5"></a>背景<br  /><div class="outline-text-5" id="text-1-1-3-3">
<p>
回归模型检验 平均响应和预测响应 误差和残差 拟合优度 学生化残差 高斯－马尔可夫定理
</p>
</div></li>
<li><a id="orgheadline6"></a>异常检测<br  /><div class="outline-text-5" id="text-1-1-3-4">
<p>
在数据挖掘中，异常检测（英语：anomaly detection）对不匹配预期模式或数据集中其他项目的项目、事件或观测值的识别。 通常异常项目会转变成银行欺诈、结构缺陷、医疗问题、文本错误等类型的问题。异常也被称为离群值、新奇、噪声、偏差和例外。
特别是在检测滥用与网络入侵时，有趣性对象往往不是罕见对象，但却是超出预料的突发活动。这种模式不遵循通常统计定义中把异常点看作是罕见对象，于是许多异常检测方法（特别是无监督的方法）将对此类数据失效，除非进行了合适的聚集。相反，聚类分析算法可能可以检测出这些模式形成的微聚类。
</p>

<p>
有三大类异常检测算法。 在假设数据集中大多数实例都是正常的前提下，无监督异常检测方法能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。监督式异常检测方法需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的统计分类问题的关键区别是异常检测的内在不均衡性）。半监督式异常检测方法根据一个给定的正常训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。
关联规则 强化学习 结构预测 特征学习 在线学习 半监督学习 语法归纳
</p>
</div></li></ol>
</div>
</div>
<div id="outline-container-orgheadline19" class="outline-3">
<h3 id="orgheadline19"><span class="section-number-3">1.2</span> 监督学习(分类 · 回归)</h3>
<div class="outline-text-3" id="text-1-2">
</div><div id="outline-container-orgheadline9" class="outline-4">
<h4 id="orgheadline9"><span class="section-number-4">1.2.1</span> 决策树</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
<a href="./DecisionTree.html">DecisionTree</a>
决策论中 （如风险管理），决策树（Decision tree）由一个决策图和可能的结果（包括资源成本和风险）组成， 用来创建到达目标的规划。决策树建立并用来辅助决策，是一种特殊的树结构。决策树是一个利用像树一样的图形或决策模型的决策支持工具，包括随机事件结果，资源代价和实用性。它是一个算法显示的方法。决策树经常在运筹学中使用，特别是在决策分析中，它帮助确定一个能最可能达到目标的策略。如果在实际中，决策不得不在没有完备知识的情况下被在线采用，一个决策树应该平行概率模型作为最佳的选择模型或在线选择模型算法。决策树的另一个使用是作为计算条件概率的描述性手段。
</p>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="orgheadline10"><span class="section-number-4">1.2.2</span> 表征（装袋, 提升，随机森林）</h4>
</div>
<div id="outline-container-orgheadline11" class="outline-4">
<h4 id="orgheadline11"><span class="section-number-4">1.2.3</span> k-NN</h4>
</div>
<div id="outline-container-orgheadline12" class="outline-4">
<h4 id="orgheadline12"><span class="section-number-4">1.2.4</span> 线性回归</h4>
</div>
<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13"><span class="section-number-4">1.2.5</span> 朴素贝叶斯</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
<a href="./NaiveBayes.html">Naive Bayes Classifcation</a>
</p>
</div>
</div>
<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="orgheadline14"><span class="section-number-4">1.2.6</span> 神经网络</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
<a href="http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html">http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html</a>
</p>
</div>
</div>
<div id="outline-container-orgheadline15" class="outline-4">
<h4 id="orgheadline15"><span class="section-number-4">1.2.7</span> 逻辑回归</h4>
</div>
<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="orgheadline16"><span class="section-number-4">1.2.8</span> 感知器</h4>
</div>
<div id="outline-container-orgheadline17" class="outline-4">
<h4 id="orgheadline17"><span class="section-number-4">1.2.9</span> 支持向量机（SVM）</h4>
</div>
<div id="outline-container-orgheadline18" class="outline-4">
<h4 id="orgheadline18"><span class="section-number-4">1.2.10</span> 相关向量机（RVM）</h4>
</div>
</div>
<div id="outline-container-orgheadline27" class="outline-3">
<h3 id="orgheadline27"><span class="section-number-3">1.3</span> 聚类</h3>
<div class="outline-text-3" id="text-1-3">
</div><div id="outline-container-orgheadline20" class="outline-4">
<h4 id="orgheadline20"><span class="section-number-4">1.3.1</span> BIRCH</h4>
</div>
<div id="outline-container-orgheadline21" class="outline-4">
<h4 id="orgheadline21"><span class="section-number-4">1.3.2</span> 层次</h4>
</div>
<div id="outline-container-orgheadline22" class="outline-4">
<h4 id="orgheadline22"><span class="section-number-4">1.3.3</span> k平均</h4>
</div>
<div id="outline-container-orgheadline23" class="outline-4">
<h4 id="orgheadline23"><span class="section-number-4">1.3.4</span> 期望最大化（EM）</h4>
</div>
<div id="outline-container-orgheadline24" class="outline-4">
<h4 id="orgheadline24"><span class="section-number-4">1.3.5</span> DBSCAN</h4>
</div>
<div id="outline-container-orgheadline25" class="outline-4">
<h4 id="orgheadline25"><span class="section-number-4">1.3.6</span> OPTICS</h4>
</div>
<div id="outline-container-orgheadline26" class="outline-4">
<h4 id="orgheadline26"><span class="section-number-4">1.3.7</span> 均值飘移</h4>
</div>
</div>
<div id="outline-container-orgheadline36" class="outline-3">
<h3 id="orgheadline36"><span class="section-number-3">1.4</span> 降维</h3>
<div class="outline-text-3" id="text-1-4">
</div><div id="outline-container-orgheadline28" class="outline-4">
<h4 id="orgheadline28"><span class="section-number-4">1.4.1</span> 因子分析</h4>
</div>
<div id="outline-container-orgheadline29" class="outline-4">
<h4 id="orgheadline29"><span class="section-number-4">1.4.2</span> CCA</h4>
</div>
<div id="outline-container-orgheadline30" class="outline-4">
<h4 id="orgheadline30"><span class="section-number-4">1.4.3</span> ICA</h4>
</div>
<div id="outline-container-orgheadline31" class="outline-4">
<h4 id="orgheadline31"><span class="section-number-4">1.4.4</span> LDA</h4>
</div>
<div id="outline-container-orgheadline32" class="outline-4">
<h4 id="orgheadline32"><span class="section-number-4">1.4.5</span> NMF</h4>
</div>
<div id="outline-container-orgheadline33" class="outline-4">
<h4 id="orgheadline33"><span class="section-number-4">1.4.6</span> PCA</h4>
<div class="outline-text-4" id="text-1-4-6">
<ul class="org-ul">
<li>Data preprocessing</li>
</ul>
<p>
feature scaling/mean normalization
\[\mu_j=\frac{1}{m} \sum_{i=1}^m x_j^{(i)}\]
replace each \(x_j^{(i)}\) with \(x_i-\mu_j\)
</p>
</div>
</div>
<div id="outline-container-orgheadline34" class="outline-4">
<h4 id="orgheadline34"><span class="section-number-4">1.4.7</span> LASSO</h4>
<div class="outline-text-4" id="text-1-4-7">
<p>
PCA vs Lasso:
unsupervised, supervised.
</p>
</div>
</div>
<div id="outline-container-orgheadline35" class="outline-4">
<h4 id="orgheadline35"><span class="section-number-4">1.4.8</span> t-SNE</h4>
</div>
</div>
<div id="outline-container-orgheadline38" class="outline-3">
<h3 id="orgheadline38"><span class="section-number-3">1.5</span> 结构预测</h3>
<div class="outline-text-3" id="text-1-5">
</div><div id="outline-container-orgheadline37" class="outline-4">
<h4 id="orgheadline37"><span class="section-number-4">1.5.1</span> 概率图模型（贝叶斯网络，CRF, HMM）</h4>
</div>
</div>
<div id="outline-container-orgheadline41" class="outline-3">
<h3 id="orgheadline41"><span class="section-number-3">1.6</span> 异常检测</h3>
<div class="outline-text-3" id="text-1-6">
</div><div id="outline-container-orgheadline39" class="outline-4">
<h4 id="orgheadline39"><span class="section-number-4">1.6.1</span> k-NN</h4>
</div>
<div id="outline-container-orgheadline40" class="outline-4">
<h4 id="orgheadline40"><span class="section-number-4">1.6.2</span> 局部离群因子</h4>
</div>
</div>
<div id="outline-container-orgheadline49" class="outline-3">
<h3 id="orgheadline49"><span class="section-number-3">1.7</span> 神经网络</h3>
<div class="outline-text-3" id="text-1-7">
</div><div id="outline-container-orgheadline42" class="outline-4">
<h4 id="orgheadline42"><span class="section-number-4">1.7.1</span> 自编码</h4>
</div>
<div id="outline-container-orgheadline43" class="outline-4">
<h4 id="orgheadline43"><span class="section-number-4">1.7.2</span> 深度学习</h4>
<div class="outline-text-4" id="text-1-7-2">
<p>
<a href="./DeepLearning.html">DeepLearning</a>
</p>
</div>
</div>
<div id="outline-container-orgheadline44" class="outline-4">
<h4 id="orgheadline44"><span class="section-number-4">1.7.3</span> 多层感知机</h4>
</div>
<div id="outline-container-orgheadline45" class="outline-4">
<h4 id="orgheadline45"><span class="section-number-4">1.7.4</span> RNN</h4>
</div>
<div id="outline-container-orgheadline46" class="outline-4">
<h4 id="orgheadline46"><span class="section-number-4">1.7.5</span> 受限玻尔兹曼机</h4>
</div>
<div id="outline-container-orgheadline47" class="outline-4">
<h4 id="orgheadline47"><span class="section-number-4">1.7.6</span> SOM</h4>
</div>
<div id="outline-container-orgheadline48" class="outline-4">
<h4 id="orgheadline48"><span class="section-number-4">1.7.7</span> CNN</h4>
</div>
</div>
<div id="outline-container-orgheadline56" class="outline-3">
<h3 id="orgheadline56"><span class="section-number-3">1.8</span> 理论</h3>
<div class="outline-text-3" id="text-1-8">
</div><div id="outline-container-orgheadline50" class="outline-4">
<h4 id="orgheadline50"><span class="section-number-4">1.8.1</span> 偏差/方差困境</h4>
</div>
<div id="outline-container-orgheadline51" class="outline-4">
<h4 id="orgheadline51"><span class="section-number-4">1.8.2</span> 计算学习理论</h4>
</div>
<div id="outline-container-orgheadline52" class="outline-4">
<h4 id="orgheadline52"><span class="section-number-4">1.8.3</span> 经验风险最小化</h4>
</div>
<div id="outline-container-orgheadline53" class="outline-4">
<h4 id="orgheadline53"><span class="section-number-4">1.8.4</span> PAC学习</h4>
</div>
<div id="outline-container-orgheadline54" class="outline-4">
<h4 id="orgheadline54"><span class="section-number-4">1.8.5</span> 统计学习</h4>
</div>
<div id="outline-container-orgheadline55" class="outline-4">
<h4 id="orgheadline55"><span class="section-number-4">1.8.6</span> VC理论</h4>
</div>
</div>
</div>

<div id="outline-container-orgheadline69" class="outline-2">
<h2 id="orgheadline69"><span class="section-number-2">2</span> Week 1</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgheadline58" class="outline-3">
<h3 id="orgheadline58"><span class="section-number-3">2.1</span> <a href="./BasicModule.html">基础库使用1</a></h3>
</div>

<div id="outline-container-orgheadline59" class="outline-3">
<h3 id="orgheadline59"><span class="section-number-3">2.2</span> <a href="./BasicModule2.html">基础库使用2</a></h3>
</div>

<div id="outline-container-orgheadline60" class="outline-3">
<h3 id="orgheadline60"><span class="section-number-3">2.3</span> What is machine learning</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Study of algorithms that
</p>
<ul class="org-ul">
<li>improve their performance P</li>
<li>at some task T</li>
</ul>
<p>
training data set, validation data set, test data set.
</p>
<ul class="org-ul">
<li>with experience E</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline63" class="outline-3">
<h3 id="orgheadline63"><span class="section-number-3">2.4</span> Well defined machine learning problem</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>supervised learning</li>
</ul>

<p>
Fitting some data to a function or function approximation.
</p>

<ul class="org-ul">
<li>unsupervised learning</li>
</ul>

<p>
Figuring out what the data is without any feedback. For instance, if we were given many data points, we could group them by similarity, or perhaps determine which variables are better than others.
</p>

<p>
\[H = {H|h: X \to Y}\]
</p>
</div>

<div id="outline-container-orgheadline61" class="outline-4">
<h4 id="orgheadline61"><span class="section-number-4">2.4.1</span> Top-Down induction of DTree</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>A to the best decision attribute for next node.</li>
<li>Assign A as decision attribute for node.</li>
<li>For each value of A, create new descendant of node.</li>
<li>Sort training examples to leaf nodes.</li>
<li>If training examples perfectly classified, then STOP, Else iterate over new leaf nodes.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline62" class="outline-4">
<h4 id="orgheadline62"><span class="section-number-4">2.4.2</span> Entropy</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
Entropy H(X) of a random variable X:
\[H(X) = -\sum{P(X=i)log_2*P(X=i)}\]
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline64" class="outline-3">
<h3 id="orgheadline64"><span class="section-number-3">2.5</span> Course logistics</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Neural Networks</li>
<li>Support Vector Machines</li>
<li>K-means Clustering</li>
<li>Principal Components Analysis</li>
<li>Anomaly Detection</li>
<li>Collaborative Filtering</li>
<li>Object Recognition</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline65" class="outline-3">
<h3 id="orgheadline65"><span class="section-number-3">2.6</span> Model Representation</h3>
<div class="outline-text-3" id="text-2-6">
<p>
To establish notation for future use, we’ll use x(i) to denote the “input” variables (living area in this example),
also called input features, and y(i) to denote the “output” or target variable that we are trying to predict (price).
(x(i),y(i)) is called a training example.
m—is called a training set.
</p>
</div>
</div>

<div id="outline-container-orgheadline66" class="outline-3">
<h3 id="orgheadline66"><span class="section-number-3">2.7</span> Cost Function</h3>
<div class="outline-text-3" id="text-2-7">
<p>
\(J(\Theta_1,\Theta_2)\)
contour is the bow projected on the 2D surface.
A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line.
</p>
</div>
</div>

<div id="outline-container-orgheadline67" class="outline-3">
<h3 id="orgheadline67"><span class="section-number-3">2.8</span> Linear Regression with One Variable</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li><span class="timestamp-wrapper"><span class="timestamp">&lt;2017-08-14 Mon&gt;</span></span></li>
</ul>
<p>
linear regression with one variable is also called simple regression. The X variable is called the predictor, Y is called the dependent.
</p>
<ul class="org-ul">
<li>some statistics:
<ul class="org-ul">
<li>t-statistics
off value according to the \(/epsilon\).</li>
<li>p value
the probability of the hypothesis that \(/beta_1\) is 0.</li>
<li>\(R^2\)
    the confidence level that \(/beta_1\) is approximately estimated.</li>
<li>RSS
residual sum of squares</li>
<li>RSE
residual standard error.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline68" class="outline-3">
<h3 id="orgheadline68"><span class="section-number-3">2.9</span> Linear Algebra Review</h3>
<div class="outline-text-3" id="text-2-9">
<ul class="org-ul">
<li>Vector</li>
<li>Matrix</li>
<li></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline70" class="outline-2">
<h2 id="orgheadline70"><span class="section-number-2">3</span> Week 2 Linear Regression with Multiple Variables</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Gradient Descent:</li>
</ul>
<p>
Taking the derivative (the tangential line to a function) of our cost function.
The slope of the tangent is the derivative at that point and it will give us a direction to move towards.
We make steps down the cost function in the direction with the steepest descent.
The size of each step is determined by the parameter α, which is called the learning rate.
</p>
<ul class="org-ul">
<li>Algorithm:</li>
</ul>
<p>
\[\Theta_j = \Theta_j + \alpha \derivative{J(\Theta_0,\Theta_1)}\]
Update simutaneously:
\[Temp_0 := \Theta_0 - \alpha \triangledown J(\Theta_0,\Theta_1)} \]
\[Temp_1 := \Theta_1 - \alpha \bigtriangledown J(\Theta_0,\Theta_1) \]
</p>
<ul class="org-ul">
<li>normalization</li>
</ul>
<p>
\[\theta_0 = \Theta_0 - \alpha\partial{J(\theta)}{\theta}\]
</p>
</div>
</div>

<div id="outline-container-orgheadline72" class="outline-2">
<h2 id="orgheadline72"><span class="section-number-2">4</span> Week 5</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline71" class="outline-3">
<h3 id="orgheadline71"><span class="section-number-3">4.1</span> Neural Networks: Learning</h3>
</div>
</div>
<div id="outline-container-orgheadline75" class="outline-2">
<h2 id="orgheadline75"><span class="section-number-2">5</span> Week 6</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orgheadline73" class="outline-3">
<h3 id="orgheadline73"><span class="section-number-3">5.1</span> Advice for Applying Machine Learning</h3>
</div>
<div id="outline-container-orgheadline74" class="outline-3">
<h3 id="orgheadline74"><span class="section-number-3">5.2</span> Machine Learning System Design</h3>
</div>
</div>
<div id="outline-container-orgheadline80" class="outline-2">
<h2 id="orgheadline80"><span class="section-number-2">6</span> Week 7</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgheadline79" class="outline-3">
<h3 id="orgheadline79"><span class="section-number-3">6.1</span> Support Vector Machine</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression.
Given a set of training examples, each marked as belonging to one of two categories,
an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.
</p>
</div>

<div id="outline-container-orgheadline76" class="outline-4">
<h4 id="orgheadline76"><span class="section-number-4">6.1.1</span> Maximal Margin Classifier</h4>
</div>

<div id="outline-container-orgheadline77" class="outline-4">
<h4 id="orgheadline77"><span class="section-number-4">6.1.2</span> Support Vector Classifiers</h4>
</div>

<div id="outline-container-orgheadline78" class="outline-4">
<h4 id="orgheadline78"><span class="section-number-4">6.1.3</span> Support Vector Machines</h4>
<div class="outline-text-4" id="text-6-1-3">
<div class="org-src-container">

<pre class="src src-emacs-lisp">from sklearn import svm
training_X = target
training_y = target names
svm_model = svm.SVC<span style="color: #AE81FF;">(</span>gamma=0.01, C=100.<span style="color: #AE81FF;">)</span>
svm_model.fit<span style="color: #AE81FF;">(</span>training_X, training_y<span style="color: #AE81FF;">)</span>
predicts = svm_model.predict<span style="color: #AE81FF;">(</span>test_X<span style="color: #AE81FF;">)</span>
from sklearn.metrics import accuracy_score
accuracy_score<span style="color: #AE81FF;">(</span>y_true, predicts<span style="color: #AE81FF;">)</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgheadline83" class="outline-2">
<h2 id="orgheadline83"><span class="section-number-2">7</span> Week 8</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgheadline81" class="outline-3">
<h3 id="orgheadline81"><span class="section-number-3">7.1</span> Unsupervised Learning</h3>
</div>
<div id="outline-container-orgheadline82" class="outline-3">
<h3 id="orgheadline82"><span class="section-number-3">7.2</span> Dimensionality Reduction</h3>
</div>
</div>
<div id="outline-container-orgheadline86" class="outline-2">
<h2 id="orgheadline86"><span class="section-number-2">8</span> Week 9</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-orgheadline84" class="outline-3">
<h3 id="orgheadline84"><span class="section-number-3">8.1</span> Anomaly Detection</h3>
</div>
<div id="outline-container-orgheadline85" class="outline-3">
<h3 id="orgheadline85"><span class="section-number-3">8.2</span> Recommender Systems</h3>
</div>
</div>
<div id="outline-container-orgheadline88" class="outline-2">
<h2 id="orgheadline88"><span class="section-number-2">9</span> Week 10</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-orgheadline87" class="outline-3">
<h3 id="orgheadline87"><span class="section-number-3">9.1</span> Large Scale Machine Learning</h3>
</div>
</div>
<div id="outline-container-orgheadline90" class="outline-2">
<h2 id="orgheadline90"><span class="section-number-2">10</span> Week 11</h2>
<div class="outline-text-2" id="text-10">
</div><div id="outline-container-orgheadline89" class="outline-3">
<h3 id="orgheadline89"><span class="section-number-3">10.1</span> Application Example: Photo OCR</h3>
</div>
</div>
<div id="outline-container-orgheadline91" class="outline-2">
<h2 id="orgheadline91"><span class="section-number-2">11</span> Bayesian network</h2>
<div class="outline-text-2" id="text-11">
<p>
<a href="./BayesianNetworkDescribingYourData.html">BayesianNetworkDescribingYourData</a>
</p>
</div>
</div>

<div id="outline-container-orgheadline94" class="outline-2">
<h2 id="orgheadline94"><span class="section-number-2">12</span> Tree-Based Methods</h2>
<div class="outline-text-2" id="text-12">
</div><div id="outline-container-orgheadline92" class="outline-3">
<h3 id="orgheadline92"><span class="section-number-3">12.1</span> Decision Trees</h3>
<div class="outline-text-3" id="text-12-1">
<p>
如何利用熵分类：
怎么样贪心地分类让熵以最快的速度降低。
</p>

<p>
信息增益：
给定特征A的信息而使得类X的信息的不确定性减少的程度。
</p>
</div>
</div>
<div id="outline-container-orgheadline93" class="outline-3">
<h3 id="orgheadline93"><span class="section-number-3">12.2</span> Bagging, Random Forests, Boosting</h3>
</div>
</div>

<div id="outline-container-orgheadline100" class="outline-2">
<h2 id="orgheadline100"><span class="section-number-2">13</span> Unsupervised Learning</h2>
<div class="outline-text-2" id="text-13">
</div><div id="outline-container-orgheadline95" class="outline-3">
<h3 id="orgheadline95"><span class="section-number-3">13.1</span> Principal Components Analysis</h3>
<div class="outline-text-3" id="text-13-1">
<ul class="org-ul">
<li>first principal component</li>
<li>second principal component</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline99" class="outline-3">
<h3 id="orgheadline99"><span class="section-number-3">13.2</span> Clustering Methods</h3>
<div class="outline-text-3" id="text-13-2">
<p>
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.
</p>
</div>

<div id="outline-container-orgheadline96" class="outline-4">
<h4 id="orgheadline96"><span class="section-number-4">13.2.1</span> K-Means Clustering</h4>
<div class="outline-text-4" id="text-13-2-1">
<p>
which group of stocks could possibly move up in the next trading period.
</p>
</div>
</div>

<div id="outline-container-orgheadline97" class="outline-4">
<h4 id="orgheadline97"><span class="section-number-4">13.2.2</span> K-Nearest Neighbors(KNN)</h4>
</div>

<div id="outline-container-orgheadline98" class="outline-4">
<h4 id="orgheadline98"><span class="section-number-4">13.2.3</span> Hierarchical Clustering</h4>
</div>
</div>
</div>
<div id="outline-container-orgheadline103" class="outline-2">
<h2 id="orgheadline103"><span class="section-number-2">14</span> Resampling Methods</h2>
<div class="outline-text-2" id="text-14">
</div><div id="outline-container-orgheadline101" class="outline-3">
<h3 id="orgheadline101"><span class="section-number-3">14.1</span> Cross-Validation</h3>
</div>

<div id="outline-container-orgheadline102" class="outline-3">
<h3 id="orgheadline102"><span class="section-number-3">14.2</span> The Bootstrap</h3>
</div>
</div>

<div id="outline-container-orgheadline104" class="outline-2">
<h2 id="orgheadline104"><span class="section-number-2">15</span> Google Natural Language Processing</h2>
</div>

<div id="outline-container-orgheadline105" class="outline-2">
<h2 id="orgheadline105"><span class="section-number-2">16</span> Outline of Machine Learning</h2>
</div>

<div id="outline-container-orgheadline110" class="outline-2">
<h2 id="orgheadline110"><span class="section-number-2">17</span> Scikit-lean API</h2>
<div class="outline-text-2" id="text-17">
</div><div id="outline-container-orgheadline106" class="outline-3">
<h3 id="orgheadline106"><span class="section-number-3">17.1</span> basics of the API</h3>
<div class="outline-text-3" id="text-17-1">
<ol class="org-ol">
<li>Choose a class of model by importing the appropriate estimator class from ScikitLearn.</li>
</ol>
<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">6</span><span style="color: #AE81FF;">]</span>: <span style="color: #F92672;">from</span> sklearn.linear_model <span style="color: #F92672;">import</span> LinearRegression
</pre>
</div>


<ol class="org-ol">
<li>Choose model hyperparameters by instantiating this class with desired values.</li>
</ol>
<div class="org-src-container">

<pre class="src src-ipython">In[7]: model = LinearRegression(fit_intercept=True)
model
Out[7]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,
normalize=False)
</pre>
</div>
<ol class="org-ol">
<li>Arrange data into a features matrix and target vector following the discussion</li>
</ol>
<p>
from before.
</p>
<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">8</span><span style="color: #AE81FF;">]</span>: <span style="color: #FD971F;">X</span> = x<span style="color: #AE81FF;">[</span>:, np.newaxis<span style="color: #AE81FF;">]</span>
X.shape
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">8</span><span style="color: #AE81FF;">]</span>: <span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">50</span>, <span style="color: #AE81FF;">1</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
<ol class="org-ol">
<li>Fit the model to your data by calling the fit() method of the model instance.</li>
</ol>
<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">9</span><span style="color: #AE81FF;">]</span>: model.fit<span style="color: #AE81FF;">(</span>X, y<span style="color: #AE81FF;">)</span>
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">9</span><span style="color: #AE81FF;">]</span>:
LinearRegression<span style="color: #AE81FF;">(</span>copy_X=<span style="color: #AE81FF;">True</span>, fit_intercept=<span style="color: #AE81FF;">True</span>, n_jobs=<span style="color: #AE81FF;">1</span>,
normalize=<span style="color: #AE81FF;">False</span><span style="color: #AE81FF;">)</span>
In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">10</span><span style="color: #AE81FF;">]</span>: model.coef_
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">10</span><span style="color: #AE81FF;">]</span>: array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span> <span style="color: #AE81FF;">1</span>.<span style="color: #AE81FF;">9776566</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">11</span><span style="color: #AE81FF;">]</span>: model.intercept_
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">11</span><span style="color: #AE81FF;">]</span>: -<span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">90331072553111635</span>
</pre>
</div>
<ol class="org-ol">
<li>Apply the model to new data:</li>
</ol>
<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">12</span><span style="color: #AE81FF;">]</span>: <span style="color: #FD971F;">xfit</span> = np.linspace<span style="color: #AE81FF;">(</span>-<span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">11</span><span style="color: #AE81FF;">)</span>
In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">13</span><span style="color: #AE81FF;">]</span>: <span style="color: #FD971F;">Xfit</span> = xfit<span style="color: #AE81FF;">[</span>:, np.newaxis<span style="color: #AE81FF;">]</span>
<span style="color: #FD971F;">yfit</span> = model.predict<span style="color: #AE81FF;">(</span>Xfit<span style="color: #AE81FF;">)</span>
</pre>
</div>
<p>
• For supervised learning, often we predict labels for unknown data using the
predict() method.
• For unsupervised learning, we often transform or infer properties of the data
using the transform() or predict() method.
</p>
</div>
</div>
<div id="outline-container-orgheadline109" class="outline-3">
<h3 id="orgheadline109"><span class="section-number-3">17.2</span> feature engineering</h3>
<div class="outline-text-3" id="text-17-2">
</div><div id="outline-container-orgheadline107" class="outline-4">
<h4 id="orgheadline107"><span class="section-number-4">17.2.1</span> Imputation of Missing Data</h4>
<div class="outline-text-4" id="text-17-2-1">
<p>
The sophisticated approaches tend to be very application-specific, and we won’t dive
into them here. For a baseline imputation approach, using the mean, median, or most
frequent value, Scikit-Learn provides the Imputer class:
</p>

<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">15</span><span style="color: #AE81FF;">]</span>: <span style="color: #F92672;">from</span> sklearn.preprocessing <span style="color: #F92672;">import</span> Imputer
<span style="color: #FD971F;">imp</span> = Imputer<span style="color: #AE81FF;">(</span>strategy=<span style="color: #E6DB74;">'mean'</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">X2</span> = imp.fit_transform<span style="color: #AE81FF;">(</span>X<span style="color: #AE81FF;">)</span>
X2
Out<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">15</span><span style="color: #AE81FF;">]</span>: array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span><span style="color: #A6E22E;">[</span> <span style="color: #AE81FF;">4</span>.<span style="color: #AE81FF;">5</span>, <span style="color: #AE81FF;">0</span>. , <span style="color: #AE81FF;">3</span>. <span style="color: #A6E22E;">]</span>,
<span style="color: #A6E22E;">[</span> <span style="color: #AE81FF;">3</span>. , <span style="color: #AE81FF;">7</span>. , <span style="color: #AE81FF;">9</span>. <span style="color: #A6E22E;">]</span>,
<span style="color: #A6E22E;">[</span> <span style="color: #AE81FF;">3</span>. , <span style="color: #AE81FF;">5</span>. , <span style="color: #AE81FF;">2</span>. <span style="color: #A6E22E;">]</span>,
<span style="color: #A6E22E;">[</span> <span style="color: #AE81FF;">4</span>. , <span style="color: #AE81FF;">5</span>. , <span style="color: #AE81FF;">6</span>. <span style="color: #A6E22E;">]</span>,
<span style="color: #A6E22E;">[</span> <span style="color: #AE81FF;">8</span>. , <span style="color: #AE81FF;">8</span>. , <span style="color: #AE81FF;">1</span>. <span style="color: #A6E22E;">]</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgheadline108" class="outline-4">
<h4 id="orgheadline108"><span class="section-number-4">17.2.2</span> Feature Pipelines</h4>
<div class="outline-text-4" id="text-17-2-2">
<ol class="org-ol">
<li>Impute missing values using the mean</li>
<li>Transform features to quadratic</li>
<li>Fit a linear regression</li>
</ol>
<div class="org-src-container">

<pre class="src src-python">To streamline this <span style="color: #F92672;">type</span> of processing pipeline, Scikit-Learn provides a pipeline <span style="color: #F92672;">object</span>,
which can be used <span style="color: #F92672;">as</span> follows:
In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">17</span><span style="color: #AE81FF;">]</span>: <span style="color: #F92672;">from</span> sklearn.pipeline <span style="color: #F92672;">import</span> make_pipeline
<span style="color: #FD971F;">model</span> = make_pipeline<span style="color: #AE81FF;">(</span>Imputer<span style="color: #66D9EF;">(</span>strategy=<span style="color: #E6DB74;">'mean'</span><span style="color: #66D9EF;">)</span>,
PolynomialFeatures<span style="color: #66D9EF;">(</span>degree=<span style="color: #AE81FF;">2</span><span style="color: #66D9EF;">)</span>,
LinearRegression<span style="color: #66D9EF;">()</span><span style="color: #AE81FF;">)</span>
</pre>
</div>

<p>
This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the
specified steps to any input data.
</p>
<div class="org-src-container">

<pre class="src src-python">In<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">18</span><span style="color: #AE81FF;">]</span>: model.fit<span style="color: #AE81FF;">(</span>X, y<span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">X with missing values, from above</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>y<span style="color: #AE81FF;">)</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>model.predict<span style="color: #66D9EF;">(</span>X<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
<span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">14</span> <span style="color: #AE81FF;">16</span> -<span style="color: #AE81FF;">1</span> <span style="color: #AE81FF;">8</span> -<span style="color: #AE81FF;">5</span><span style="color: #AE81FF;">]</span>
<span style="color: #AE81FF;">[</span> <span style="color: #AE81FF;">14</span>. <span style="color: #AE81FF;">16</span>. -<span style="color: #AE81FF;">1</span>. <span style="color: #AE81FF;">8</span>. -<span style="color: #AE81FF;">5</span>.<span style="color: #AE81FF;">]</span>
</pre>
</div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
