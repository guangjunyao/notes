<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-09-13 Thu 18:07 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="weiwu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "5em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "left",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. Basic</a></li>
<li><a href="#orgheadline6">2. Structure</a>
<ul>
<li><a href="#orgheadline2">2.1. The Computational Graph</a></li>
<li><a href="#orgheadline3">2.2. tf.train API</a></li>
<li><a href="#orgheadline4">2.3. Implementing Gradient Descent</a></li>
<li><a href="#orgheadline5">2.4. Saving and Restoring Models</a></li>
</ul>
</li>
<li><a href="#orgheadline7">3. Visualizing the Graph and Training Curves Using TensorBoard</a></li>
<li><a href="#orgheadline10">4. TensorBoard: Embedding Visualization</a>
<ul>
<li><a href="#orgheadline8">4.1. Load data</a></li>
<li><a href="#orgheadline9">4.2. Projections</a></li>
</ul>
</li>
<li><a href="#orgheadline11">5. Deep learning architecture</a></li>
<li><a href="#orgheadline22">6. Recurrent Neural Networks (RNN)</a>
<ul>
<li><a href="#orgheadline12">6.1. output formular</a></li>
<li><a href="#orgheadline13">6.2. hidden state</a></li>
<li><a href="#orgheadline14">6.3. structure</a></li>
<li><a href="#orgheadline15">6.4. basic RNNs</a></li>
<li><a href="#orgheadline16">6.5. Static Unrolling Through Time</a></li>
<li><a href="#orgheadline17">6.6. Dynamic Unrolling Through Time</a></li>
<li><a href="#orgheadline18">6.7. Training RNNs</a></li>
<li><a href="#orgheadline19">6.8. The difficulty of Training over Many Time Steps</a></li>
<li><a href="#orgheadline20">6.9. Long Short Term Memory(LSTM)</a></li>
<li><a href="#orgheadline21">6.10. Gated Recurrent Unit(GRU)</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1"><span class="section-number-2">1</span> Basic</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>tensor:</li>
</ul>
<p>
a generalization of the concept of a vector
</p>
</div>
</div>


<div id="outline-container-orgheadline6" class="outline-2">
<h2 id="orgheadline6"><span class="section-number-2">2</span> Structure</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2"><span class="section-number-3">2.1</span> The Computational Graph</h3>
<div class="outline-text-3" id="text-2-1">
<p>
A TensorFlow program is typically split into two parts: the first part builds a computation graph (this is called the construction phase), and the second part runs it (this is the execution phase). The construction phase typically builds a computation graph representing the ML model and the computations required to train it.
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">node1</span> = tf.constant<span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">3</span>.<span style="color: #AE81FF;">0</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">node2</span> = tf.constant<span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">4</span>.<span style="color: #AE81FF;">0</span><span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">also tf.float32 implicitly</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>node1, node2<span style="color: #AE81FF;">)</span>
</pre>
</div>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">sess</span> = tf.Session<span style="color: #AE81FF;">()</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>sess.run<span style="color: #66D9EF;">(</span><span style="color: #A6E22E;">[</span>node1, node2<span style="color: #A6E22E;">]</span><span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">a</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">b</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">adder_node</span> = a + b  <span style="color: #75715E;"># </span><span style="color: #75715E;">+ provides a shortcut for tf.add(a, b)</span>
</pre>
</div>
<p>
To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. Variables allow us to add trainable parameters to a graph. They are constructed with a type and initial value:
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">W</span> = tf.Variable<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>.<span style="color: #AE81FF;">3</span><span style="color: #66D9EF;">]</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">b</span> = tf.Variable<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>-.<span style="color: #AE81FF;">3</span><span style="color: #66D9EF;">]</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">x</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">linear_model</span> = W*x + b
</pre>
</div>
<p>
Constants are initialized when you call tf.constant, and their value can never change. By contrast, variables are not initialized when you call tf.Variable. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">init</span> = tf.global_variables_initializer<span style="color: #AE81FF;">()</span>
sess.run<span style="color: #AE81FF;">(</span>init<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">y</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">squared_deltas</span> = tf.square<span style="color: #AE81FF;">(</span>linear_model - y<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">loss</span> = tf.reduce_sum<span style="color: #AE81FF;">(</span>squared_deltas<span style="color: #AE81FF;">)</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>sess.run<span style="color: #66D9EF;">(</span>loss, <span style="color: #A6E22E;">{</span>x: <span style="color: #E6DB74;">[</span><span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">2</span>, <span style="color: #AE81FF;">3</span>, <span style="color: #AE81FF;">4</span><span style="color: #E6DB74;">]</span>, y: <span style="color: #E6DB74;">[</span><span style="color: #AE81FF;">0</span>, -<span style="color: #AE81FF;">1</span>, -<span style="color: #AE81FF;">2</span>, -<span style="color: #AE81FF;">3</span><span style="color: #E6DB74;">]</span><span style="color: #A6E22E;">}</span><span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
<p>
A variable is initialized to the value provided to tf.Variable but can be changed using operations like tf.assign.
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">fixW</span> = tf.assign<span style="color: #AE81FF;">(</span>W, <span style="color: #66D9EF;">[</span>-<span style="color: #AE81FF;">1</span>.<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">fixb</span> = tf.assign<span style="color: #AE81FF;">(</span>b, <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">1</span>.<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
sess.run<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>fixW, fixb<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span>sess.run<span style="color: #66D9EF;">(</span>loss, <span style="color: #A6E22E;">{</span>x: <span style="color: #E6DB74;">[</span><span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">2</span>, <span style="color: #AE81FF;">3</span>, <span style="color: #AE81FF;">4</span><span style="color: #E6DB74;">]</span>, y: <span style="color: #E6DB74;">[</span><span style="color: #AE81FF;">0</span>, -<span style="color: #AE81FF;">1</span>, -<span style="color: #AE81FF;">2</span>, -<span style="color: #AE81FF;">3</span><span style="color: #E6DB74;">]</span><span style="color: #A6E22E;">}</span><span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">2.2</span> tf.train API</h3>
<div class="outline-text-3" id="text-2-2">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">import</span> tensorflow <span style="color: #F92672;">as</span> tf

<span style="color: #75715E;"># </span><span style="color: #75715E;">Model parameters</span>
<span style="color: #FD971F;">W</span> = tf.Variable<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>.<span style="color: #AE81FF;">3</span><span style="color: #66D9EF;">]</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">b</span> = tf.Variable<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>-.<span style="color: #AE81FF;">3</span><span style="color: #66D9EF;">]</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">Model input and output</span>
<span style="color: #FD971F;">x</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">linear_model</span> = W*x + b
<span style="color: #FD971F;">y</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32<span style="color: #AE81FF;">)</span>

<span style="color: #75715E;"># </span><span style="color: #75715E;">loss</span>
<span style="color: #FD971F;">loss</span> = tf.reduce_sum<span style="color: #AE81FF;">(</span>tf.square<span style="color: #66D9EF;">(</span>linear_model - y<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">sum of the squares</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">optimizer</span>
<span style="color: #FD971F;">optimizer</span> = tf.train.GradientDescentOptimizer<span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">0</span>.<span style="color: #AE81FF;">01</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">train</span> = optimizer.minimize<span style="color: #AE81FF;">(</span>loss<span style="color: #AE81FF;">)</span>

<span style="color: #75715E;"># </span><span style="color: #75715E;">training data</span>
<span style="color: #FD971F;">x_train</span> = <span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">2</span>, <span style="color: #AE81FF;">3</span>, <span style="color: #AE81FF;">4</span><span style="color: #AE81FF;">]</span>
<span style="color: #FD971F;">y_train</span> = <span style="color: #AE81FF;">[</span><span style="color: #AE81FF;">0</span>, -<span style="color: #AE81FF;">1</span>, -<span style="color: #AE81FF;">2</span>, -<span style="color: #AE81FF;">3</span><span style="color: #AE81FF;">]</span>
<span style="color: #75715E;"># </span><span style="color: #75715E;">training loop</span>
<span style="color: #FD971F;">init</span> = tf.global_variables_initializer<span style="color: #AE81FF;">()</span>
<span style="color: #FD971F;">sess</span> = tf.Session<span style="color: #AE81FF;">()</span>
sess.run<span style="color: #AE81FF;">(</span>init<span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">reset values to wrong</span>
<span style="color: #F92672;">for</span> i <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span><span style="color: #AE81FF;">(</span><span style="color: #AE81FF;">1000</span><span style="color: #AE81FF;">)</span>:
  sess.run<span style="color: #AE81FF;">(</span>train, <span style="color: #66D9EF;">{</span>x: x_train, y: y_train<span style="color: #66D9EF;">}</span><span style="color: #AE81FF;">)</span>

<span style="color: #75715E;"># </span><span style="color: #75715E;">evaluate training accuracy</span>
<span style="color: #FD971F;">curr_W</span>, <span style="color: #FD971F;">curr_b</span>, <span style="color: #FD971F;">curr_loss</span> = sess.run<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>W, b, loss<span style="color: #66D9EF;">]</span>, <span style="color: #66D9EF;">{</span>x: x_train, y: y_train<span style="color: #66D9EF;">}</span><span style="color: #AE81FF;">)</span>
<span style="color: #F92672;">print</span><span style="color: #AE81FF;">(</span><span style="color: #E6DB74;">"W: %s b: %s loss: %s"</span>%<span style="color: #66D9EF;">(</span>curr_W, curr_b, curr_loss<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-3">
<h3 id="orgheadline4"><span class="section-number-3">2.3</span> Implementing Gradient Descent</h3>
<div class="outline-text-3" id="text-2-3">
<p>
When using Gradient Descent, remember that it is important to
first normalize the input feature vectors, or else training may be
much slower.
</p>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5"><span class="section-number-3">2.4</span> Saving and Restoring Models</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Once you have trained your model, you should save its parameters to disk so you can
come back to it whenever you want, use it in another program, compare it to other
models, and so on. Moreover, you probably want to save checkpoints at regular inter‐
vals during training so that if your computer crashes during training you can con‐
tinue from the last checkpoint rather than start over from scratch.
TensorFlow makes saving and restoring a model very easy. Just create a Saver node at
the end of the construction phase (after all variable nodes are created); then, in the
execution phase, just call its save() method whenever you want to save the model,
passing it the session and path of the checkpoint file.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline7" class="outline-2">
<h2 id="orgheadline7"><span class="section-number-2">3</span> Visualizing the Graph and Training Curves Using TensorBoard</h2>
<div class="outline-text-2" id="text-3">
<p>
The first step is to tweak your program a bit so it writes the graph definition and
some training stats—for example, the training error (MSE)—to a log directory that
TensorBoard will read from. You need to use a different log directory every time you
run your program, or else TensorBoard will merge stats from different runs, which
will mess up the visualizations. The simplest solution for this is to include a time‐
stamp in the log directory name. Add the following code at the beginning of the pro‐
gram:
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">from</span> datetime <span style="color: #F92672;">import</span> datetime
<span style="color: #FD971F;">now</span> = datetime.utcnow<span style="color: #AE81FF;">()</span>.strftime<span style="color: #AE81FF;">(</span><span style="color: #E6DB74;">"%Y%m%d%H%M%S"</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">root_logdir</span> = <span style="color: #E6DB74;">"tf_logs"</span>
<span style="color: #FD971F;">logdir</span> = <span style="color: #E6DB74;">"{}/run-{}/"</span>.<span style="color: #F92672;">format</span><span style="color: #AE81FF;">(</span>root_logdir, now<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">mse_summary</span> = tf.summary.scalar<span style="color: #AE81FF;">(</span><span style="color: #E6DB74;">'MSE'</span>, mse<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">file_writer</span> = tf.summary.FileWriter<span style="color: #AE81FF;">(</span>logdir, tf.get_default_graph<span style="color: #66D9EF;">()</span><span style="color: #AE81FF;">)</span>
</pre>
</div>

<p>
Next you need to update the execution phase to evaluate the mse<sub>summary</sub> node regularly during training (e.g., every 10 mini-batches). This will output a summary that
you can then write to the events file using the file<sub>writer</sub>. Here is the updated code:
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #F92672;">for</span> batch_index <span style="color: #F92672;">in</span> <span style="color: #F92672;">range</span><span style="color: #AE81FF;">(</span>n_batches<span style="color: #AE81FF;">)</span>:
    <span style="color: #FD971F;">X_batch</span>, <span style="color: #FD971F;">y_batch</span> = fetch_batch<span style="color: #AE81FF;">(</span>epoch, batch_index, batch_size<span style="color: #AE81FF;">)</span>
    <span style="color: #F92672;">if</span> batch_index % <span style="color: #AE81FF;">10</span> == <span style="color: #AE81FF;">0</span>:
        <span style="color: #FD971F;">summary_str</span> = mse_summary.<span style="color: #F92672;">eval</span><span style="color: #AE81FF;">(</span>feed_dict=<span style="color: #66D9EF;">{</span>X: X_batch, y: y_batch<span style="color: #66D9EF;">}</span><span style="color: #AE81FF;">)</span>
        <span style="color: #FD971F;">step</span> = epoch * n_batches + batch_index
        file_writer.add_summary<span style="color: #AE81FF;">(</span>summary_str, step<span style="color: #AE81FF;">)</span>
    sess.run<span style="color: #AE81FF;">(</span>training_op, feed_dict=<span style="color: #66D9EF;">{</span>X: X_batch, y: y_batch<span style="color: #66D9EF;">}</span><span style="color: #AE81FF;">)</span>
<span style="color: #AE81FF;">[</span>...<span style="color: #AE81FF;">]</span>
file_writer.close<span style="color: #AE81FF;">()</span>
</pre>
</div>

<ol class="org-ol">
<li>activate virtualenv.</li>
<li>start tensorboard server.</li>
</ol>
<div class="org-src-container">

<pre class="src src-bash">tensorboard --logdir tf_logs/ --host <span style="color: #AE81FF;">192.168.1.199</span> --port <span style="color: #AE81FF;">8008</span>
</pre>
</div>
<ol class="org-ol">
<li>open browser, go to <a href="http://192.168.1.199:8008/">http://192.168.1.199:8008/</a></li>
</ol>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-2">
<h2 id="orgheadline10"><span class="section-number-2">4</span> TensorBoard: Embedding Visualization</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">4.1</span> Load data</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Step 1: Load a TSV file of vectors.
Example of 3 vectors with dimension 4:
0.1\t0.2\t0.5\t0.9
0.2\t0.1\t5.0\t0.2
0.4\t0.1\t7.0\t0.8
</p>


<p>
Step 2 (optional): Load a TSV file of metadata.
Example of 3 data points and 2 columns.
Note: If there is more than one column, the first row will be parsed as column labels.
Pokémon\tSpecies
Wartortle\tTurtle
Venusaur\tSeed
Charmeleon\tFlame
</p>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-3">
<h3 id="orgheadline9"><span class="section-number-3">4.2</span> Projections</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The Embedding Projector has three methods of reducing the dimensionality of a data set: two linear and one nonlinear. Each method can be used to create either a two- or three-dimensional view.
</p>

<ul class="org-ul">
<li><a href="http://setosa.io/ev/principal-component-analysis/">Principal Component Analysis</a></li>
</ul>
<p>
A straightforward technique for reducing dimensions is Principal Component Analysis (PCA). The Embedding Projector computes the top 10 principal components. The menu lets you project those components onto any combination of two or three. PCA is a linear projection, often effective at examining global geometry.
</p>

<ul class="org-ul">
<li><a href="https://distill.pub/2016/misread-tsne/">t-SNE</a></li>
</ul>
<p>
A popular non-linear dimensionality reduction technique is t-SNE. The Embedding Projector offers both two- and three-dimensional t-SNE views. Layout is performed client-side animating every step of the algorithm. Because t-SNE often preserves some local structure, it is useful for exploring local neighborhoods and finding clusters. Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. See this great article for how to use t-SNE effectively.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline11" class="outline-2">
<h2 id="orgheadline11"><span class="section-number-2">5</span> Deep learning architecture</h2>
<div class="outline-text-2" id="text-5">
<p>
Tensorflow is a mathmatical library including neural network, Keras is a framework based on Tensorflow.
</p>
<ul class="org-ul">
<li>Lower Level: This is where frameworks like Tensorflow, MXNet, Theano, and PyTorch sit. This is the level where mathematical operations like Generalized Matrix-Matrix multiplication and Neural Network primitives like Convolutional operations are implemented.</li>
<li>Higher Level: This is where frameworks like Keras sit. At this Level, the lower level primitives are used to implement Neural Network abstraction like Layers and models. Generally, at this level other helpful APIs like model saving and model training are also implemented.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline22" class="outline-2">
<h2 id="orgheadline22"><span class="section-number-2">6</span> Recurrent Neural Networks (RNN)</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgheadline12" class="outline-3">
<h3 id="orgheadline12"><span class="section-number-3">6.1</span> output formular</h3>
<div class="outline-text-3" id="text-6-1">
<p>
\[y_{(t)}=\psi(x_{(t)}^t * w_x + y_{(t-1)}^t * w_y + b)\]
</p>

<p>
For the whole layer:
\[Y_{(t)}=\psi(X_{(t)}^t * W_x + Y_{(t-1)}^t * W_y + b)\]
\[=\psi([X_{(t)} Y_{(t-1)}]W+b)\] with \(W=[W_x W_y]^t\)
</p>
</div>
</div>
<div id="outline-container-orgheadline13" class="outline-3">
<h3 id="orgheadline13"><span class="section-number-3">6.2</span> hidden state</h3>
<div class="outline-text-3" id="text-6-2">
<p>
In general a cell’s state at time step t, denoted h(t) (the “h” stands for “hidden”), is a
function of some inputs at that time step and its state at the previous time step: h(t) =
f(h(t–1), x(t)). Its output at time step t, denoted y(t), is also a function of the previous
state and the current inputs.
</p>
</div>
</div>
<div id="outline-container-orgheadline14" class="outline-3">
<h3 id="orgheadline14"><span class="section-number-3">6.3</span> structure</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>seq to seq: stock price.</li>
<li>seq to vector: sentiment.</li>
<li>vector to seq: caption of image.</li>
<li>delayed seq to seq: translation, Encoder-Decoder.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline15" class="outline-3">
<h3 id="orgheadline15"><span class="section-number-3">6.4</span> basic RNNs</h3>
<div class="outline-text-3" id="text-6-4">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">n_inputs</span> = <span style="color: #AE81FF;">3</span>
<span style="color: #FD971F;">n_neurons</span> = <span style="color: #AE81FF;">5</span>
<span style="color: #FD971F;">X0</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32, <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">None</span>, n_inputs<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">X1</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32, <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">None</span>, n_inputs<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">Wx</span> = tf.Variable<span style="color: #AE81FF;">(</span>tf.random_normal<span style="color: #66D9EF;">(</span>shape=<span style="color: #A6E22E;">[</span>n_inputs, n_neurons<span style="color: #A6E22E;">]</span>,dtype=tf.float32<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">Wy</span> = tf.Variable<span style="color: #AE81FF;">(</span>tf.random_normal<span style="color: #66D9EF;">(</span>shape=<span style="color: #A6E22E;">[</span>n_neurons,n_neurons<span style="color: #A6E22E;">]</span>,dtype=tf.float32<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">b</span> = tf.Variable<span style="color: #AE81FF;">(</span>tf.zeros<span style="color: #66D9EF;">(</span><span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">1</span>, n_neurons<span style="color: #A6E22E;">]</span>, dtype=tf.float32<span style="color: #66D9EF;">)</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">Y0</span> = tf.tanh<span style="color: #AE81FF;">(</span>tf.matmul<span style="color: #66D9EF;">(</span>X0, Wx<span style="color: #66D9EF;">)</span> + b<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">Y1</span> = tf.tanh<span style="color: #AE81FF;">(</span>tf.matmul<span style="color: #66D9EF;">(</span>Y0, Wy<span style="color: #66D9EF;">)</span> + tf.matmul<span style="color: #66D9EF;">(</span>X1, Wx<span style="color: #66D9EF;">)</span> + b<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">init</span> = tf.global_variables_initializer<span style="color: #AE81FF;">()</span>

<span style="color: #75715E;"># </span><span style="color: #75715E;">feed data</span>
<span style="color: #F92672;">import</span> numpy <span style="color: #F92672;">as</span> np
<span style="color: #75715E;"># </span><span style="color: #75715E;">Mini-batch: instance 0,instance 1,instance 2,instance 3</span>
<span style="color: #FD971F;">X0_batch</span> = np.array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span><span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">0</span>, <span style="color: #AE81FF;">1</span>, <span style="color: #AE81FF;">2</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">3</span>, <span style="color: #AE81FF;">4</span>, <span style="color: #AE81FF;">5</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">6</span>, <span style="color: #AE81FF;">7</span>, <span style="color: #AE81FF;">8</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">9</span>, <span style="color: #AE81FF;">0</span>, <span style="color: #AE81FF;">1</span><span style="color: #A6E22E;">]</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">t = 0</span>
<span style="color: #FD971F;">X1_batch</span> = np.array<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span><span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">9</span>, <span style="color: #AE81FF;">8</span>, <span style="color: #AE81FF;">7</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">0</span>, <span style="color: #AE81FF;">0</span>, <span style="color: #AE81FF;">0</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">6</span>, <span style="color: #AE81FF;">5</span>, <span style="color: #AE81FF;">4</span><span style="color: #A6E22E;">]</span>, <span style="color: #A6E22E;">[</span><span style="color: #AE81FF;">3</span>, <span style="color: #AE81FF;">2</span>, <span style="color: #AE81FF;">1</span><span style="color: #A6E22E;">]</span><span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span> <span style="color: #75715E;"># </span><span style="color: #75715E;">t = 1</span>
<span style="color: #F92672;">with</span> tf.Session<span style="color: #AE81FF;">()</span> <span style="color: #F92672;">as</span> sess:
  init.run<span style="color: #AE81FF;">()</span>
  <span style="color: #FD971F;">Y0_val</span>, <span style="color: #FD971F;">Y1_val</span> = sess.run<span style="color: #AE81FF;">(</span><span style="color: #66D9EF;">[</span>Y0, Y1<span style="color: #66D9EF;">]</span>, feed_dict=<span style="color: #66D9EF;">{</span>X0: X0_batch, X1: X1_batch<span style="color: #66D9EF;">}</span><span style="color: #AE81FF;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgheadline16" class="outline-3">
<h3 id="orgheadline16"><span class="section-number-3">6.5</span> Static Unrolling Through Time</h3>
<div class="outline-text-3" id="text-6-5">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">X0</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32, <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">None</span>, n_inputs<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">X1</span> = tf.placeholder<span style="color: #AE81FF;">(</span>tf.float32, <span style="color: #66D9EF;">[</span><span style="color: #AE81FF;">None</span>, n_inputs<span style="color: #66D9EF;">]</span><span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">basic_cell</span> = tf.contrib.rnn.BasicRNNCell<span style="color: #AE81FF;">(</span>num_units=n_neurons<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">output_seqs</span>, <span style="color: #FD971F;">states</span> = tf.contrib.rnn.static_rnn<span style="color: #AE81FF;">(</span>
basic_cell, <span style="color: #66D9EF;">[</span>X0, X1<span style="color: #66D9EF;">]</span>, dtype=tf.float32<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">Y0</span>, <span style="color: #FD971F;">Y1</span> = output_seqs
</pre>
</div>
<p>
The static<sub>rnn</sub>() function calls the cell factory’s _<sub>call</sub>_<sub>()</sub> function once per input, creating two copies of the cell (each containing a layer of five recurrent neurons), with shared weights and bias
terms, and it chains them just like we did earlier.
</p>

<p>
The static<sub>rnn</sub>() function returns two objects.
</p>

<p>
The first is a Python list containing the output tensors for each time step.
</p>

<p>
The second is a tensor containing the final states of the network. When you are using basic cells, the final state is simply equal to the last output.
</p>
</div>
</div>
<div id="outline-container-orgheadline17" class="outline-3">
<h3 id="orgheadline17"><span class="section-number-3">6.6</span> Dynamic Unrolling Through Time</h3>
<div class="outline-text-3" id="text-6-6">
<p>
The dynamic<sub>rnn</sub>() function uses a while<sub>loop</sub>() operation to run over the cell the
appropriate number of times, and you can set swap<sub>memory</sub>=True if you want it to
swap the GPU’s memory to the CPU’s memory during backpropagation to avoid
OOM errors. Conveniently, it also accepts a single tensor for all inputs at every time
step (shape [None, n<sub>steps</sub>, n<sub>inputs</sub>]) and it outputs a single tensor for all out‐
puts at every time step (shape [None, n<sub>steps</sub>, n<sub>neurons</sub>]); there is no need to
stack, unstack, or transpose. The following code creates the same RNN as earlier
using the dynamic<sub>rnn</sub>() function. It’s so much nicer!
</p>
</div>
</div>
<div id="outline-container-orgheadline18" class="outline-3">
<h3 id="orgheadline18"><span class="section-number-3">6.7</span> Training RNNs</h3>
<div class="outline-text-3" id="text-6-7">
<p>
To train an RNN, the trick is to unroll it through time (like we just did) and then
simply use regular backpropagation. This strategy is called backpro‐
pagation through time (BPTT).
</p>
</div>
</div>
<div id="outline-container-orgheadline19" class="outline-3">
<h3 id="orgheadline19"><span class="section-number-3">6.8</span> The difficulty of Training over Many Time Steps</h3>
<div class="outline-text-3" id="text-6-8">
<ul class="org-ul">
<li>good parameter initialization</li>
<li>nonsaturating activation functions (e.g., ReLU),</li>
<li>Batch Normalization,</li>
<li>Gradient Clip‐</li>
<li>ping, and faster optimizers.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline20" class="outline-3">
<h3 id="orgheadline20"><span class="section-number-3">6.9</span> Long Short Term Memory(LSTM)</h3>
<div class="outline-text-3" id="text-6-9">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #FD971F;">lstm_cell</span> = tf.contrib.rnn.BasicLSTMCell<span style="color: #AE81FF;">(</span>num_units=n_neurons<span style="color: #AE81FF;">)</span>
</pre>
</div>

<p>
LSTM cells manage two state vectors, and for performance reasons they are kept
separate by default.
<img src="./Images/LSTM_cell.png" alt="LSTM_cell.png" />
its state is split in two vectors: h(t) and c(t) (“c” stands for “cell”). You can think of h(t) as the short-term state and c(t) as the long-term state.
</p>


<p>
As the long-term state
c(t–1) traverses the network from left to right, you can see that it first goes through a
forget gate, dropping some memories, and then it adds some new memories via the
addition operation (which adds the memories that were selected by an input gate).
The result c(t) is sent straight out, without any further transformation. So, at each time
step, some memories are dropped and some memories are added. Moreover, after the
addition operation, the long-term state is copied and passed through the tanh func‐
tion, and then the result is filtered by the output gate. This produces the short-term
state h(t) (which is equal to the cell’s output for this time step y(t)).
</p>

<p>
The main layer is the one that outputs g(t). It has the usual role of analyzing the
current inputs x(t) and the previous (short-term) state h(t–1). In a basic cell, there is
nothing else than this layer, and its output goes straight out to y(t) and h(t). In con‐
trast, in an LSTM cell this layer’s output does not go straight out, but instead it is
partially stored in the long-term state.
</p>

<p>
The three other layers are gate controllers. Since they use the logistic activation
function, their outputs range from 0 to 1. As you can see, their outputs are fed to
element-wise multiplication operations, so if they output 0s, they close the gate,
and if they output 1s, they open it. Specifically:
— The forget gate (controlled by f(t)) controls which parts of the long-term state
should be erased.
— The input gate (controlled by i(t)) controls which parts of g(t) should be added
to the long-term state (this is why we said it was only “partially stored”).
— Finally, the output gate (controlled by o(t)) controls which parts of the longterm state should be read and output at this time step (both to h(t)) and y(t).
In short, an LSTM cell can learn to recognize an important input (that’s the role of the
input gate), store it in the long-term state, learn to preserve it for as long as it is
needed (that’s the role of the forget gate), and learn to extract it whenever it is needed.
This explains why they have been amazingly successful at capturing long-term pat‐
terns in time series, long texts, audio recordings, and more.
</p>
</div>
</div>
<div id="outline-container-orgheadline21" class="outline-3">
<h3 id="orgheadline21"><span class="section-number-3">6.10</span> Gated Recurrent Unit(GRU)</h3>
<div class="outline-text-3" id="text-6-10">
<p>
The main simplifications are:
• Both state vectors are merged into a single vector h(t).
• A single gate controller controls both the forget gate and the input gate. If the
gate controller outputs a 1, the input gate is open and the forget gate is closed. If it outputs a 0, the opposite happens. In other words, whenever a memory must
be stored, the location where it will be stored is erased first. This is actually a fre‐
quent variant to the LSTM cell in and of itself.
• There is no output gate; the full state vector is output at every time step. How‐
ever, there is a new gate controller that controls which part of the previous state
will be shown to the main layer.
</p>
</div>
</div>
</div>
</div>
</body>
</html>
