<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-11-07 Tue 13:28 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="weiwu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../styles/demo/css/demo.css"/>
<link href="https://fonts.proxy.ustclug.org/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "5em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "left",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline12">1. Decision Tree</a>
<ul>
<li><a href="#orgheadline1">1.1. Goals</a></li>
<li><a href="#orgheadline2">1.2. Terminology</a></li>
<li><a href="#orgheadline8">1.3. Tree construction</a>
<ul>
<li><a href="#orgheadline3">1.3.1. Pseudo code</a></li>
<li><a href="#orgheadline5">1.3.2. Information gain</a></li>
<li><a href="#orgheadline6">1.3.3. 条件熵</a></li>
<li><a href="#orgheadline7">1.3.4. 互信息</a></li>
</ul>
</li>
<li><a href="#orgheadline11">1.4. Pros and cons of trees</a>
<ul>
<li><a href="#orgheadline9">1.4.1. Pros:</a></li>
<li><a href="#orgheadline10">1.4.2. Cons:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline15">2. Random Forest</a>
<ul>
<li><a href="#orgheadline13">2.1. Context</a></li>
<li><a href="#orgheadline14">2.2. Goals</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline12" class="outline-2">
<h2 id="orgheadline12"><span class="section-number-2">1</span> Decision Tree</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="./code/tree.py">decision<sub>tree.py</sub></a>
<a href="https://metacademy.org/graphs/concepts/decision_trees">https://metacademy.org/graphs/concepts/decision_trees</a>
</p>
</div>
<div id="outline-container-orgheadline1" class="outline-3">
<h3 id="orgheadline1"><span class="section-number-3">1.1</span> Goals</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Know what a decision tree is.</li>
</ul>
<p>
选一个特征，使得熵降低程度最大，直到降为0. greedy searching.
</p>
<ul class="org-ul">
<li>Give examples of functions which can't be represented compactly (e.g. majority, parity)</li>
<li>Be able to fit a decision tree using a recursive greedy strategy.</li>
</ul>
<p>
特征选择，决策树的生成，决策树的修剪。
</p>

<p>
特征的选择，计算每个特征的信息增益，ID3, C4.5, CART。
</p>

<p>
recursive searching生成树。
</p>

<ul class="org-ul">
<li>剪枝(pruning a tree)</li>
</ul>
<p>
The standard approach is therefore to grow a “full” tree, and then to perform pruning. This
can be done using a scheme that prunes the branches giving the least increase in the error.
</p>

<p>
cost function: Gini function, CART.
</p>
<ul class="org-ul">
<li>What is the information gain criterion, and why does it produce better splits than classification accuracy?</li>
<li>Be aware that decision trees can be unstable, in that the structure changes dramatically with respect to small changes in the training data.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2"><span class="section-number-3">1.2</span> Terminology</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>CART</li>
</ul>
<p>
Classification and regression trees or CART models, also called decision trees are defined by recursively partitioning the input space, and defining a local model in each resulting region of input space. This can be represented by a tree, with one leaf per region.
</p>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">1.3</span> Tree construction</h3>
<div class="outline-text-3" id="text-1-3">
<p>
To build a decision tree, you need to make a first decision on the dataset to dictate
which feature is used to split the data. To determine this, you try every feature and measure which split will give you the best results. After that, you’ll split the dataset into subsets. The subsets will then traverse down the branches of the first decision node. If the
data on the branches is the same class, then you’ve properly classified it and don’t need
to continue splitting it. If the data isn’t the same, then you need to repeat the splitting
process on this subset. The decision on how to split this subset is done the same way as
the original dataset, and you repeat this process until you’ve classified all the data.
</p>
</div>
<div id="outline-container-orgheadline3" class="outline-4">
<h4 id="orgheadline3"><span class="section-number-4">1.3.1</span> Pseudo code</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
```text
Check if every item in the dataset is in the same class:
If so return the class label
Else
find the best feature to split the data
split the dataset
create a branch node
for each split
call createBranch and add the result to the branch node
return branch nod
```
</p>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-4">
<h4 id="orgheadline5"><span class="section-number-4">1.3.2</span> Information gain</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
measure of information of a set is known as Shannon entropy, entropy is defined as the expected value of the information.
information:
\[l(x_i) = log_2p(x_i)\]
</p>
</div>
<ol class="org-ol"><li><a id="orgheadline4"></a>entropy, or deviance, expected value of all the information of all posible values of our class.<br  /><div class="outline-text-5" id="text-1-3-2-1">
<p>
\[H=-\sum_{i=1}^n p(x_i)log_2p(x_i)\]
Goals
</p>
<ul class="org-ul">
<li>Understand the notion of entropy of a discrete random variable.</li>
<li>What is the largest possible entropy of a discrete random variable which takes on r possible values?</li>
<li>Know the definitions of joint entropy and conditional entropy.</li>
<li>Derive the chain rule for writing joint entropy as a sum of conditional entropies.</li>
<li>Show that the entropy of a set of independent random variables is the sum of the individual entropies.</li>
</ul>
</div></li></ol>
</div>
<div id="outline-container-orgheadline6" class="outline-4">
<h4 id="orgheadline6"><span class="section-number-4">1.3.3</span> 条件熵</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
\[H(Y|X) = H(X,Y) - H(X)\]
</p>
</div>
</div>
<div id="outline-container-orgheadline7" class="outline-4">
<h4 id="orgheadline7"><span class="section-number-4">1.3.4</span> 互信息</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
\[I(X,Y) = H(Y) - H(Y|X)\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline11" class="outline-3">
<h3 id="orgheadline11"><span class="section-number-3">1.4</span> Pros and cons of trees</h3>
<div class="outline-text-3" id="text-1-4">
</div><div id="outline-container-orgheadline9" class="outline-4">
<h4 id="orgheadline9"><span class="section-number-4">1.4.1</span> Pros:</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>they are easy to interpret.</li>
<li>they can easily handle mixed discrete and continuous inputs.</li>
<li>they are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points).</li>
<li>they perform automatic variable selection.</li>
<li>they are relatively robust to outliers.</li>
<li>they scale well to large data sets.</li>
<li>they can be modified to handle missing inputs.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="orgheadline10"><span class="section-number-4">1.4.2</span> Cons:</h4>
<div class="outline-text-4" id="text-1-4-2">
<ul class="org-ul">
<li>they do not predict very accurately compared to other kinds of model. This is in part due to the</li>
</ul>
<p>
greedy nature of the tree construction algorithm.
</p>
<ul class="org-ul">
<li>A related problem is that trees are unstable:</li>
</ul>
<p>
small changes to the input data can have large eﬀects on the structure of the tree, due to the hierarchical nature of the tree-growing process, causing errors at the top to aﬀect the rest of the tree.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline15" class="outline-2">
<h2 id="orgheadline15"><span class="section-number-2">2</span> Random Forest</h2>
<div class="outline-text-2" id="text-2">
<p>
Random forests are a machine learning algorithm which averages the predictions over decision trees restricted to random subsets of the input features. They are widely used because they often perform very well with almost no parameter tuning.
</p>
</div>
<div id="outline-container-orgheadline13" class="outline-3">
<h3 id="orgheadline13"><span class="section-number-3">2.1</span> Context</h3>
<div class="outline-text-3" id="text-2-1">
<p>
This concept has the prerequisites:
</p>
<ul class="org-ul">
<li>decision trees (Random forests are ensembles of decision trees.)</li>
<li>bagging (Bagging is a part of the random forest algorithm.)</li>
</ul>
<p>
Bagging is a technique for reducing the variance of a learning algorithm by averaging the predictions obtained from random resamplings of the training data. It can improve the performance of unstable algorithms such as decision trees.
</p>

<ul class="org-ul">
<li>generalization (Averaging over random resamplings is intended to improve generalization performance.)</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline14" class="outline-3">
<h3 id="orgheadline14"><span class="section-number-3">2.2</span> Goals</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Know the basic random forest algorithm</li>
<li>What effect does varying the number of features have? What are the advantages of larger or smaller values?</li>
<li>How do you determine the relevance of each of the input features to the classification?</li>
<li>How do you estimate out-of-sample error as the training is progressing?</li>
</ul>
</div>
</div>
</div>
</div>
</body>
</html>
