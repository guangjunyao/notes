<!DOCTYPE html>
<html>
<head>
<!-- 2018-04-12 Thu 13:58 -->
<meta  charset="utf-8">
<meta  name="viewport" content="width=device-width, initial-scale=1">
<title></title>
<meta  name="generator" content="Org-mode">
<meta  name="author" content="weiwu">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
<link href="css/style.css" rel="stylesheet" type="text/css" />
<link rel="stylesheet" type="text/css" href="./styles/demo/css/style.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div class="outline-2" id="meta">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left"><b>Author</b></td>
<td class="org-left">weiwu (victor.wuv@gmail.com)</td>
</tr>

<tr>
<td class="org-left"><b>Date</b></td>
<td class="org-left">2018-04-12 13:58:03</td>
</tr>
</tbody>
</table>
</div>


<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">Workflow</h2>
<div class="outline-text-2" id="text-orgheadline1">
<ol class="org-ol">
<li>define goal.</li>
<li>search paper/report.</li>
<li>read pdf/code, take notes.</li>
<li>present general idea.</li>
<li>write pseudo code, set input parameters.</li>
<li>write/modify code.</li>
<li>plugging gs to run with data.</li>
<li>check out result.</li>
<li>prepare presentation of all previous steps.</li>
<li>present and get feedback.</li>
<li>deploy function definition, function sampling.</li>
</ol>
</div>
</div>

<div id="outline-container-orgheadline75" class="outline-2">
<h2 id="orgheadline75">Deep Learning</h2>
<div class="outline-text-2" id="text-orgheadline75">
</div><div id="outline-container-orgheadline74" class="outline-3">
<h3 id="orgheadline74">Natural Language Processing</h3>
<div class="outline-text-3" id="text-orgheadline74">
</div><div id="outline-container-orgheadline19" class="outline-4">
<h4 id="orgheadline19">Personalized Intelligent Agents(PIA)</h4>
<div class="outline-text-4" id="text-orgheadline19">
</div><div id="outline-container-orgheadline13" class="outline-5">
<h5 id="orgheadline13">Objective:</h5>
<div class="outline-text-5" id="text-orgheadline13">
</div><ul class="org-ul"><li><a id="orgheadline2"></a>Short Term<br ><div class="outline-text-6" id="text-orgheadline2">
<ul class="org-ul">
<li>make traditional and mature NLP model customizable visually by end users at multiple levels e.g., training data, model parameter.</li>
<li>incorporate customizable NLP signals into both qualitative and quantitative investment strategies and research process.</li>
<li>recommendation of context sensitive synonyms.</li>
<li>Hot-word trending analysis on various topics, technology, industrial sectors, analysts comment, company announcement, Deep Learning authors comments, popular topics, papges, etc. Scrape the historical archives of web text in order to get for each post the following information: date, keywords, text.</li>
</ul>
</div></li>

<li><a id="orgheadline3"></a>Long Term<br ><div class="outline-text-6" id="text-orgheadline3">
<ul class="org-ul">
<li>context sensitive, Personalized Intelligent Agents, forming an eco-system collaborating, competing and evolving together on behalf of their owners.</li>
<li>Graph2vec, 把word2vec应用在用户workflow使用序列上，根据用户使用app/func的顺序，把app/func看做单词，也是可以形成这样的序列数据，进而训练处每个app对应的向量。利用这个向量计算app之间的相似度，把真正内容相关的app聚合在一起，推荐给用户。</li>
<li>可计算的知识服务引擎, like WolframAlpha.</li>
</ul>
</div>
<ul class="org-ul"><li><a id="orgheadline4"></a>智能投顾<br ><div class="outline-text-7" id="text-orgheadline4">
<ul class="org-ul">
<li>基于金融理财产品知识图谱的语义查询。知识图谱从互联网上公开信息构建，并支持如“高收益”这样的语义搜索。</li>
<li>搜索相关主题的股票，及其新闻，公告，如“雄安题材”，“苹果主题”，“独角兽”等。</li>
</ul>
</div></li>
<li><a id="orgheadline5"></a>金融新闻Bots<br ><div class="outline-text-7" id="text-orgheadline5">
<ul class="org-ul">
<li>实时监控各大金融新闻网站的数据，构建了一个监控平台。 根据用户输入的自然语言问题，自动推送与其相关的新闻，并标记其中和用户输入问题相关的段落。</li>
<li>Event-Driven Stock prediction, using sentiment analysis on the web-crawling tokens/documents with doc2vec.</li>
</ul>
</div>
<ul class="org-ul"><li><a id="orgheadline6"></a>金融搜索<br ><div class="outline-text-8" id="text-orgheadline6">
<p>
显示当前关键词所有的新闻消息，公告，根据消息数量得出的热度。
</p>
</div></li></ul></li>
<li><a id="orgheadline7"></a>智能选股<br ><div class="outline-text-7" id="text-orgheadline7">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left">百科类筛选</td>
<td class="org-left">ST股票， 非ST股票， 上证50股票</td>
</tr>

<tr>
<td class="org-left">数据类筛选</td>
<td class="org-left">股价小于100块股票</td>
</tr>

<tr>
<td class="org-left">事件类筛选</td>
<td class="org-left">刚复牌股票，今年刚上市的股票</td>
</tr>

<tr>
<td class="org-left">选股类筛选</td>
<td class="org-left">每股收益在0.2-0.3之间，且市盈率小于30的股票;浙江省市盈率小于20倍的公司</td>
</tr>
</tbody>
</table>
</div></li>

<li><a id="orgheadline8"></a>知识图谱分析<br ><div class="outline-text-7" id="text-orgheadline8">
<p>
通过知识图谱相关技术从招股书、年报、公司公告、券商研究报告、新闻等半结构化表格和非结构化文本数据中批量自动抽取公司的股东、子公司、供应商、客户、合作伙伴、竞争对手等信息，构建出公司的知识图谱。
</p>

<p>
关系可以是企业的上下游、合作、竞争对手、子母公司、投资、对标等关系，可以是高管与企业间的任职等关系，也可以是行业间的逻辑关系，实体则是投资机构、投资人、企业等等，把它们用知识图谱表示出来，从而进行更深入的知识推理。
</p>

<p>
在某个宏观经济事件或者企业相关事件发生的时候，券商分析师、交易员、基金公司基金经理等投资研究人员可以通过此图谱做更深层次的分析和更好的投资决策，比如在美国限制向中兴通讯出口的消息发布之后，如果我们有中兴通讯的客户供应商、合作伙伴以及竞争对手的关系图谱，就能在中兴通讯停牌的情况下快速地筛选出受影响的国际国内上市公司从而挖掘投资机会或者进行投资组合风险控制。
</p>

<p>
某公司打算借壳上市，通过知识图谱相关技术寻找相关联的公司，找到有可能的潜在借壳对象。
<img src="./img/knowledge_graph_company.jpg" alt="knowledge_graph_company.jpg">
</p>


<div class="figure">
<p><img src="./img/knowledge_graph_organization.png" alt="knowledge_graph_organization.png">
</p>
</div>
</div></li>
<li><a id="orgheadline12"></a>Event-Driven Strategies<br ><ul class="org-ul"><li><a id="orgheadline9"></a>Goal:<br ><div class="outline-text-8" id="text-orgheadline9">
<p>
Detect event from announcement, predict stock price.
</p>
</div></li>
<li><a id="orgheadline10"></a>events:<br ><div class="outline-text-8" id="text-orgheadline10">
<ul class="org-ul">
<li>Earnings Releases:</li>
</ul>
<p>
Corporate earnings tend to move markets when they come in above or below the market’s expectations, which means that it’s important for active traders to understand the expected figures beforehand.
</p>
<ul class="org-ul">
<li>Mergers &amp; Acquisitions:</li>
</ul>
<p>
M&amp;A tends to produce dramatic increases or decreases in share prices depending on the terms of the deal, while creating an opportunity for arbitrage strategies between the buyer and seller.
</p>
<ul class="org-ul">
<li>Spin-Offs:</li>
</ul>
<p>
Spin-offs tend to see an initial decline in share price as institutional investors who received shares sell off their stake to comply with regulatory requirements or other rules, thereby creating opportunities for traders.
</p>
</div></li>
<li><a id="orgheadline11"></a>Tool:<br ><div class="outline-text-8" id="text-orgheadline11">
<p>
Named Entity Recognization, Entity Relation Extraction, Recurrent Neural Networks, CNN.
</p>
</div></li></ul></li></ul></li></ul>
</div>

<div id="outline-container-orgheadline14" class="outline-5">
<h5 id="orgheadline14">Overview Steps:</h5>
<div class="outline-text-5" id="text-orgheadline14">
<ul class="org-ul">
<li>数据爬虫 -&gt; 数据清洗(干净数据) -&gt; 语义分析, 命名实体识别(识别金融实体、实体提取及消歧、关系提取,分区语义网络) -&gt; 文档树/表 -&gt; 图谱(根据业务需要，定义并识别金融实体间的各种关系，进而生成知识图谱) -&gt; 本体，本体存储 -&gt; 语用分析 -&gt; 分析推理，逻辑(定义并表达业务逻辑，通过在知识图谱上实现各种具体任务来体现数据价值，如推理等，实现数据到智能的升华。)</li>

<li>Pipeline: choose category and depth in GS skill -&gt; return pages GID -&gt; fetch paper via GID in python -&gt; text processing -&gt; word embedding models, entity extraction, classification, topic extraction, relation extraction, sequence to sequenc, sentiment analysis -&gt; help users to select X,y.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline15" class="outline-5">
<h5 id="orgheadline15">Model</h5>
<div class="outline-text-5" id="text-orgheadline15">
<ul class="org-ul">
<li>State of the Art NL models based on DNN
<ul class="org-ul">
<li>CBOW</li>
<li>Skip gram</li>
<li>RNN(Sequence to sequence)</li>
<li>LSTM</li>
<li>Convolutional Network Sentence classification</li>
</ul></li>
<li>Libraries
<ul class="org-ul">
<li>Gensim</li>
<li>SPACY</li>
<li>IEPY</li>
<li>Stanford coreNLP</li>
<li>NLTK</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-5">
<h5 id="orgheadline18">Data</h5>
<div class="outline-text-5" id="text-orgheadline18">
</div><ul class="org-ul"><li><a id="orgheadline16"></a>Text Labeled with (semi) structured data<br ><div class="outline-text-6" id="text-orgheadline16">
<ul class="org-ul">
<li class="on"><code>[X]</code> wiki page titles and page text, wiki category</li>
<li class="off"><code>[&#xa0;]</code> company filings text, filing classification, event time series data (attributes manually extracted)</li>
<li class="off"><code>[&#xa0;]</code> finance conference scripts.</li>
<li class="off"><code>[&#xa0;]</code> corporate telephone comference notes, memo.</li>
<li class="off"><code>[&#xa0;]</code> company filings text, company industry classification</li>
<li class="off"><code>[&#xa0;]</code> textbook or CFA chapter name, chapter exercise questions, multiple choice questions</li>
<li class="off"><code>[&#xa0;]</code> SSRN paper keyword, category, title, abstract, data, model and conclusion</li>
</ul>
</div></li>
<li><a id="orgheadline17"></a>Text Unlabeled<br ><div class="outline-text-6" id="text-orgheadline17">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> analyst research reports, analyst research reports, report classification, analyst ranking</li>
<li class="off"><code>[&#xa0;]</code> textbooks</li>
</ul>
</div></li></ul>
</div>
</div>
<div id="outline-container-orgheadline39" class="outline-4">
<h4 id="orgheadline39"><span class="todo TODO">TODO</span> Knowledge Graph</h4>
<div class="outline-text-4" id="text-orgheadline39">
</div><div id="outline-container-orgheadline20" class="outline-5">
<h5 id="orgheadline20"><span class="done DONE">DONE</span> entity relationship<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-19 Mon&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></h5>
<div class="outline-text-5" id="text-orgheadline20">
<ul class="org-ul">
<li>命名实体识别</li>
</ul>
<p>
现在常用的方法有「条件随机场（CRF）」、「最大熵隐马尔科夫」、「隐马尔科夫」等序列标注模型。 主要的处理思想有:
</p>

<ul class="org-ul">
<li class="on"><code>[X]</code> finish join learning entity extraction paper.<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-20 Tue&gt;</span></span></li>
<li class="off"><code>[&#xa0;]</code> summerize text first, then event extraction?</li>
<li class="on"><code>[X]</code> find source code and scheme for this paper.</li>
<li class="off"><code>[&#xa0;]</code> from survey paper -&gt; book -&gt; reference paper -&gt; citation paper -&gt; application -&gt; open source library.</li>
<li class="off"><code>[&#xa0;]</code> company relation</li>
<li class="off"><code>[&#xa0;]</code> analyst relation</li>
<li class="off"><code>[&#xa0;]</code> entity extraction resolution detection like author, publisher.</li>
<li class="on"><code>[X]</code> pseudo code of node, edge upload.</li>
<li class="on"><code>[X]</code> summerize nlp library extraction result comparison in jupyter notebook.</li>
<li class="on"><code>[X]</code> find the difference of attirbutes not in Juyuan database, searching for useful information.</li>
</ul>
<p>
聚源数据库已经包含了大量的公司信息，暂时没有在百科三元组发现更有价值的信息。
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> extract the triple relation information.</li>
<li class="on"><code>[X]</code> visualization of triples.</li>
<li class="off"><code>[&#xa0;]</code> NER of all listed company pages content what analyst care about: 有关内容包括：主要产品，产业链，竞争对手，合作伙伴，投资方，key person(如公司跟投资人关联), 上市交易所，sentiment, 分析师评级，评论，公司重大公告.</li>
<li class="on"><code>[X]</code> Chinese NER model is missing, searching. models are in the Chinese model jar file.</li>
<li class="on"><code>[X]</code> test stanford-corenlp to extract keywords and NER en.</li>
<li class="on"><code>[X]</code> compare nlp libraries.</li>
<li class="on"><code>[X]</code> extract Named Entity Recognition.</li>
<li class="off"><code>[&#xa0;]</code> extract RDF company triples.</li>
<li class="off"><code>[&#xa0;]</code> listed companies triples importing to neo4j.</li>
<li class="off"><code>[&#xa0;]</code> read Q&amp;A knowledge graph paper.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline21" class="outline-5">
<h5 id="orgheadline21"><span class="done DONE">DONE</span> 语料收集:<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></h5>
<div class="outline-text-5" id="text-orgheadline21">
<ul class="org-ul">
<li>目标语料格式：</li>
</ul>
<p>
实体1  实体2  关系  包括实体1，实体2和他们之间关系的语句。
</p>
<ul class="org-ul">
<li>加快语料收集的想法：
<ol class="org-ol">
<li>自定义字典法，利用已有的种子实体。</li>
<li>在SSE上搜索已经有的投资，收购等种子实体关系，得到语料。</li>
<li>利用NER<sub>IDCNN</sub><sub>CRF的实体识别得到语料里面的实体</sub>，现有模型支持人名，组织机构和位置。</li>
<li>从distant supervision的方法中获取灵感，可以首先找到具有确定关系的实体对，然后再去爬取该实体对共同出现的语句作为正样本。负样本则从实体库中随机产生没有关系的实体对，最后去爬取这样实体对共同出现的语句，这样的语句可以通过网络爬虫从雪球，google news抓取。*这样保证了语料收集的快速性和关系数量的扩展性*。</li>
<li>对于具有确定关系的实体对，从百度百科Triples得到。</li>
</ol></li>

<li class="on"><code>[X]</code> finish Att BLSTM paper.<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></li>
<li class="on"><code>[X]</code> 先完成“投资”这一类语料的收集。</li>
<li class="on"><code>[X]</code> 目标：按实体 实体 关系 语料内容的格式放入训练文件，以供模型训练。</li>
<li class="on"><code>[X]</code> 丰富语料的思路：通过word2vec 相似词找到“投资”的相似词，如设立，增资，入股，收购，并购，换股;再找以上6个词的相似词。</li>
</ul>
<p>
下表为投资这一大类所包含的相似关系。
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">设立</th>
<th scope="col" class="org-left">增资</th>
<th scope="col" class="org-left">入股</th>
<th scope="col" class="org-left">收购</th>
<th scope="col" class="org-left">并购</th>
<th scope="col" class="org-left">换股</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">成立</td>
<td class="org-left">受让</td>
<td class="org-left">现金出资</td>
<td class="org-left">要约收购</td>
<td class="org-left">海外并购</td>
<td class="org-left">转股</td>
</tr>

<tr>
<td class="org-left">发起设立</td>
<td class="org-left">扩股</td>
<td class="org-left">携手</td>
<td class="org-left">拟收购</td>
<td class="org-left">重组</td>
<td class="org-left">交换</td>
</tr>

<tr>
<td class="org-left">组建</td>
<td class="org-left">扩股</td>
<td class="org-left">间接持有</td>
<td class="org-left">并表</td>
<td class="org-left">整合</td>
<td class="org-left">配股</td>
</tr>

<tr>
<td class="org-left">新设</td>
<td class="org-left">占股</td>
<td class="org-left">所持</td>
<td class="org-left">过户</td>
<td class="org-left">兼并</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">出资</td>
<td class="org-left">转让给</td>
<td class="org-left">联手</td>
<td class="org-left">收购了</td>
<td class="org-left">业务整合</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">共同出资</td>
<td class="org-left">认缴</td>
<td class="org-left">正式成为</td>
<td class="org-left">资产收购</td>
<td class="org-left">借壳上市</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">全资</td>
<td class="org-left">定向增发</td>
<td class="org-left">转让给</td>
<td class="org-left">通过收购</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">参股</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">参股</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">入驻</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">创投</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgheadline35" class="outline-5">
<h5 id="orgheadline35">实体和关系的联合抽取处理思想：<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-22 Thu&gt; </span></span> -</h5>
<div class="outline-text-5" id="text-orgheadline35">
</div><ul class="org-ul"><li><a id="orgheadline22"></a>goal<br ><div class="outline-text-6" id="text-orgheadline22">
<ol class="org-ol">
<li>利用NER<sub>IDCNN</sub><sub>CRF的实体识别得到语料里面的实体</sub>，现有模型支持人名，组织机构和位置。</li>
<li>RE<sub>BGRU</sub><sub>2ATT关系识别</sub>。</li>
<li class="off"><code>[&#xa0;]</code> Joint extraction of events and entities within a document context.</li>
</ol>
<p>
Conceptually the method can be applied to Chinese event extraction if you have a training corpus annotated with entities and events.
</p>

<p>
However, it would require significant changes on the code for feature generation. The current code makes use of the outputs of Stanford CoreNLP (English) and features extracted from English resources like WordNet, FrameNet, and NELL.
</p>
</div></li>

<li><a id="orgheadline23"></a><span class="done DONE">DONE</span> pseudo code<br ><div class="outline-text-6" id="text-orgheadline23">
</div></li>
<li><a id="orgheadline24"></a>review<br ><div class="outline-text-6" id="text-orgheadline24">
<ul class="org-ul">
<li>the limits of GRU, its memory performance without attention, find out the threshold.</li>
<li class="off"><code>[&#xa0;]</code> selecting GRU or LSTM Depends on length of input sentence.</li>
<li class="off"><code>[&#xa0;]</code> using existing Knowledge graph and collected  training data.</li>
<li class="off"><code>[&#xa0;]</code> use quantitative research, economic indicator formula, analyst report as training data.</li>
<li class="off"><code>[&#xa0;]</code> what's gold-standard entity information.</li>
</ul>
</div></li>
<li><a id="orgheadline25"></a>study book DL for RE.<br ><div class="outline-text-6" id="text-orgheadline25">
<ul class="org-ul">
<li class="on"><code>[X]</code> GRU network, difference between LSTM. simpler.</li>
<li class="off"><code>[&#xa0;]</code> entity mention detection的过程和处理结构.</li>
</ul>
</div></li>


<li><a id="orgheadline27"></a>bugs:<br ><ul class="org-ul"><li><a id="orgheadline26"></a>multiple white space in the entities.<br ></li></ul></li>

<li><a id="orgheadline28"></a><span class="done DONE">DONE</span> presentation<br ><div class="outline-text-6" id="text-orgheadline28">
<ul class="org-ul">
<li class="on"><code>[X]</code> RNN structure.</li>
<li class="on"><code>[X]</code> how to use RNN to extract entity.</li>
<li class="on"><code>[X]</code> GRU network, difference between LSTM.</li>
<li class="on"><code>[X]</code> Bi-directional LSTM.</li>
<li class="on"><code>[X]</code> build RNN tensorflow code.</li>
<li class="on"><code>[X]</code> pseudo code for GRU attention Relation Extraction.</li>
<li class="on"><code>[X]</code> 看deep learning for information extraction书relation extraction和event extraction.</li>
<li class="on"><code>[X]</code> 写summary。</li>
</ul>
</div></li>

<li><a id="orgheadline29"></a>TextRank<br ><div class="outline-text-6" id="text-orgheadline29">
<ul class="org-ul">
<li class="on"><code>[X]</code> test text rank example.</li>
<li class="on"><code>[X]</code> paper - TextRank: Bringing Order into Texts.</li>
<li class="on"><code>[X]</code> plot graph.</li>
<li class="on"><code>[X]</code> pseudo code.</li>
</ul>
</div>
<ul class="org-ul"><li><a id="orgheadline30"></a><span class="done DONE">DONE</span> implementation usecase on GS.<br ><div class="outline-text-7" id="text-orgheadline30">
<ul class="org-ul">
<li class="on"><code>[X]</code> create a research article node of pdf format.</li>
<li class="on"><code>[X]</code> convert the pdf to text.</li>
<li class="on"><code>[X]</code> get the highlight of the text.</li>
<li class="on"><code>[X]</code> change the number of highlights to a fraction of total words or certain number.</li>
<li class="on"><code>[X]</code> feed the highlight into next step.</li>
<li class="on"><code>[X]</code> create highlight nodes on GS.</li>
<li class="on"><code>[X]</code> list tool documents.</li>
</ul>
</div></li>
<li><a id="orgheadline31"></a><span class="done DONE">DONE</span> customize textrank to textrank4zh, change output keywords number.<br ><div class="outline-text-7" id="text-orgheadline31">
</div></li>
<li><a id="orgheadline32"></a>train model to recognize company, indicator, signal.<br ></li>
<li><a id="orgheadline33"></a>deep learning Named Entity Recognization(NER) model on GS.<br ></li>
<li><a id="orgheadline34"></a>问题<br ><div class="outline-text-7" id="text-orgheadline34">
<ul class="org-ul">
<li class="on"><code>[X]</code> 什么是textrank算法.</li>
</ul>
<p>
It's a <b>graph-based ranking</b> algorithm that deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> 它能解决什么问题。extractive summarization, keywords extraction.</li>
<li class="on"><code>[X]</code> 对语言类型（中英文）是否有要求：对语言没有要求。</li>
</ul>
</div></li></ul></li></ul>
</div>
<div id="outline-container-orgheadline36" class="outline-5">
<h5 id="orgheadline36"><span class="done DONE">DONE</span> extract the structure of a document, represent as a graph</h5>
<div class="outline-text-5" id="text-orgheadline36">
<p>
<a href="https://www.iwencai.com/msgconsule/search?qs=pc_~soniu~info~all~resultpage~topsearchbox&amp;tid=report&amp;w=%E8%B4%B5%E5%B7%9E%E8%8C%85%E5%8F%B0">https://www.iwencai.com/msgconsule/search?qs=pc_~soniu~info~all~resultpage~topsearchbox&amp;tid=report&amp;w=%E8%B4%B5%E5%B7%9E%E8%8C%85%E5%8F%B0</a>
</p>


<div class="figure">
<p><img src="./img/knowledge_graph_report.png" alt="knowledge_graph_report.png">
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline37" class="outline-5">
<h5 id="orgheadline37">extract the knowledge from company and products.</h5>
<div class="outline-text-5" id="text-orgheadline37">

<div class="figure">
<p><img src="./img/knowledge_graph_fruit.png" alt="knowledge_graph_fruit.png">
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline38" class="outline-5">
<h5 id="orgheadline38">extract keywords from documents, create a knowledge graph.</h5>
</div>
</div>
<div id="outline-container-orgheadline40" class="outline-4">
<h4 id="orgheadline40"><span class="done DONE">DONE</span> Word Embedding(Word2Vec):<span class="timestamp-wrapper"><span class="timestamp">&lt;2017-12-01 Fri&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-12-31 Sun&gt;</span></span></h4>
<div class="outline-text-4" id="text-orgheadline40">
</div>
<div id="outline-container-orgheadline41" class="outline-5">
<h5 id="orgheadline41">Goal/use case</h5>
<div class="outline-text-5" id="text-orgheadline41">
<ul class="org-ul">
<li>use such word2vec to find similar keywords.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline42" class="outline-5">
<h5 id="orgheadline42">jobs: 数据收集， 清洗</h5>
<div class="outline-text-5" id="text-orgheadline42">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> train analyst report and save model on hdfs, load this model as a j node.</li>
<li class="off"><code>[&#xa0;]</code> gs similar words function test use analyst report.</li>
<li class="on"><code>[X]</code> upload all vocabulary in word2vec model to Neo4j.</li>
<li class="on"><code>[X]</code> create a function: word<sub>rec</sub>(model, keywords, topn)</li>
<li class="off"><code>[&#xa0;]</code> manually add categories and page links in sql file.</li>
<li class="on"><code>[X]</code> return word embedding model to next step in GS.</li>
<li class="on"><code>[X]</code> word embedding的设计文档修改.</li>
<li class="on"><code>[X]</code> pack pages into a corpus file.</li>
<li class="off"><code>[&#xa0;]</code> compare cutting on paragraph and document.</li>
<li class="off"><code>[&#xa0;]</code> train few files to see if there's repeat training on word2vec.</li>
<li class="off"><code>[&#xa0;]</code> extract data from financial documents — usually PDFs — in an automated way, and to produce “better-than-human” analyses. extract data from tables and text.</li>
<li class="off"><code>[&#xa0;]</code> train function names based on wiki pages on functions, models, and python/matlab/sas/cpp-reference manuals, function names and function descriptions, excel formula, VBA, VB, guass, whatever software which has a function dictionary and manual.</li>
<li class="off"><code>[&#xa0;]</code> retrieve pages title and id under categories from mysql.</li>
<li class="off"><code>[&#xa0;]</code> LSA or LDA analysis on unstructured text, which will give a clustering of words on every topic.</li>
<li class="off"><code>[&#xa0;]</code> visualize vocabulary embedding using t-SNE which project embedding vectors into 2-D surface from an proper perspective using tensorboard locally which can ignore uploading to projector online.</li>
<li class="off"><code>[&#xa0;]</code> create LSTM networks on xarray data.</li>
<li class="on"><code>[X]</code> create test program to run word embedding, to visualize output.</li>
<li class="off"><code>[&#xa0;]</code> <b>What is fueling heavy investment in machine learning in the financial industry and how does it fit into customers’ workflows?</b>
  A lot of our customers’ workflows are being automated, entirely or partially. What they’re doing today is more on the cognitive side: strategy and portfolio selection, formulating the investment theses, etc. People are trying to solve many, many problems in finance using these methods, because they allow for the building of more sophisticated intelligence into trading and client facing workflows. These methods can improve efficiency, or, crucially, allow us to approach problems which heretofore were intractable – due to complicated interactions in the data, complexity of the problem, availability of data or computational resources, and so on.</li>
<li class="on"><code>[X]</code> provide xarray data to Zhou.</li>
<li class="on"><code>[X]</code> provide Sun Chinese wiki.</li>
<li class="on"><code>[X]</code> network Bloomberg about tensorflow.</li>
<li class="on"><code>[X]</code> retrieving speed test from mongodb.</li>
<li class="on"><code>[X]</code> test case on finance domain word embedding prediction.</li>
<li class="on"><code>[X]</code> dumping wiki pages to mongodb.</li>
<li class="on"><code>[X]</code> testing GPU server.</li>
<li class="on"><code>[X]</code> configuring deep learning hardware, operation system, software.</li>
<li class="on"><code>[X]</code> test sets simularity, A-B=C-D?, A+B=?</li>
<li class="on"><code>[X]</code> incremental training finance pages based via online training.
online training can not continue missing frequency in pretrained google binary file.</li>
<li class="off"><code>[&#xa0;]</code> cut/training Chinese osets words into files.</li>
<li class="off"><code>[&#xa0;]</code> compare similarity between category and end-to-node oset element.</li>
<li class="off"><code>[&#xa0;]</code> compare the results from GS searching engine and word embedding.</li>
<li class="off"><code>[&#xa0;]</code> import xml pages to elasticsearch.</li>
<li class="on"><code>[X]</code> clustering categories by word embedding, osets, idea.
To calculate the similarity matrix between all 160706 vocabulary in RAM, 160706 *160706 *4(bytes)/1024(bytes)/1014(bytes)=99491MB will be needed.</li>
<li class="on"><code>[X]</code> use <a href="http://www.cis.lmu.de/pub/phraseEmbedding.txt.bz2">phrase embedding</a> as test.
better phrasing results.</li>
<li class="on"><code>[X]</code> take a look at cite space iii.</li>
<li class="on"><code>[X]</code> test word2vec model from finance.</li>
<li class="on"><code>[X]</code> cut paragraph to short sentences, then phrase.</li>
<li class="on"><code>[X]</code> phrase text8</li>
<li class="on"><code>[X]</code> train phrasing sentences word2vec model.</li>
<li class="on"><code>[X]</code> phrase detection with google pretrained vectors.</li>
<li class="on"><code>[X]</code> find available library to extract wiki content.</li>
<li class="on"><code>[X]</code> find all page titles from level 5 finance sub-categories.</li>
<li class="on"><code>[X]</code> extract page section from wiki xml file.</li>
<li class="off"><code>[&#xa0;]</code> parse Chinese wiki, remove stopwords.</li>
<li class="on"><code>[X]</code> model wiki token corpus.</li>
<li class="off"><code>[&#xa0;]</code> <a href="https://github.com/ryankiros/skip-thoughts">skip-thought</a>.</li>
<li class="on"><code>[X]</code> find corporate finance/mba questions corpos.</li>
<li class="off"><code>[&#xa0;]</code> read A primer on Neural Network Models.</li>
<li class="on"><code>[X]</code> tensorflow structure.</li>
<li class="on"><code>[X]</code> train word2vec model.</li>
<li class="on"><code>[X]</code> test finding similar words from Wiki corpus.</li>
<li class="on"><code>[X]</code> download wiki xml file.</li>
<li class="on"><code>[X]</code> transfer wiki xml file to text format.</li>
<li class="on"><code>[X]</code> load pre-trained vector matrix, predict the context using a word based on the Skip-Gram model.</li>
<li class="on"><code>[X]</code> overview of word2vec, why does it work.</li>
<li class="off"><code>[&#xa0;]</code> video explained by Xin Rong.</li>
<li class="off"><code>[&#xa0;]</code> forward propagation vs backward propagation, CNN explained by Andrew Ng.</li>
<li class="on"><code>[X]</code> paper word2vec Parameter Learning Explained.</li>
<li class="on"><code>[X]</code> understand Tensorflow Word2Vec example.</li>
<li class="on"><code>[X]</code> build a backward propagation network.</li>
<li class="off"><code>[&#xa0;]</code> fi or function def from output of wants whose idea word2vec is close to target want's idea.</li>
</ul>
<p>
建一个想法，根据这个想法找到匹配的FI, or FD. 例如，建一个optimize需求，自动推荐black litterman model, or markowitz mean/variance model.
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> fi and its function def whose word2vec is close to word2vec of function instances of current function def to be built.</li>
</ul>
<p>
当前FI,查找相关的下一步FI.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline43" class="outline-4">
<h4 id="orgheadline43"><span class="done DONE">DONE</span> Chinese wiki model. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-01 Mon&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-12 Fri&gt;</span></span></h4>
<div class="outline-text-4" id="text-orgheadline43">
</div>
<div id="outline-container-orgheadline44" class="outline-5">
<h5 id="orgheadline44">jobs: 训练中文维基数据，嵌入GS</h5>
<div class="outline-text-5" id="text-orgheadline44">
<ul class="org-ul">
<li>choose model using most related model, use wiki category relation similarity to choose model, train specific field category model. get the related category tree, use regular expression to get responding categories from the wiki xml file.</li>
<li class="on"><code>[X]</code> train financial fields model(58+ categories).</li>
<li class="on"><code>[X]</code> use similarity distance to find the nearest category of target words.</li>
<li class="on"><code>[X]</code> similarity test on specific model.</li>
<li class="on"><code>[X]</code> add all pages title to jieba dict.</li>
<li class="on"><code>[X]</code> 中文短语处理，当短语不存在词汇库中时，拆开成词输入到模型。</li>
<li class="on"><code>[X]</code> <p>
preprocessing workflow.
英文text preprocessing需要的注意一些点，及应提供的选择
</p>
<ol class="org-ol">
<li>cut段落或文章</li>
<li>phrase是否进行转换</li>
<li>停词(a, the, of, that, this, he, I&#x2026;)是否保留</li>
<li>数字是否转为英文单词, 中间有数字的单词是否保留(th8)</li>
<li>提取词干（时态转换，单复数单词转换）</li>
<li>标点（撇号'，所有格,缩写如don’t），符号（%,#,&amp;,?,@,\,/,",是否保留）</li>
<li>大小写转换（句首大写转小写，保留全部大写词，专有名词首字母大写保留）</li>
</ol>
<p>
中文分词（主要利用结巴分词）
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> 1. cut段落或文章</li>
<li class="on"><code>[X]</code> 2. 去停词</li>
<li class="on"><code>[X]</code> 去标点符号</li>
<li class="on"><code>[X]</code> 去数字</li>
</ul></li>
<li class="off"><code>[&#xa0;]</code> word2vec fast text comparison.</li>
<li class="on"><code>[X]</code> compare the training results with or without stopwords.</li>
<li class="on"><code>[X]</code> demo code.</li>
<li class="on"><code>[X]</code> visualize &amp; compare results.</li>
<li class="on"><code>[X]</code> create index for zhwiki.</li>
<li class="on"><code>[X]</code> test model.</li>
<li class="on"><code>[X]</code> assign wiki pages extraction task.</li>
<li class="on"><code>[X]</code> insert Chinese wiki to mongo, transform traditional Chinese to simple Chinese.</li>
<li class="on"><code>[X]</code> get rid of the stopwords.</li>
<li class="on"><code>[X]</code> retrie Chinese financial wiki pages from mongo and train.</li>
<li class="off"><code>[&#xa0;]</code> fix zhwiki to mongodb words count.
:wait:</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline45" class="outline-4">
<h4 id="orgheadline45"><span class="done DONE">DONE</span> Building the Wikipedia Knowledge Graph in Neo4j <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-13 Sat&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-09 Fri&gt;</span></span></h4>
<div class="outline-text-4" id="text-orgheadline45">
<p>
<a href="file:///home/weiwu/website/leolle.github.io/CS/MachineLearning/NaturalLanguageProcessing.html">NLP</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> wiki SQL database links graph.</li>
<li class="on"><code>[X]</code> pulling wiki knowledge categories(id), pages(id) and relations to local csv, sql file.</li>
</ul>
</div>
<div id="outline-container-orgheadline46" class="outline-5">
<h5 id="orgheadline46"><span class="done DONE">DONE</span> Data dumps/Import -&gt; create nodes</h5>
<div class="outline-text-5" id="text-orgheadline46">
<ul class="org-ul">
<li>methods</li>
</ul>
<p>
<a href="https://meta.wikimedia.org/wiki/Data_dumps">https://meta.wikimedia.org/wiki/Data_dumps</a>
</p>

<p>
<a href="https://meta.wikimedia.org/wiki/Data_dumps/Import_examples">https://meta.wikimedia.org/wiki/Data_dumps/Import_examples</a>
</p>

<p>
<a href="https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/">https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/</a>
</p>
<ul class="org-ul">
<li>tools</li>
</ul>
<p>
<a href="http://wikipapers.referata.com/wiki/List_of_visualization_tools">http://wikipapers.referata.com/wiki/List_of_visualization_tools</a>
</p>

<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> Import into an empty wiki of el wiktionary on Linux with MySQL, or Neo4j</li>
<li class="off"><code>[&#xa0;]</code> create special wiki reference edge between read only text nodes</li>
<li class="on"><code>[X]</code> watch the youtube video</li>
</ul>
<p>
<a href="https://www.youtube.com/watch?v=o6wueyweC34%20">https://www.youtube.com/watch?v=o6wueyweC34%20</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> read Neo4j document</li>
</ul>
<p>
<a href="http://guides.neo4j.com/wiki">http://guides.neo4j.com/wiki</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> try Neo4j sandbox</li>
</ul>
<p>
<a href="https://neo4j.com/sandbox-v2/">https://neo4j.com/sandbox-v2/</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> create Neo4j docker.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline47" class="outline-5">
<h5 id="orgheadline47"><span class="done DONE">DONE</span> create wiki knowledge graph -&gt; create edges</h5>
<div class="outline-text-5" id="text-orgheadline47">
<ul class="org-ul">
<li class="on"><code>[X]</code> extract gid from get skill to graph.</li>
<li class="on"><code>[X]</code> importing wiki categories and page edge relation to Neo4j.</li>
<li class="on"><code>[X]</code> 上传完备份我再建边.</li>
<li class="on"><code>[X]</code> 加一个loop detection算法，现在只做了direct cycle detection algorithm.
<ul class="org-ul">
<li class="on"><code>[X]</code> use networkx to detect loop.</li>
<li class="on"><code>[X]</code> it's too hard to detect cycles in the whole graph. Starting in a small categories.</li>
<li class="on"><code>[X]</code> don't add direct loop edges to a graph, find<sub>cycles</sub> will only show such direct loop. save this graph.</li>
<li class="on"><code>[X]</code> remove direct cycle and full cycle at a node completely.</li>
</ul></li>
<li class="on"><code>[X]</code> skill GID generating in Python.</li>
<li class="on"><code>[X]</code> 把节点上传. wiki 上传了1040229 page, 381475 categories.</li>
<li class="on"><code>[X]</code> train word2vec model based on GID.</li>
<li class="on"><code>[X]</code> import edge, loop detecting for linking categories nodes.</li>
<li class="on"><code>[X]</code> fetching pages binary content via GID.</li>
<li class="on"><code>[X]</code> test response GID, same with GID saved on Chrome.</li>
<li class="on"><code>[X]</code> test fetching binary text with GID.</li>
<li class="on"><code>[X]</code> extract page to neo4j from xml file.</li>
</ul>
<p>
businessID.domain = <a href="https://zh.wikipedia.org/wiki/">https://zh.wikipedia.org/wiki/</a>:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
node.url = encoded<sub>url</sub>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> import category to neo4j from sql file.</li>
</ul>
<p>
businessID.domain = <a href="https://zh.wikipedia.org/wiki/Category">https://zh.wikipedia.org/wiki/Category</a>:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> double check GID with Shenbing after importing a small set of page.</li>
<li class="on"><code>[X]</code> import page from mongo to neo4j.</li>
<li class="on"><code>[X]</code> backup neo4j after importing categories and page.</li>
<li class="on"><code>[X]</code> delete edges.</li>
<li class="on"><code>[X]</code> importing wiki categories nodes and page nodes to Neo4j.</li>
<li class="on"><code>[X]</code> test importing wiki categories nodes.</li>
<li class="on"><code>[X]</code> skill<sub>2</sub><sub>graph</sub></li>
</ul>
<p>
<code>C-M-r</code> in gs, create 查路径, drag GID: 81F49335AC9C4D84A5F27F7A02AAABBA into the input box, input Parent GID in the parent box.
</p>
</div>
<ul class="org-ul"><li><a id="orgheadline48"></a>Thomson Reuters Knowledge graph perim<br ><div class="outline-text-6" id="text-orgheadline48">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> read how to use the RFM dataset.</li>
</ul>
</div></li>
<li><a id="orgheadline49"></a>relation extraction from training data<br ><div class="outline-text-6" id="text-orgheadline49">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> search paper and public code.</li>
<li class="on"><code>[X]</code> Stanford NLP relation extraction video.</li>
</ul>
</div></li></ul>
</div>
<div id="outline-container-orgheadline50" class="outline-5">
<h5 id="orgheadline50"><span class="done DONE">DONE</span> manual import unsaved categories and edges into Neo4j.</h5>
<div class="outline-text-5" id="text-orgheadline50">
<ul class="org-ul">
<li class="on"><code>[X]</code> find unsaved categories under 金融 category.</li>
<li class="on"><code>[X]</code> save those to a sql file.</li>
<li class="on"><code>[X]</code> upload sql file and edge.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline73" class="outline-4">
<h4 id="orgheadline73"><span class="todo TODO">TODO</span> information extraction system</h4>
<div class="outline-text-4" id="text-orgheadline73">
</div><div id="outline-container-orgheadline51" class="outline-5">
<h5 id="orgheadline51"><span class="done DONE">DONE</span> opinion sentiment analysis. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-09 Fri&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-16 Fri&gt;</span></span></h5>
<div class="outline-text-5" id="text-orgheadline51">
</div>
<ul class="org-ul"><li><a id="orgheadline52"></a><span class="done DONE">DONE</span> read<sub>RMDB</sub><sub>table</sub> -&gt; NLP<sub>sentiment</sub><sub>analysis</sub> -&gt; generate<sub>sentiment</sub><sub>signal</sub>.<br ><div class="outline-text-6" id="text-orgheadline52">
<p>
<a href="https://nlp.stanford.edu/courses/cs224n/2011/reports/nccohen-aatreya-jameszjj.pdf">sentiment prediction</a>
</p>
<ul class="org-ul">
<li>Sentiment analysis 算法.</li>
</ul>
<p>
SVM, HMM, naive bayes, 最大熵, K-NN, Dictionary.
</p>
<ul class="org-ul">
<li>爬取Google news, 雪球， 虎嗅， 微信上所有300支股票的文档，再进行sentiment analysis, 结果再排序，选最好的5只。</li>
<li class="off"><code>[&#xa0;]</code> sentiment score做为单因子测试, upload sentiment data to hadoop and test factor in FS.</li>
<li class="on"><code>[X]</code> read paper <b>joint extraction of entities and relations</b>.</li>
<li class="off"><code>[&#xa0;]</code> read paper <b>Anomalies and Investor Sentiment</b>.</li>
<li class="on"><code>[X]</code> 情感分析指标的设计在GS上实现。</li>
<li class="on"><code>[X]</code> news, market-view articles sentiment analysis.</li>
<li class="on"><code>[X]</code> 发现2018-02-08, 情感指数0.54，2-9日出现大跌。</li>
</ul>
<p>
能否用这个指数来预警，今天可以扩大一下样本空间，看看上证在1%下跌的情况下前一日的情感指数值是如何变化。
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> search paper and books how to use sentiment analysis.</li>
</ul>
</div></li></ul>
</div>
<div id="outline-container-orgheadline57" class="outline-5">
<h5 id="orgheadline57">information retrieval system</h5>
<div class="outline-text-5" id="text-orgheadline57">
</div><ul class="org-ul"><li><a id="orgheadline56"></a>goal<br ><ul class="org-ul"><li><a id="orgheadline53"></a>question and answering from a document<br ><div class="outline-text-7" id="text-orgheadline53">
<ul class="org-ul">
<li>what is tha data.</li>
<li>what is the algorithm.</li>
<li>what is the conclusion.</li>
</ul>
</div></li>
<li><a id="orgheadline54"></a><span class="todo TODO">TODO</span> News summary<br ><div class="outline-text-7" id="text-orgheadline54">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> classify 1 year of analyst research articles.</li>
<li class="off"><code>[&#xa0;]</code> convert PDFs to text files.</li>
<li class="off"><code>[&#xa0;]</code> summerize articles</li>
</ul>
</div></li>
<li><a id="orgheadline55"></a>syntactic parsing<br ></li></ul></li></ul>
</div>

<div id="outline-container-orgheadline66" class="outline-5">
<h5 id="orgheadline66">NLP tools</h5>
<div class="outline-text-5" id="text-orgheadline66">
</div><ul class="org-ul"><li><a id="orgheadline58"></a>Latent Dirichlet Allocation(LDA)<br ><div class="outline-text-6" id="text-orgheadline58">
<p>
Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.
</p>
</div></li>

<li><a id="orgheadline63"></a>Information Extraction<br ><ul class="org-ul"><li><a id="orgheadline59"></a>Entity extraction<br ><div class="outline-text-7" id="text-orgheadline59">
<p>
Extract company, signal, strategy from text documents.
</p>
</div></li>
<li><a id="orgheadline60"></a>Relation extraction<br ><div class="outline-text-7" id="text-orgheadline60">
<p>
Extract relation between entities, which can create a knowledge network.
</p>
</div></li>
<li><a id="orgheadline61"></a>Event extraction<br ></li>

<li><a id="orgheadline62"></a><span class="done DONE">DONE</span> Keywords extraction<br ><div class="outline-text-7" id="text-orgheadline62">
<p>
Extract the keywords from a text document or news.
</p>
</div></li></ul></li>
<li><a id="orgheadline64"></a><span class="done DONE">DONE</span> Word Embedding<br ><div class="outline-text-6" id="text-orgheadline64">
<p>
Recommend similar words.
</p>
</div></li>

<li><a id="orgheadline65"></a><span class="done DONE">DONE</span> Sentiment analysis<br ><div class="outline-text-6" id="text-orgheadline65">
<p>
To create a sentiment index for a keyword, which can be provided as a indicator.
</p>
</div></li></ul>
</div>
<div id="outline-container-orgheadline67" class="outline-5">
<h5 id="orgheadline67"><span class="todo TODO">TODO</span> NLP information system workflow</h5>
<div class="outline-text-5" id="text-orgheadline67">
<p>
The goal of this information system to create a workflow, which can make current task interact with a website tab. This website content may has company, data, factor, strategy that fits current workflow/current task context for agent/user.
</p>
</div>

<ul class="org-ul"><li><a id="orgheadline68"></a>highlight related keywords<br ><div class="outline-text-6" id="text-orgheadline68">
<p>
Starting from selecting one node at GS, the information system can recommend other connected keywords that maybe under the same topic from the openned website.
</p>

<p>
The keyword could be a company, data and other things like mentioned above. At the same time, these keywords should be under the same category.
</p>
</div>

<ul class="org-ul"><li><a id="orgheadline71"></a>usecase 1 find task according to website:<br ><ul class="org-ul"><li><a id="orgheadline69"></a>use LDA to classify the current openned website content.<br ><div class="outline-text-8" id="text-orgheadline69">
<p>
or alternatively extract 1 or 2 document keyphrases from openned website, use this keyphrase as a tag for the website.
</p>
</div></li>

<li><a id="orgheadline70"></a>find most similar task on GS:<br ><div class="outline-text-8" id="text-orgheadline70">
<ol class="org-ol">
<li>calculate similarity between target task and current website theme.</li>
<li>calculation process:</li>
</ol>
<p>
#1. embedding all available task instance.
#2. use keywords, search name of task.
</p>
</div></li></ul></li>

<li><a id="orgheadline72"></a><span class="todo TODO">TODO</span> usecase 2 highlight entities according to GS task:<br ><div class="outline-text-7" id="text-orgheadline72">
<ol class="org-ol">
<li>create a task named 查找动量指标, create requested want afterwards.</li>
<li>open a website(document) at the same time, extract entity related to (动量，指标） in that document via word embedding.</li>
<li>map these entities to GS.</li>
</ol>
</div></li></ul></li></ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline76" class="outline-2">
<h2 id="orgheadline76">Paper Summary</h2>
<div class="outline-text-2" id="text-orgheadline76">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left">Author(year)</td>
<td class="org-left">Paper Title</td>
<td class="org-left">Market and sample data</td>
<td class="org-left">Methodology</td>
<td class="org-left">Findings</td>
</tr>

<tr>
<td class="org-left">Lee and Chung (1996)</td>
<td class="org-left">Investigating the effects of price limits on stock market efficiency.</td>
<td class="org-left">Korean Stock Exchange: the opening and closing price series from January 1990 to December 1993 of 30 active individual stocks which experience considerable numbers of price limits hits are selected by the highest turnover ratio from different industry</td>
<td class="org-left">1. OLS regression analysis 2. Regression analyses are performed using GARCH (1,1) procedure Comparing the results from original data with the results from proxy data. Proxy data means that on the day of closing price hitting the limits, the closing price is not the equilibrium price which is measured by the next day’s opening price.</td>
<td class="org-left">There are positive serial correlations between returns due to the price limits, suggesting the rejection of market efficiency. When removing the effects of price limits, the serial correlations between returns disappear.</td>
</tr>

<tr>
<td class="org-left">Xiao Dingy∗, Yue Zhangz, Ting Liuy, Junwen Duan</td>
<td class="org-left">Deep Learning for Event-Driven Stock Prediction.</td>
<td class="org-left">s&amp;p 500, 15 individual stocks</td>
<td class="org-left">deep learning stock prediction，先做entity relation extraction, 然后把event 做embedding，再用CNN训练，最后预测。</td>
<td class="org-left">6% more than state of art prediction</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</body>
</html>
